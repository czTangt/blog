[{"categories":["android"],"content":"记录安卓逆向的相关操作。 ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:0:0","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"Android 命令\r这里对于基于 Linux 的 Android 系统命令进行简单罗列。 ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:1:0","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"基础命令\rps 命令可输出当前设备正在运行的进程。在 Android 8 之后，ps 命令只能打印出当前进程，需要加上 -e 参数才能打印出全部的进程。 ps -ef | grep \u003cpackage-infomation\u003e netstat 该命令输出 App 连接的 IP、端口、协议等网络相关信息，通常使用的参数组合为 -alpe。netstat -alpe 用于查看所有 sockets 连接的IP和端口以及相应的进程名和 pid。 netstat -alpe | grep \u003cpackage-infomation\u003e lsof 该命令可以用于查看对应进程打开的文件 lsof -p \u003cpid\u003e -l | grep \u003cfile_name\u003e ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:1:1","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"手机信息查看\r通过查看手机信息来决定使用什么版本的工具和程序。 cat /proc/cpuinfo # 查看手机架构 getprop ro.product.cpu.abi # 查看cpu处理器位数 ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:1:2","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"包信息查看\r# 获取当前活动（处于前台）的系统服务信息然后过滤出正在活动的任务 dumpsys activity top | grep TASK # 查看包的信息 dumpsys package \u003cpackage-name\u003e # 查看包含 /data 的虚拟内存区域的详细信息，即查看包含 /data 名称的进程的相关信息。这里就是找到对应虚拟内存区域中的所有文件，它可以寻找包名所在文件的目录 cat /proc/\u003cpid\u003e/maps | grep /data ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:1:3","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"其他\ram 命令主要用于启动或停止服务、发送广播、启动 Activity 等。 # 以 Debug 模式启动 App am start-activity -D -N \u003cpackage-name\u003e/.\u003cclass-name\u003e # 启动指定的 Activity # 按 AndroidManifest 的路径（com.kbtx.redpack_simple.FlagActivity），打开指定控件，这里就是类的路径前面为包的路径，所以直接用 \".\"替代，不一致的情况下需要写全。 am start -n com.kbtx.redpack_simple/.FlagActivity am start -n owasp.mstg.uncrackable2/sg.vantagepoint.uncrackable2.MainActivity # 监视设备上的 Activity Manager 输出 am monitor # 结束进程，这里也可以通过 kill -9 \u003cpid\u003e 使用信号杀死，但是可能会出现进程重启杀不掉的情况 am force-stop \u003cpackage-name\u003e pm 它是 Android 中 packageManager 的命令行，是用于管理 package 的命令。 # 列出所有的包名 pm list package # 寻找包名所在文件的目录 pm path \u003cpackage-name\u003e logcat 可以查看日志的输出 # 查看日志输出 logcat | grep ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:1:4","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"Adb 相关\r","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:2:0","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"Adb 原理\rAdb（Android Debug Bridge）是 Android 开发工具链中的一个重要组件，属于 Android Open Source Project（AOSP）的一部分。它主要用于在开发环境中实现 PC 端与 Android 设备（包括物理设备和模拟器）之间的通信，支持多种设备操作，是连接 PC 端开发环境与移动终端的桥梁。 Adb 采用 C/S（客户端/服务器）架构，主要由以下三个组件构成： Client：运行在 PC 端，主要用于发送命令。用户可以通过命令行工具（如 adb）调用客户端，执行如文件推送（push）、应用安装（install）等操作。客户端会解析这些命令，进行预处理，并将处理后的指令或数据通过网络发送给服务器端。 Server：作为后台进程运行在 PC 端。负责检测 USB 端口，感知设备的连接和拔除，以及模拟器实例的启动或停止。它会维护一张设备状态表，为每个设备标记状态，如 offline（离线）、bootloader（引导程序模式）、recovery（恢复模式）或 online（在线）。同时将客户端的请求通过 USB 或 TCP/IP 方式发送到对应的设备上的 adbd（守护进程）。 Daemon：存在于 Android 设备上，由设备的 init 进程启动，系统开机时自动运行。处理来自服务器端的命令请求，执行相关命令，并将结果返回给服务器端。 Adb 组件\r","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:2:1","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"工作原理\r客户端启动：当用户在 PC 终端中输入 Adb 命令时，客户端会启动。 服务端检测与启动： 客户端会检查 PC 后台是否已运行 Adb 服务器。如果未运行，客户端会自动启动服务器进程。 服务器启动后，会绑定到本地 TCP 端口 5037，并监听来自客户端的命令。 设备连接建立： 服务器会尝试与所有连接的设备建立连接。对于通过 USB 连接的设备，服务器会通过 USB 接口与设备上的 adbd 守护进程通信。 对于模拟器，服务器会扫描本地 TCP 端口范围 5555-5585（前 16 个模拟器使用奇数号端口进行 Adb 通信，偶数号端口用于控制台连接）。例如： 模拟器 1：控制台端口 5554，Adb 端口 5555。 模拟器 2：控制台端口 5556，Adb 端口 5557。 通信过程： 客户端将命令发送到服务器（端口 5037）。 服务器根据设备状态表，将命令转发到对应的设备上的 adbd。 adbd 执行命令并将结果返回给服务器，服务器再将结果返回给客户端。 工作原理\r","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:2:2","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"使用归纳\r安卓模拟器\r通常认为，开启模拟器的 USB 调试后，使用 adb connect 127.0.0.1:port 命令进行连接，然后通过 adb devices 查看设备列表，会看到以 \u003cip:port\u003e 格式显示的设备。然而，结合真机调试和模拟器测试，可以发现： 真机：使用 USB 连接时，插入 USB 线并开启 USB 调试后，设备会被 Adb 服务器识别，adb devices 显示的设备名称为设备的 序列号。 模拟器：通常不使用 USB 调试，而是通过 TCP/IP 连接。直接使用 adb connect 命令连接特定端口即可发现设备。在 NAT 模式下，模拟器必须有 WIFI 连接。模拟器通过 WIFI 连接到主机网卡时，会绑定本地端口，但 Adb 服务器可能无法自动发现模拟器（尤其是模拟器未使用默认端口范围 5555-5585），因此需要手动使用 adb connect 命令进行绑定。 多设备下的 Adb\r在多设备环境中（例如多个模拟器或真机），直接使用 adb shell 等命令可能会因无法确定目标设备而报错。此时需要通过 -s \u003c名称\u003e 参数指定设备进行调试。例如： adb -s 127.0.0.1:21503 shell 在使用 adb 和 frida-ps 进行设备检测时，可能会发现识别出的信息不同。这里连接真机喝逍遥模拟器，前者通过 USB 连接，后者使用 nat 连接 WIFI。以下是相关工具的参数说明： adb --help global options: -a listen on all network interfaces, not just localhost -d use USB device (error if multiple devices connected) -e use TCP/IP device (error if multiple TCP/IP devices available) -s SERIAL use device with given serial (overrides $ANDROID_SERIAL) frida-ps --help Options: -U, --usb connect to USB device -D ID, --device=ID connect to device with the given ID objection --help Options: -N, --network Connect using a network connection instead of USB. -h, --host TEXT [default: 127.0.0.1] -p, --port INTEGER [default: 27042] -ah, --api-host TEXT [default: 127.0.0.1] -ap, --api-port INTEGER [default: 8888] -g, --gadget TEXT Name of the Frida Gadget/Process to connect to. [default: Gadget] -S, --serial TEXT A device serial to connect to. 尝试之后可以得到下面的结论： 使用 Adb 的 -d 选项可以匹配 USB 连接的真机，而 -e 选项可以连接模拟器。 使用 frida-ps 的 -U 选项连接的却是模拟器，这表明二者的检测机制不同。 对于 frida 和 frida-ps，可以使用 -D ID 参数，通过 adb devices 中显示的设备 ID（名称）来匹配不同的设备。 对于 objection，则使用 -S serial 参数来识别设备，这里的 serial 就是设备的 ID。 无线（WIFI）连接\r在测试 objection 的 -p 选项时，发现在模拟器中运行 frida-server 监听 7878 端口时，objection 无法连接，即使设置了 adb forward tcp:27042 tcp:7878 也无法成功连接。因此，尝试使用无线连接的真机来测试 -p 选项是否能够成功。 为了使手机支持无线调试，需要确保手机上存在 adbd 进程。有两种方法可以实现： 通过 USB 连接启动无线调试：使用 Adb 启动 USB 连接手机的无线调试功能。虽然这种方法有些多余（既然已经通过 USB 连接了，再使用无线连接似乎没有必要），但可以作为一种测试手段。 adb tcpip 5555 adb connect \u003cPHONE_IP_ADDRESS\u003e:5555 # `\u003cPHONE_IP_ADDRESS\u003e` 是手机在局域网中的 IP 地址。 使用 Android 命令行指令启动无线调试：可以通过 adb shell 或下载 Termux 应用来执行命令行操作，详细步骤可以参考 adb wifi调试问题。 adb shell setprop service.adb.tcp.port 5555 adb shell stop adbd adb shell start adbd adb connect \u003cPHONE_IP_ADDRESS\u003e:5555 配置完成后在 adb devices 中可以看到一个无线连接的设备。但要确保手机和 PC 在同一个局域网内，因此可以通过 ping 命令测试网络连通性，如果可以 ping 通，说明无线连接可行。然后在手机上启动 frida-server，监听端口（例如 7878）。并在在 PC 上使用 objection 连接无线设备 objection -N -h 192.168.123.213 -p 7878 -g com.example.app explore，如果连接成功，说明无线连接可行。 然后断开连接，使用 USB 连接真机，并在手机上启动 frida-server 的 7878 端口。之后使用 objection 连接 objection -S SERIAL -g com.example.app explore。最后可以发现无论是否使用端口转发，-p 7878 选项都无法成功连接目标真机。 最后发现 frida 将模拟器视为 USB 连接设备，而 Adb 将其识别为 TCP/IP 连接设备。模拟器的 WIFI 功能在这种情况下更像是一个摆设。同时objection 对于 USB 连接设备不提供 frida-server 的端口切换功能，仅对无线连接的设备提供此功能。因此，USB 设备（无论是真机还是模拟器）在使用 objection 时，建议使用 frida-server 的默认端口（27042）。 叠甲\r这个只是我拿真机与模拟器测试的结果，可能存在别的方案使得上述结论无效。 总结\r总结\r这里总结 adb，frida，frida-ps，objection 连接真机和模拟器设备等多设备情况时的处理方法。 List of devices attached 8C5X1J8JR device 127.0.0.1:21503 device 192.168.123.213:5555 device ## WIFI 连接是后来加的，和 8C5X1J8JR 是同一台真机设备 adb\r## 模拟器设备 adb -e shell ## USB连接设备 adb -d shell ## 这两个命令面对同类设备只存在一个的情况时可以使用 ## WIFI连接设备 adb -a shell ## 这个只有单个设备时才能使用，条件比上面两个更苛刻 ## 通用连接 adb -s 192.168.123.213:5555 shell ## -s SERIAL根据序列号来连接，是通用连接方式，多设备都可以连接 Adb 将模拟器识别为 TCP/IP 连接设备，它通过本地回环地址的端口（模拟器 adbd 使用的端口）进行连接。这里提及的模拟器都是使用 NAT 网络连接，其中的 WIFI 是一个摆设，但是如果改为桥接模式，那么可以将模拟器视为一个 WIFI 连接的设备。 frida\r## 通用连接 frida-ps -D ID ## 直接根据上面的 SERIAL充当ID 进行连接 ## 模拟器连接 frida-ps -U ## 还存在一个USB设备的情况下，这个命令只匹配模拟器，若是只有一个模拟器没有真机，也匹配模拟器 ## 端口修改连接 ./frida-server-16.0.8-android-arm64 -l 0.0.0.0:19233 adb forward tcp:19233 tcp:19233 frida -H 127.0.0.1:19233 -f 包名 -l .\\js_example.js frida 将模拟器视为优先级更高的 USB 连接设备，所以对于模拟器的连接直接使用 -U 就可以连接 objection\r## USB设备-\u003eUSB连接的手机和模拟器，这个情况下，frida-server不能更改监听的端口 objection -S SERIAL -g \u003cpackage-name\u003e explore ## WIFI设备，这个情况下可以随意更改frida-server监听端口 objection -N -h 192.168.123.213 -p 7878 -g \u003cpackage-name\u003e explore objection 的 -p 设置只能适用于无线连接设备，USB 连接设备（真机和模拟器）都不可以进行设置。这里 frida-serve","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:2:3","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"Adb 命令\r上文介绍了 Adb 的原理，我们通常需要它来处理相关命令。这里通过 adb shell 进入 adb 环境，它相当于 Android 系统中的 shell 环境，可以执行上文罗列的 Linux 命令。同时可以直接使用 adb shell + linux command 来直接执行 linux 命令，然后结果就会反馈到当前终端。简单连接设备的命令如下： # 查看当前连接的设备 adb devices # 连接模拟器设备，这里是蓝叠的 adb connect 127.0.0.1:5555 # 多个设备下，使用 -s 连接单独设备，或者根据模拟器与真机分别使用-e与-d选项 adb -s 127.0.0.1:5555 shell ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:2:4","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"动态调式\rAndroid 应用分为 java 和 native 两层，所以进行动调时就需要分别对待。这个 博客 就是描述 adb 参与 android 调式的流程，可以结合下面的内容进行理解。 ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:3:0","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"文件修改\r在进行动态调式之前，Android 应用可能采取不同措施防止动态调式，所以就需要修改 apk 的一些设置来达到可以进行动态调式的目的。 修改 debug 权限\r首先在 AndroidManifest.xml 里添加可调试权限 android:debuggable=\"true\"，然后重新签名打包。这里的添加是在 \u003cactivity\u003e 标签中添加，例如：\u003cactivity android:name=\"example\" android:debuggable=\"true\"\u003e。 或者可以使用 Magisk 命令，但是重启失效。具体可以看这篇 博客 来进行操作。 adb shell # adb进入命令行模式 su # 切换至超级用户 magisk resetprop ro.debuggable 1 # 修改权限 stop;start; # 一定要通过该方式重启 修改应用文件\r上述添加权限之后需要保存修改的文件，这时就需要用到 apktool 来进行重打包和签名了。参考 这篇博客 的具体修改流程如下： apktool 反编译 apktool d -f \u003capk_path\u003e -o \u003cfolder_path\u003e 这里 apktool 解包到指定目录下，\u003capk_path\u003e 指待反编译 apk 的路径，\u003cfolder_path\u003e 指把反编译的文件输出的文件夹。这样反编译可以查看 smail 代码，同时反编译出的 AndroidManifest.xml 文件也可以被读取。 apktool 重打包 apktool b \u003cfolder_path\u003e -p \u003capk_path\u003e 这里重新打包的 apk 会在 \u003capk_path\u003e/dist/ 目录中生成。如此一来，就得到了新的修改过的 apk 文件。 生成签名 jarsigner -verbose -keystore \u003ckeystore_path\u003e -signedjar \u003csigned_apk_path\u003e \u003cunsigned_apk_path\u003e \u003ckeystore_alias\u003e 这里 \u003ckeystore_path\u003e 是指 keystore 文件存放的路径，\u003csigned_apk_path\u003e 是指签名后的 apk，\u003cunsigned_apk_path\u003e 是指未签名的 apk，\u003ckeystore_alias\u003e 是指之前生成 keystore 文件时所使用的别名。 ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:3:1","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"jeb 动调\r经过上述修改，就可以在 jeb 中启动动态调试了，首先在 jeb 中相应语句下断点 ctrl + b，之后 debug 模式启动 app。 adb shell am start -D -n com.zj.wuaipojie/.ui.MainActivity # 分析 adb shell am start -D -n \u003cpackage-name\u003e/.\u003cclass-name\u003e # 这里类名前面的文字和包名相同，所以使用\".\"来替代，如果不同就需要写完整路径了 # am start -n 表示启动一个activity，也可以用这种方式直接启动MainActivity之外的Activity # am start -D 表示将应用设置为可调试模式 adb forward tcp:port jdwp:PID jdb -connect com.sun.jdi.SocketAttach:hostname=localhost,port=xxx 然后jeb直接附加程序进行动调即可，具体步骤看这个 博客。 ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:3:2","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"IDA 动调\rIDA 动调分为两种模式，一种是以 attach 模式启动，第二种则以 debug 模式启动，二者的区别在于使用场景，有时候要动态调试的参数在 app 一启动的时候就产生了，时机较早，所以需要以 debug 模式去挂起 app。 attach 模式\r这里首先需要在 IDA Debugger -\u003e Debugger options 中进行设置，防止因为别的进程等事宜暂停。（这个可有可无，但是对于高版本而言，例如我的 Android 12，最好还是加上，详细原因看下面），然后直接在分析的 so 中打断点，之后进行调式。 Debugger\r# 首先在 adb 中运行 android-server adb shell ./data/loval/tmp/android-server # 之后进行端口转发，使用 adb forward 命令之后，adb.exe(server) 会监听 23946 端口等待 IDA 连接，同时连接移动端 android-server 的 23946 端口 adb forward tcp:23946 tcp:23946 # 这里修改端口可以使用 ./android-server -p 19233，然后转发对应的端口，最后在IDA中填写相应端口即可 # 然后直接运行apk即可，下面语句也是运行apk的语句 adb shell am start -n com.zj.wuaipojie/.ui.ChallengeEight 最后在 IDA 中选择 Remote ARM Linux/Android debugger，然后设置 IP 为 127.0.0.1 端口匹配即可调试 so文件。这里注意尽量保持一个设备在场，开启 as 的话，IDA 会识别到别的虚拟机中去。attach 之后，等待左下角的加载条加载完，呈现 AU：idle，这样就可以在 IDA 中直接 F9 或者点击 Start 了，然后 IDA 会处于不可操作状态。操作权转移到了 app 上，在 app 中输入 flag，然后点击 check 等逻辑之后，若是它会调用 so 文件，那么就会因为之前的断点断在我们需要分析的内容处，这样我们就可以进行调试了。 这里主要说一下我在 Android12 中遇到的问题，一开始没有设置上述 Debugger options，然后直接启动的 apk。之后遇到了 got unknown signal #33 、got unknown signal #35 等 signal 错误，然后多跳几个就会崩溃。之后我设置了 Debugger options，然后采用 adb shell am start -n com.zj.wuaipojie/.ui.ChallengeEight 形式打开 app，最后竟然正常调试了！然后我又取消了设置和启动方式，改为一开始的形式启动，发现也正常了！。。。现在感觉这个系统真是玄学，之后若是再次调试，先设置 Debugger options，感觉它应该有点用处。（补充一下，在这个期间还设置了在 Debugger setup 中设置了 exceptions，但是没有任何作用） debug 模式\r按照这篇 文章 进行配置操作。不过因为 so 文件的加载前会先执行 JNI_Onload，如果要反反调试，那么可能就需要在 JNI_Onload 中打断点了。具体可以参考这篇 文章。 调试问题\rIDA 找不到 so 的情况 因为APK设置了该属性 android:extractNativeLibs=\"false\"，所以在 apk 的安装目录中不显示 so 文件，这个时候可以直接将 base.apk 作为so文件进行处理； 或者可以尝试手动复制一份对应的 so 文件放到data/app/\u003cpackage-name\u003e/lib目录下； 安卓高版本(android10及以上)与低版本的差异 高版本因为 libc.so 的路径改变了，所以显示不出所有的线程。之后使用命令 IDA_LIBC_PATH=/apex/com.android.runtime/lib64/bionic/libc.so ./android_server64 启动服务可以完整显示 ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:3:3","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"frida 使用\rFrida-tools 共有六个小工具，分别是 frida CLI、frida-ps、frida-trace、frida-discover、frida-ls-devices、frida-kill。下面就介绍对于工具的使用。 ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:4:0","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"frida\r参数讲解\r单独使用 frida 就可以完成绝大多数功能，下面罗列其相关参数。 --version # 显示程序的版本号并退出 -h, --help # 显示此帮助信息并退出 -D ID, --device=ID # 连接到指定ID的设备上 -U, --usb # 连接到USB设备 -R, --remote # 连接到远程frida服务器 -f FILE, --file=FILE # spawn这个应用 -F, --attach-frontmost # 附加到最前端的应用程序 -p PID, --attach-pid=PID # 附加到PID --debug # 启用兼容Node.js的脚本调试器 -l SCRIPT, --load=SCRIPT # 加载脚本。 -q # 安静模式（无提示），并在-l和-e之后退出。 --no-pause # 启动后自动启动主线程 -o LOGFILE, --output=LOGFILE # 输出到日志文件 启动\rattach 模式 frida -UF # 测试是否连接成功 frida -U pid(通常为\u003cpackage-name\u003e) -l hook.js spawn 模式 frida -U -f com.android.settings -l hook.js --no-pause(后面--no……高版本不写) ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:4:1","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"frida-server\rcd /data/adb/tools # frida-server放在了这个目录下 chmod 777 * ./frida-server-16.0.8-android-arm64 # 防止反调试，切换端口监听（默认端口为27042），这里监听任何对手机 1314 端口的网络连接，需要注意端口转发问题 ./frida-server-16.0.8-android-arm64 -l 0.0.0.0:1314 adb forward tcp:1314 tcp:1314 # 由此可以查看 frida 监听的端口 netstat -tulp | grep frida ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:4:2","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"frida-ps\rfrida-ps 用于列出进程的一个命令行工具。 参数讲解\r--version # 显示程序的版本号并退出 -h, --help # 显示此帮助信息并退出 -D ID, --device=ID # 连接到指定ID的设备上 -U, --usb # 连接到USB设备 -R, --remote # 连接到远程frida服务器 -H HOST, --host=HOST # 连接到HOST上的远程frida-server。 -O FILE, --options-file=FILE # 包含额外命令行选项的文本文件 -a, --applications # 仅列出应用程序 -i, --installed # 包括所有已安装的应用程序 -j, --json # 以JSON格式输出结果 常用命令\r# 查看当前手机运行的进程 frida-ps -U # 查看设备所有应用程序 frida-ps -Ua # 查看设备上的所有已安装应用程序和对应的名字 frida-ps -Uai # 获得设备的进程名和包名 frida-ps -U -a ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:4:3","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"相关分析\r获取包名\r首先确保 android 中运行了 frida-server，然后直接使用 frida-ps -Uai。它可以列出设备上运行的进程及其包名，然后选择自己需要的程序包名即可。 定位 native 函数位置\r我们要确定某一个方法，比如 com.bilibili.nativelibrary.LibBili.s 这个方法在 so 中什么位置，叫什么名字。先做如下约定，防止名称混淆。 JNI 函数：Java 函数所对应的 native 函数 JNI 方法：JNI 提供的两百多个 API，比如 GetStringUTFChars 等等。 将一个 JAVA方法 和 JNI函数 连结在一起，有 静态绑定和动态绑定 两种方式，也叫静态注册和动态注册。我们并不能一眼确定某个函数是静态绑定还是动态绑定，过去我们会静态分析反编译的 Java 代码，找到代码中加载的 so，然后在 so 里做 JNI函数的分析和定位（如果是 java_开头为静态注册，为 JNI_OnLoad 中调用 RegisterNative 函数 则是动态注册）。这里通过 frida 和开源工具可以直接进行识别。 静态注册\r如果一个函数采用静态绑定，那么对应 native 函数 的命名遵照以下规则： 前缀 Java_； 紧跟着类的全名（间隔符从由 . 替换成 _）； 最后是方法名； 比如对于 com.bilibili.nativelibrary.LibBili.s 这个类，它如果采用静态注册，那么 JNI函数 就会是 Java_com_bilibili_nativelibrary_LibBili_s。我们可以使用 frida 自带的分析工具 frida trace 进行批量 native hook。下面命令中 -UF 指附加到手机最前台的应用，即当前运行的应用，因此这里需要先打开 App。-i \"Java_com*\" 指 Hook 该 app 当前加载的所有以 \"Java_com\" 开头的 native 函数，-i 后面双引号里是函数名，支持正则表达式的语法。这个时候就可以对 app 进行相关操作，然后会有相应 hook 输出，然后看看输出的类名是否为自己的目标，若是，则确定为静态注册。 frida-trace -UF -i \"Java_com*\" 动态注册\r直接 hook RegisterNatives 函数，然后查看输出的内容是否为自己的目标。同时这里可以使用 unidbg 进行模拟执行，会直接输出动态注册的函数和地址（应对 hook 后不主动调用的情况）。 frida -U -f \u003cpackage_name\u003e -l path/hook_RegisterNatives.js 下面是脚本 hook_RegisterNatives.js 的代码。 function hook_RegisterNatives(taget_class) { var symbols = Module.enumerateSymbolsSync(\"libart.so\"); var addrRegisterNatives = null; for (var i = 0; i \u003c symbols.length; i++) { var symbol = symbols[i]; //_ZN3art3JNI15RegisterNativesEP7_JNIEnvP7_jclassPK15JNINativeMethodi if (symbol.name.indexOf(\"art\") \u003e= 0 \u0026\u0026 symbol.name.indexOf(\"JNI\") \u003e= 0 \u0026\u0026 symbol.name.indexOf(\"RegisterNatives\") \u003e= 0 \u0026\u0026 symbol.name.indexOf(\"CheckJNI\") \u003c 0) { addrRegisterNatives = symbol.address; console.log(\"RegisterNatives is at \", symbol.address, symbol.name); } } if (addrRegisterNatives != null) { Interceptor.attach(addrRegisterNatives, { onEnter: function (args) { var env = args[0]; var java_class = args[1]; var class_name = Java.vm.tryGetEnv().getClassName(java_class); //console.log(class_name); if (class_name === taget_class) { console.log(\"\\n[RegisterNatives] method_count:\", args[3]); var methods_ptr = ptr(args[2]); var method_count = parseInt(args[3]); for (var i = 0; i \u003c method_count; i++) { var name_ptr = Memory.readPointer(methods_ptr.add(i * Process.pointerSize * 3)); var sig_ptr = Memory.readPointer(methods_ptr.add(i * Process.pointerSize * 3 + Process.pointerSize)); var fnPtr_ptr = Memory.readPointer(methods_ptr.add(i * Process.pointerSize * 3 + Process.pointerSize * 2)); var name = Memory.readCString(name_ptr); var sig = Memory.readCString(sig_ptr); var find_module = Process.findModuleByAddress(fnPtr_ptr); console.log(\"[RegisterNatives] java_class:\", class_name, \"name:\", name, \"sig:\", sig, \"fnPtr:\", fnPtr_ptr, \"module_name:\", find_module.name, \"module_base:\", find_module.base, \"offset:\", ptr(fnPtr_ptr).sub(find_module.base)); } } } }); } } function main() { // 只有类名为com.bilibili.nativelibrary.LibBili，才打印输出 hook_RegisterNatives(\"com.bilibili.nativelibrary.LibBili\") } setImmediate(main); ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:4:4","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"错误汇总\r","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:5:0","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"程序安装\r删除问题\r使用 adb install -t xxxx.apk 报错（-t 是覆盖安装） adb: failed to install .\\xxxx.apk: Failure [INSTALL_FAILED_UPDATE_INCOMPATIBLE: Package xxxx signatures do not match previously installed version; ignoring!] adb: failed to install .\\xxxx.apk: Failure [INSTALL_FAILED_TEST_ONLY: installPackageLI] 这里是因为原先的文件还存在，覆盖之前的文件会导致签名不对。如果用手机自带的卸载不能卸载干净，所以需要使用 adb 卸载干净。 adb uninstall \u003cpackage-name\u003e 签名问题\r使用 adb install xxxx.apk 安装报错 adb: failed to install .\\xxxx.apk: Failure [INSTALL_PARSE_FAILED_NO_CERTIFICATES: Failed to collect certificates from /data/app/vmdl2062104312.tmp/base.apk: Attempt to get length of null array] 这里是因为没有签名，签名后再安装即可。 ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:5:1","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"Adb 错误\r连不上设备\r报错 daemon not running； starting now at tcp:5037，这里就是服务启动问题，按照下面流程操作即可。 adb kill-server # 如果使用 devices 没有显示设备，尝试先杀死服务，然后连接报错就进行下面的流程 netstat -ano | findstr 5037 # 找出占用 5037 端口的应用的 pid taskkill -f -pid \u003cpid\u003e # 关闭掉该应用 ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:5:2","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["android"],"content":"frida 错误\rapply 错误\r下面是很常见的错误，但是对于程序运行没有太大的影响。查阅资料只知道是对象创建的问题，但是不知道自己错在了什么地方。 TypeError: cannot read property 'apply' of undefined at \u003canonymous\u003e (frida/runtime/core.js:51) 目前根据经验与尝试，估计为 setImmediate 的错误，现象如下。 function main(){ otherfunc() } setImmediate(main) // 正确 setImmediate(main()) // 错误，直接报错 undefined 由此可以判断，当 setImmediate 的参数为函数调用形式（如 main()）时，实际上会先执行 main()，并将 main() 的返回值传递给 setImmediate。如果main() 没有返回值（默认返回 undefined），则会导致上面的错误。因此正确方法就是将函数名（引用，如 main）传递给 setImmediate，而不是函数调用结果。 加载错误\r下面的错误都是因为加载问题导致的，它在 so 没有加载之前就先进行 hook 了，自然会出现错误。 Error: unable to find module 'libil2cpp.so' Base errot 因此可以使用下面 my_hook_dlopen 函数来调用目标函数，这样确保了 so 打开之后再进行 hook，但是可能存在调用两次的情况。另外可以把 setImmediate 调换为setTimeout，后面加一个延时时间，使得函数延后一段时间之后再调用，这样就有可能实现在函数加载之后再 hook 了。或者可以先注入，然后在 frida 界面调用目标函数，一般也可以实现在函数加载之后再调用的逻辑。 function my_hook_dlopen(soName) { Interceptor.attach(Module.findExportByName(null, \"dlopen\"), { onEnter: function (args) { var pathptr = args[0]; if (pathptr !== undefined \u0026\u0026 pathptr != null) { var path = ptr(pathptr).readCString(); // console.log(path); if (path.indexOf(soName) \u003e= 0) { console.log(\"[+] Successfully call the target libso\"); this.is_can_hook = true; } } }, onLeave: function (retval) { if (this.is_can_hook) { target_func(soName); } } } ); } ","date":"2025.1.31","objectID":"/blog/posts/android/android-reverse/:5:3","tags":["reverse","android"],"title":"Android Reverse","uri":"/blog/posts/android/android-reverse/"},{"categories":["config"],"content":"记录一下 docker 的安装和使用 ","date":"2025.1.20","objectID":"/blog/posts/config/docker/:0:0","tags":["config"],"title":"Docker","uri":"/blog/posts/config/docker/"},{"categories":["config"],"content":"Docker 安装\r这里就是根据 Win11 安装 Docker Desktop 进行配置，安装了 wsl2 和 docker，然后进行了位置的迁移。不过这里注意，新版的 docker 只有 docker-desktop 了，所以文章中对于 docker-desktop-data 的操作可以不用管，具体可以看 官方网址。 同时在上面进行配置后，docker-desktop 会形成一个 ext4.vhdx 文件，表示存储的硬盘。但是之后在 Docker Desktop 的 Resources 中更换 Disk image 位置时，会将之前的 ext4.vhdx 文件重新组织到 DockerDesktopWSL/main 目录下，然后在 DockerDesktopWSL/disk 目录下生成一个 docker_data.vhdx 文件。猜测这里不进行迁移也行，修改这里就会自己进行位置迁移。 20250120220914\r","date":"2025.1.20","objectID":"/blog/posts/config/docker/:1:0","tags":["config"],"title":"Docker","uri":"/blog/posts/config/docker/"},{"categories":["config"],"content":"Docker 镜像\r由于 Docker 的镜像拉去被墙了，所以需要别的方法来获取，这里就是根据 Docker 停服了怎么办 来进行的操作。在 Docker Desktop 的 Docker Engine 中进行修改，不过随时有可能没了。 同时可以自己写 Dockerfile 来创建镜像。下面就是通过 Dockerfile 来构建一个 docker 镜像的例子，这里 RUN 属性只在创建容器时才会进行调用，而 CMD 属性则在使用容器时就会被调用。 # Use an official Python runtime as a parent image FROM python:3.9 # Set the working directory in the container WORKDIR /app # Install curl or wget, if not already available RUN apt-get update \u0026\u0026 apt-get install -y curl # Copy the current directory contents into the container at /usr/src/app COPY . /app # Install any needed packages specified in requirements.txt RUN pip install --no-cache-dir -r /app/requirements.txt # Download and extract the Linux kernel source RUN curl -L https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.14.tar.xz | tar -xJ # Run bash when the container launches CMD [\"/bin/bash\"] ","date":"2025.1.20","objectID":"/blog/posts/config/docker/:2:0","tags":["config"],"title":"Docker","uri":"/blog/posts/config/docker/"},{"categories":["config"],"content":"Docker 使用\r","date":"2025.1.20","objectID":"/blog/posts/config/docker/:3:0","tags":["config"],"title":"Docker","uri":"/blog/posts/config/docker/"},{"categories":["config"],"content":"镜像操作\rdocker pull \u003c镜像名\u003e:\u003c标签\u003e：拉取镜像 docker images：列出本地所有的 Docker 镜像 docker rmi \u003c镜像ID或镜像名\u003e：删除本地的一个镜像 ","date":"2025.1.20","objectID":"/blog/posts/config/docker/:3:1","tags":["config"],"title":"Docker","uri":"/blog/posts/config/docker/"},{"categories":["config"],"content":"容器操作\rdocker run [选项] \u003c镜像名\u003e:\u003c标签\u003e：从镜像启动一个容器 -i：交互模式 -t：分配一个伪终端 -d：后台运行 -p：端口映射（如 -p 80:80） -v：挂载卷（如 -v /host/path:/container/path） docker ps：列出当前正在运行的容器 docker ps -a：列出所有容器（包括已停止的） docker stop \u003c容器ID或容器名\u003e：停止一个运行中的容器 docker start \u003c容器ID或容器名\u003e：启动一个已停止的容器 docker rm \u003c容器ID或容器名\u003e：删除一个已停止的容器 docker exec -it \u003c容器ID或容器名\u003e /bin/bash：进入一个正在运行的容器并启动交互式终端 ","date":"2025.1.20","objectID":"/blog/posts/config/docker/:3:2","tags":["config"],"title":"Docker","uri":"/blog/posts/config/docker/"},{"categories":["config"],"content":"Docker-Compose\r","date":"2025.1.20","objectID":"/blog/posts/config/docker/:4:0","tags":["config"],"title":"Docker","uri":"/blog/posts/config/docker/"},{"categories":["config"],"content":"功能定位\rDocker 命令：Docker 命令主要用于管理单个容器的生命周期，例如启动、停止、删除容器等。它适合简单的单容器场景，灵活性高，但需要手动管理容器之间的依赖关系和配置。 Docker Compose：Docker Compose 用于定义和管理多容器应用程序，适合复杂的应用场景（如微服务架构）。它通过一个 docker-compose.yml 文件集中管理服务、网络、卷等，简化了多容器的配置和部署。 简单而言，docker 适合管理单一容器，而 docker-compose 适合管理多容器，其包含容器的依赖、网络、数据卷、环境变量等配置管理。 ","date":"2025.1.20","objectID":"/blog/posts/config/docker/:4:1","tags":["config"],"title":"Docker","uri":"/blog/posts/config/docker/"},{"categories":["config"],"content":"使用方式\rDocker 命令\r需要手动编写和执行命令。例如，启动一个 Nginx 容器： docker run -d --name web -p 80:80 nginx 对于多容器场景 需要分别启动每个容器，并手动处理它们之间的依赖关系。例如，如果容器 A 依赖容器 B，必须先启动容器 B，再启动容器 A； 需要手动创建和管理网络，并将容器连接到网络； 手动创建和管理数据卷，并将其挂载到容器； 需要手动在命令行中指定环境变量。 Docker Compose\r使用 docker-compose.yml 文件定义服务，然后在该文件目录下使用 docker-compose up 就可以启动了。举例如下： # 指定 Docker Compose 文件的版本为 3.1，在较新的 docker 版本中不再需要了，可以删除 # version: '3.1' # 定义服务（容器） services: # 定义名为 db 的服务 db: image: mysql # 使用 MySQL 官方镜像 # 设置环境变量 environment: MYSQL_ROOT_PASSWORD: example # 设置 MySQL 的 root 用户密码为 example # 定义数据卷 volumes: - mydata:/var/lib/mysql # 将名为 mydata 的数据卷挂载到容器的 /var/lib/mysql 目录，用于持久化 MySQL 数据 # 健康检查配置 healthcheck: # 使用 mysqladmin ping 命令检查 MySQL 是否健康 test: [\"CMD\", \"mysqladmin\", \"ping\", \"-h\", \"localhost\"] interval: 5s # 每 5 秒检查一次 timeout: 10s # 每次检查的超时时间为 10 秒 retries: 5 # 重试 5 次后标记为不健康 # 定义网络 networks: - mynet # 将 db 服务连接到名为 mynet 的网络 # 定义名为 web 的服务 web: image: nginx # 使用 Nginx 官方镜像 # 定义依赖关系，web 服务依赖于 db 服务 depends_on: db: condition: service_healthy # 确保 db 服务健康后再启动 web 服务 # 定义端口映射 ports: - \"8080:80\" # 将主机的 8080 端口映射到容器的 80 端口 # 定义网络 networks: - mynet # 将 web 服务连接到名为 mynet 的网络 # 设置重启策略为 always，确保容器在异常退出后自动重启 restart: always # 定义数据卷 volumes: mydata: # 定义名为 mydata 的数据卷，用于持久化 MySQL 数据 # 定义网络 networks: mynet: # 定义名为 mynet 的网络 driver: bridge # 使用 bridge 驱动创建网络 ","date":"2025.1.20","objectID":"/blog/posts/config/docker/:4:2","tags":["config"],"title":"Docker","uri":"/blog/posts/config/docker/"},{"categories":["config"],"content":"命令对比\r功能 Docker 命令 Docker Compose 命令 启动容器 docker run docker-compose up 停止容器 docker stop docker-compose stop 查看日志 docker logs docker-compose logs 构建镜像 docker build docker-compose build 查看运行中的容器 docker ps docker-compose ps 进入容器 docker exec -it docker-compose exec 删除容器 docker rm docker-compose rm 这里 docker-compose down 会停止并删除容器，之后再启动就会根据之前的镜像来创建容器了。 ","date":"2025.1.20","objectID":"/blog/posts/config/docker/:4:3","tags":["config"],"title":"Docker","uri":"/blog/posts/config/docker/"},{"categories":["AI"],"content":"这里主要记录在大模型原理和技术课程中的学习。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:0:0","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"语言模型基础\r语言是概率的。并且，语言的概率性与认知的概率性存在着密不可分的关系。语言模型（Language Models, LMs）旨在准确预测语言符号的概率。，语言模型经历了从规则模型到统计模型，再到神经网络模型的发展历程，逐步从呆板的机械式问答程序成长为具有强大泛化能力的多任务智能模型。下面按照语言模型发展的顺序依次进行讲解。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:1:0","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"基于统计方法的语言模型\rn-grams 语言模型\r定义：n-grams 是一种基于统计的语言模型，通过统计语料库中词序列的频率来预测语言符号的概率。 计算方法：n-grams 模型通过计算词序列的相对频率来预测文本的概率。公式为： $$ P_{n-grams}(w_{1:N}) = \\prod_{i=n}^{N} \\frac{C(w_{i-n+1}:i)}{C(w_{i-n+1}:i-1)} $$ 其中，$C(w_{i-n+1}:i)$ 是词序列在语料库中出现的次数。 马尔可夫假设：n-grams 模型基于 n 阶马尔可夫假设，即当前词的概率只与前 n 个词有关。 零概率问题：当 n 较大时，语料库中可能找不到与 n-gram 完全匹配的词序列，导致零概率问题。可以通过平滑技术来缓解。 n-grams 的统计学原理\r极大似然估计：n-grams 模型通过极大似然估计来近似词序列的概率。具体来说，n-grams 模型通过统计词序列的频率来估计条件概率 $P(w_i|w_{i-n+1}:i-1)$。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:1:1","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"基于 RNN 的语言模型\rRNN 的基本原理\r循环传播范式：RNN 通过环路将历史状态叠加到当前状态上，从而能够基于历史信息进行预测。 隐状态：RNN 的隐状态 $h_t$ 通过以下公式计算： $$ h_t = g(W_H h_{t-1} + W_I x_t) $$ 其中，$g(\\cdot)$ 是激活函数，$W_H$ 和 $W_I$ 是权重矩阵。 梯度消失与爆炸：RNN 在训练过程中容易遇到梯度消失或梯度爆炸问题，可以通过 GRU 或 LSTM 等门控结构来缓解。 基于 RNN 的语言模型\r预测下一个词：基于 RNN 的语言模型通过当前词和隐状态来预测下一个词的概率： $$ P(w_{i+1}|w_{1:i}) = P(w_{i+1}|w_i, h_{i-1}) $$ 损失函数：通常使用交叉熵损失函数来训练 RNN 语言模型： $$ l_{CE}(o_i) = -\\log o_i [w_{i+1}] $$ Teacher Forcing：在训练过程中，使用标准答案作为下一轮的输入，以减少错误级联放大问题。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:1:2","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"基于 Transformer 的语言模型\rTransformer 的基本原理\r注意力机制：Transformer 通过自注意力机制将前文信息叠加到当前状态上。自注意力层的输出为： $$ Attention(x_t) = \\sum_{i=1}^{t} \\alpha_{t,i} v_i $$ 其中，$\\alpha_{t,i}$ 是注意力权重。 全连接前馈层：全连接前馈层负责记忆存储，公式为： $$ FFN(v) = \\max(0, W_1 v + b_1) W_2 + b_2 $$ 层正则化与残差连接：层正则化用于加速训练，残差连接用于缓解梯度消失问题。 基于 Transformer 的语言模型\r下一词预测：Transformer 语言模型通过上文预测下一个词的概率： $$ P(w_{i+1}|w_{1:i}) = o_i [w_{i+1}] $$ 损失函数：同样使用交叉熵损失函数进行训练。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:1:3","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"语言模型的采样方法\r概率最大化方法： 贪心搜索：每轮选择概率最大的词，容易陷入局部最优。 波束搜索：每轮保留多个候选词，最终选择联合概率最大的词序列。 随机采样方法： Top-K 采样：每轮选择概率最高的 K 个词，然后根据概率分布进行随机采样。 Top-P 采样：设定概率阈值 p，选择累积概率超过 p 的词进行采样。 Temperature 机制：通过调整 Temperature 参数来控制采样的随机性。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:1:4","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"语言模型的评测\r内在评测： 困惑度（Perplexity）：衡量语言模型对测试文本的“困惑”程度，困惑度越低，模型性能越好。 $$ PPL(s_{test}) = P(w_{1:N})^{-\\frac{1}{N}} $$ 外在评测： 基于统计指标的评测：如 BLEU 和 ROUGE，通过计算生成文本与标准答案的重合程度来评估模型性能。 基于语言模型的评测：如 BERTScore 和 G-EVAL，利用语言模型的上下文词嵌入或生成能力来评估生成文本的质量。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:1:5","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"大语言模型架构\r","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:2:0","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"大数据与大模型\r大数据与大模型的关系： 数据规模的增长为模型提供了更丰富的信息源，模型规模的扩大增加了模型的表达能力。 模型规模和数据规模的增长共同作用，提升了模型的性能和新功能的涌现。 扩展法则： Kaplan-McCandlish 扩展法则：模型性能与模型规模和数据规模高度正相关，模型规模的增长速度应略快于数据规模。 Chinchilla 扩展法则：数据集规模与模型规模同等重要，理想的数据集大小应为模型规模的20倍。 涌现能力： 随着模型规模和数据规模的提升，大语言模型涌现出新的能力，如上下文学习、常识推理、代码生成、逻辑推理等。 这些能力并非通过特定任务训练获得，而是随着模型复杂度的提升自然涌现。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:2:1","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"大语言模型架构概览\r主流模型架构\rEncoder-only 架构：仅使用编码器，适合自然语言理解任务，如文本分类、情感分析等。 Encoder-Decoder 架构：包含编码器和解码器，适合序列到序列任务，如机器翻译、文本摘要等。 Decoder-only 架构：仅使用解码器，适合开放式生成任务，如文本生成、故事生成等。 注意力矩阵\rEncoder-only：双向注意力，捕捉输入序列中所有Token的关系。 Encoder-Decoder：结合编码器的双向注意力和解码器的单向注意力，适合复杂生成任务。 Decoder-only：单向注意力，适合自回归生成任务。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:2:2","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"基于 Encoder-only 架构\rBERT 模型\r结构：基于 Transformer 编码器，包含多层自注意力和前馈网络。 预训练任务：掩码语言建模（MLM）和下文预测（NSP）。 下游任务：文本分类、问答系统、语义相似度计算等。 BERT 衍生模型\rRoBERTa：优化了 BERT 的预训练过程，使用更大的数据集和动态掩码。 ALBERT：通过参数共享和嵌入分解减少参数量，提升训练效率。 ELECTRA：引入生成器-判别器架构，提升训练效率和下游任务表现。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:2:3","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"基于 Encoder-Decoder 架构\rT5 模型\r结构：统一的文本到文本生成框架，适合多种 NLP 任务。 预训练任务：Span Corruption，预测被掩码的连续文本片段。 下游任务：通过 Prompt 工程和微调适配多种任务。 BART 模型\r结构：结合编码器和解码器，适合文本生成和理解任务。 预训练任务：多种文本破坏任务，如 Token 遮挡、句子打乱等。 下游任务：文本生成、文本摘要、问答系统等。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:2:4","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"基于 Decoder-only 架构\rGPT 系列\rGPT-1：初代 Decoder-only 模型，使用下一词预测任务进行预训练。 GPT-2：增加模型规模和预训练数据，提升任务泛化能力。 GPT-3：大幅增加模型规模和数据规模，涌现出上下文学习能力。 ChatGPT 和 GPT-4：引入人类反馈强化学习（RLHF），提升指令跟随能力。 LLaMA 系列\rLLaMA1：小模型+大数据理念，优化了 Transformer 的嵌入和注意力机制。 LLaMA2：增加预训练数据规模，引入人类反馈强化学习。 LLaMA3：大幅增加预训练数据规模，提升跨语言处理能力。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:2:5","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"非 Transformer 架构\r状态空间模型（SSM）： SSM：通过状态变量捕捉系统状态的变化，适合处理长序列数据。 RWKV：结合RNN和Transformer的优点，实现高效的长序列处理。 Mamba：引入选择机制和硬件感知算法，提升长序列处理效率。 测试时训练（TTT）： TTT：在推理阶段动态更新模型参数，提升长上下文建模能力。 优势：线性时间复杂度，适合处理超长序列任务。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:2:6","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"Prompt 工程\r","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:3:0","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"Prompt 工程简介\r定义与背景\rPrompt 工程是指通过精心设计的指令（Prompt）来引导生成式人工智能模型执行特定任务，而无需进行繁琐的微调。它是用户输入给模型的指令或问题，通常以自然语言的形式呈现，目的是引导模型生成符合任务需求的输出。传统的自然语言处理（NLP）研究遵循 “预训练-微调-预测” 范式，即在大规模语料库上进行预训练，然后针对特定任务进行微调，最后在微调后的模型上进行预测。然而，随着大语言模型（LLM）的规模和能力的提升，一种新的范式—— “预训练-提示预测” 应运而生。这种范式通过精心设计的 Prompt 直接引导模型适应下游任务，避免了微调的高昂成本。 核心思想与重要性\rPrompt 工程的核心思想是通过设计合适的 Prompt，将新任务转化为模型在预训练阶段已经熟悉的形式，从而利用模型的泛化能力完成任务。大语言模型在预训练阶段学习了大量的语言知识和任务模式，具备强大的 泛化能力。同时，大语言模型还具备强大的 指令跟随能力，能够根据用户的指令生成符合预期的输出。Prompt 工程通过设计清晰、具体的指令，确保模型能够准确理解任务需求并生成高质量的输出。 Prompt 工程的重要性体现在多个方面。首先，它避免了传统微调方法带来的高昂成本。传统的微调方法需要针对每个任务进行特定的训练，耗费大量的计算资源和时间。而 Prompt 工程通过设计有效的 Prompt，避免了微调带来的高昂成本。其次，Prompt 工程提供了一种灵活的方式来执行各种任务，用户只需通过修改 Prompt 即可引导模型适应不同的任务需求。最后，Prompt 工程在文本生成、情感分析、问答系统、代码生成等多个领域都有广泛的应用，能够显著提升模型的性能。 关键要素\rPrompt 工程的成功依赖于对模型和任务的深入理解。一个有效的 Prompt 通常由四个关键要素组成：任务说明、上下文、问题和输出格式。任务说明用于明确描述模型需要完成的任务，确保模型理解任务的核心要求。上下文则提供与任务相关的背景信息或示例，帮助模型更好地理解任务。问题是用户的具体问题或需要处理的信息，为模型提供明确的起点。输出格式则指定模型输出的格式，如 JSON、XML 等，确保输出结构化和易于处理。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:3:1","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"上下文学习\r定义与核心思想\r上下文学习（In-Context Learning, ICL）是一种通过构造特定的 Prompt 来引导大语言模型理解并学习下游任务的范式。这些特定的 Prompt 中通常包含任务说明和一系列示例，模型能够从这些上下文信息中学习任务的逻辑和规则，从而在没有额外训练的情况下生成符合任务要求的输出。上下文学习的核心思想是利用大语言模型在预训练阶段学到的泛化能力，通过提供任务相关的上下文信息，使模型能够快速适应新的任务。 与传统的微调方法不同，上下文学习不需要对模型进行额外的训练，而是通过设计有效的 Prompt 来引导模型完成任务。这种方法特别适用于需要快速适应新任务的场景，例如用户通过 API 或页面与大语言模型进行交互时。上下文学习的出现为 “语言模型即服务”（LLM as a Service）模式奠定了坚实的基础。 形式与示例选择\r上下文学习可以根据提供的示例数量分为三种形式：零样本（Zero-shot）、单样本（One-shot）和少样本（Few-shot）。零样本上下文学习仅提供任务说明，不提供任何示例，依赖模型的泛化能力完成任务。单样本上下文学习提供一个示例，帮助模型理解任务的基本模式。少样本上下文学习则提供少量示例（通常几个至十几个），显著提升模型在特定任务上的表现。 示例的选择对上下文学习的效果至关重要。示例的选择主要依赖于相似性和多样性。相似性是指选择与待解决问题最为相近的示例，帮助模型更好地理解任务。多样性则要求示例涵盖尽量广的内容，扩大示例对任务的覆盖范围，增强模型应对各种问题的能力。常见的示例选择方法包括直接检索、聚类检索和迭代检索。直接检索根据相似性选择示例，聚类检索通过聚类保证示例的多样性，而迭代检索则兼顾相似性和多样性，动态选择示例。 影响因素与优化策略\r上下文学习的性能受到多种因素的影响，包括预训练数据、模型规模以及演示示例的质量。预训练数据的领域丰富度和任务多样性直接影响模型的上下文学习能力。模型规模越大，上下文学习的能力通常越强，尤其是在参数数量达到亿级别及以上的模型中，上下文学习的效果更为显著。 演示示例的质量也对上下文学习的效果有重要影响。示例的格式、输入-输出映射的正确性、示例数量和顺序都会影响模型的表现。例如，在复杂推理任务中，使用思维链（Chain of Thought, CoT）形式的示例可以显著提升模型的推理能力。此外，增加示例数量通常能够提升上下文学习的性能，但随着示例数量的增多，性能提升的速率会逐渐减缓。 为了优化上下文学习的效果，可以采取以下策略： 设计清晰的 Prompt：确保任务说明明确，上下文信息丰富且相关，输出格式规范。 选择合适的示例：根据任务需求选择相似性和多样性兼备的示例，帮助模型更好地理解任务。 调整示例顺序：不同的示例顺序可能对模型的表现产生显著影响，可以通过实验找到最优的示例顺序。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:3:2","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"思维链\r定义与核心思想\r思维链（Chain of Thought, CoT）是一种通过模拟人类解决复杂问题时的思考过程，引导大语言模型生成中间推理步骤的 Prompt 范式。它的核心思想是通过在 Prompt 中嵌入逐步推理的示例或提示，帮助模型在生成答案的过程中引入逻辑连贯的中间步骤，从而显著提升模型在复杂推理任务中的表现。 传统的语言模型在处理简单任务（如文本分类、情感分析）时表现良好，但在面对需要复杂推理的任务（如算术求解、常识判断、符号推理）时，往往表现不佳。这种现象被称为“Flat Scaling Curves”，即模型规模的扩大并未带来预期的性能提升。思维链技术的出现解决了这一问题，它通过引导模型逐步推理，显著提升了模型在复杂任务中的表现。 思维链的核心在于构造合适的 Prompt，以触发模型生成推理路径。早期的方法通过在 Prompt 中加入少量包含推理过程的示例（Few-Shot Demonstrations），引导模型模仿这些示例逐步生成答案。随着技术的发展，思维链衍生出了多种变体，如 Zero-Shot CoT、Auto-CoT、思维树（Tree of Thoughts, ToT）和思维图（Graph of Thoughts, GoT）等。 主要模式与应用场景\r思维链技术可以根据推理方式的不同分为三种主要模式：按部就班、三思后行和集思广益。 按部就班： 特点：模型逐步推理，逻辑连贯，推理路径形成一条清晰的链条。 代表方法：CoT、Zero-Shot CoT、Auto-CoT。 应用场景：适用于需要逐步推理的任务，如算术求解、逻辑推理等。 三思后行： 特点：模型在每一步推理中评估当前情况，选择最佳推理方向，允许回溯和重新选择。 代表方法：思维树（ToT）、思维图（GoT）。 应用场景：适用于需要探索多种推理路径的任务，如创意写作、复杂问题求解等。 集思广益： 特点：模型生成多条推理路径，整合后得到最优答案。 代表方法：Self-Consistency。 应用场景：适用于需要高准确性和可靠性的任务，如代码生成、复杂决策等。 优化策略与未来方向\r思维链技术的成功依赖于 Prompt 的设计和模型的推理能力。为了进一步提升思维链的效果，可以采取以下优化策略： 调整推理详细程度：根据任务需求调整思维链的详细程度。对于简单任务，可以直接给出最终答案；对于复杂任务，则需要展示完整的推理步骤。 结合多种模式：根据任务特点灵活选择思维链的模式。例如，在需要高准确性的任务中使用 Self-Consistency，在需要创意的任务中使用 ToT 或 GoT。 优化示例选择：选择与任务高度相关的示例，确保示例的多样性和代表性，帮助模型更好地理解任务。 未来，思维链技术可能会在以下方向进一步发展： 自动化推理：通过自动生成推理路径，减少对人工示例的依赖。 多模态推理：将思维链技术应用于多模态任务，如图像生成、视频理解等。 个性化推理：根据用户需求生成个性化的推理路径，提升用户体验。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:3:3","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"Prompt 技巧\rPrompt 技巧通过 规范 Prompt 编写、合理归纳提问、适时使用思维链 和 善用心理暗示，显著提升了大语言模型的交互效率和输出质量。规范 Prompt 编写确保模型准确理解任务需求，合理归纳提问帮助模型提供更详细、准确的回答，适时使用思维链提升模型在复杂推理任务中的表现，善用心理暗示则通过角色扮演和情景代入生成更符合预期的内容。这些技巧的结合使用，能够最大化地发挥大语言模型的潜力，使其在多样化的应用场景中发挥出卓越的性能。 规范 Prompt 编写\r编写规范的 Prompt 是确保大语言模型生成高质量输出的基础。一个有效的 Prompt 通常由四个关键要素组成：任务说明、上下文、问题和输出格式。任务说明用于明确描述模型需要完成的任务，确保模型理解任务的核心要求。上下文则提供与任务相关的背景信息或示例，帮助模型更好地理解任务。问题是用户的具体问题或需要处理的信息，为模型提供明确的起点。输出格式则指定模型输出的格式，如 JSON、XML 等，确保输出结构化和易于处理。 在设计 Prompt 时，任务说明应尽量明确和具体，避免模糊不清的描述。例如，在情感分类任务中，任务说明“判断下面句子的情感为积极还是消极”比“分类下面的句子”更加清晰。上下文应丰富且相关，避免包含冗余或不必要的信息。输出格式应规范，确保模型生成的输出易于解析和处理。此外，Prompt 的排版也非常重要，使用分隔符、空白和缩进可以增强 Prompt 的可读性，帮助模型更好地理解任务。 合理归纳提问\r在与大语言模型的交互中，提问的质量直接影响到信息触达的效率和深度。一个精心设计的提问不仅能够明确表达需求，还能引导模型聚焦于问题的核心，从而获得精准且有价值的答案。合理归纳提问的策略主要包括复杂问题拆解和追问。 复杂问题拆解： 特点：将复杂问题分解为多个子问题，逐步引导模型回答。 应用场景：适用于需要深入分析的复杂任务，如数据分析、决策支持等。 追问： 特点：通过连续提问引导模型提供更详细、准确的回答。 形式： 深入追问：挖掘特定话题的深层信息。 扩展追问：拓宽讨论的广度，收集更多相关信息。 反馈追问：指出模型输出中的错误或不足，请求更正或澄清。 适时使用思维链（CoT）\r思维链技术（Chain of Thought, CoT）在处理涉及复杂推理的任务时非常有效。通过引导模型生成中间推理步骤，可以显著提升模型在算术、常识判断和符号推理等任务中的表现。然而，并非所有任务都适合使用 CoT，因此需要根据任务类别、模型规模和模型能力来决定是否使用 CoT。 任务类别： 适合使用 CoT 的任务：算术求解、逻辑推理、符号推理等需要复杂推理的任务。 不适合使用 CoT 的任务：情感分类、简单问答等任务，标准的 Prompt 方法已足够有效。 模型规模： 适合使用 CoT 的模型：参数量超过千亿的巨型模型，如 GPT-4、PaLM 等。 不适合使用 CoT 的模型：规模较小的模型可能生成逻辑不连贯的思维链，导致结果不准确。 模型能力： 未经推理微调的模型：适合使用 CoT，能够显著提升推理能力。 经过推理微调的模型：如 ChatGPT、GPT-4，可能已经内化了 CoT 推理路径，无需显式使用 CoT。 善用心理暗示\r心理暗示是一种通过角色扮演和情景代入的方式，引导大语言模型生成更符合预期的内容的技术。通过为大语言模型设定特定的角色或情景，可以显著提升模型在特定任务中的表现。 角色扮演： 特点：通过设定特定角色，引导模型生成更符合角色需求的内容。 情景代入： 特点：将模型置于特定情境中，生成更具深度和情感共鸣的回答。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:3:4","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"相关应用\rPrompt 工程在多个领域展现了广泛的应用前景。基于大语言模型的 Agent 通过角色扮演、记忆检索、任务规划和行动执行，能够自主完成复杂任务。数据合成技术通过生成高质量的训练数据，缓解了数据资源枯竭的问题。Text-to-SQL 技术通过自然语言查询生成 SQL 语句，降低了数据查询的门槛。GPTs 则通过自定义 Prompt 和工具，实现了个性化的模型定制。这些应用场景充分展示了 Prompt 工程在提升大语言模型交互和执行能力方面的独特优势和广阔前景。 基于大模型的 Agent\r基于大语言模型的智能体（Agent）是一种能够自主感知环境并采取行动以实现特定目标的实体。Agent 的核心能力包括规划、决策、工具调用等复杂操作，这些操作依赖于 Prompt 工程技术的支持。Agent 的框架通常由四个模块组成：配置模块、记忆模块、计划模块和行动模块。 配置模块：通过角色扮演技术定义 Agent 的角色，设定 Agent 的背景、技能和职责。这些角色设定信息以上下文的形式嵌入到每次交互的 Prompt 中，确保 Agent 的行为符合角色需求。 记忆模块：存储 Agent 的知识与交互记忆，通过检索增强技术获取相关记忆。记忆模块利用上下文学习技术构造和优化查询，帮助 Agent 更精准地检索到相关记忆。 计划模块：将复杂任务分解为多个子任务，通过思维链技术进行任务规划。计划模块利用少样本示例调整子任务的粒度，确保任务流程的顺畅与高效。 行动模块：将计划模块生成的计划转化为具体的行动步骤，并借助外部工具执行这些步骤。行动模块通过调用 API 接口的示例作为上下文，生成调用 API 的代码并执行。 数据合成\r数据合成是通过大语言模型生成高质量训练数据的技术，能够缓解高质量数据资源枯竭的问题。数据合成的代表性方法是 Self-Instruct，它通过多步骤调用大语言模型，生成大量丰富且多样化的指令数据。 构建任务池：人工设计少量指令数据作为初始任务池。 指令生成：从任务池中随机抽取示例，生成新的指令。 指令分类：将生成的指令分类为分类任务或生成任务。 数据生成：根据指令生成输入和回答部分。 数据过滤：去除低质量或重复的数据，确保生成的数据质量。 数据合成的意义在于，它不仅能够缓解高质量数据资源的枯竭问题，还能通过生成多样化的数据集，提高模型的泛化能力和鲁棒性。此外，数据合成还能在保护隐私的前提下，为特定领域的垂直数据提供有效的补充。 Text-to-SQL\rText-to-SQL 技术将自然语言查询翻译成可以在数据库中执行的 SQL 语句，是实现零代码或低代码数据查询的有效途径。传统的 Text-to-SQL 方法需要大量训练数据，而基于大语言模型的零样本 Text-to-SQL 方法则通过 Prompt 工程技术优化生成效果。 清晰提示（Clear Prompting）： 清晰布局：通过明确符号划分指令、上下文和问题，确保指令模板清晰。 清晰上下文：设计零样本 Prompt，指示模型从数据库中召回与问题相关的表和列。 提示校准（Calibration with Hints）：通过角色扮演技术校准模型的偏差，确保生成的 SQL 语句准确。 一致输出（Consistent Output）：使用 Self-Consistency 方法对多种推理路径进行采样，选择一致的答案。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:3:5","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"参数高效微调\r","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:4:0","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"简介\r参数高效微调（Parameter-Efficient Fine-Tuning, PEFT） 是一种针对大语言模型在垂直领域适配的技术。由于大语言模型在预训练阶段通常使用海量数据，但在某些垂直领域（如医学、金融、法学等）中，预训练数据涉及较少，导致模型在这些领域的表现不佳。为了提升模型在这些领域的性能，通常需要对其进行微调。然而，大语言模型的参数量巨大，全量微调的成本高昂，尤其是在计算资源和存储资源有限的情况下，难以实现。因此，参数高效微调技术应运而生，旨在通过减少需要更新的参数量，降低微调成本，同时保持模型的性能。 在介绍参数高效微调之前，首先需要回顾两种常见的下游任务适配方法：上下文学习和指令微调。上下文学习通过设计提示词（Prompt）来驱动模型完成任务，无需更新模型参数，具有快速适配的优势。然而，上下文学习的性能通常不如微调，且提示词的设计需要大量人工成本，不同提示词的效果差异较大。此外，随着提示词中样例的增加，推理阶段的计算代价也会显著增加。指令微调则是通过构建指令数据集，对模型进行监督微调，使其能够更好地理解和执行各种自然语言处理任务。尽管指令微调能够显著提升模型在下游任务上的性能，但其计算资源需求较高，尤其是在大模型上，全量微调需要大量的内存和计算资源，普通消费级GPU难以胜任。 为了克服上下文学习和指令微调的局限性，参数高效微调技术通过仅更新 模型的一小部分参数，显著降低了微调的计算和存储成本。主流的参数高效微调方法可以分为三类：参数附加方法、参数选择方法 和 低秩适配方法。参数附加方法通过在模型中添加新的、较小的可训练模块（如适配器层）来实现高效微调；参数选择方法则通过选择性地更新模型中的部分参数（如偏置项或特定层的参数）来减少计算负担；低秩适配方法则通过低秩矩阵近似原始权重更新矩阵，仅微调低秩矩阵，从而大幅减少参数量。 参数高效微调的优势主要体现在以下几个方面：1) 计算效率高：由于仅更新少量参数，PEFT显著降低了训练时的计算资源消耗；2) 存储效率高：通过减少需要微调的参数量，PEFT降低了微调模型的存储需求，特别适用于内存受限的设备；3) 适应性强：PEFT能够快速适应不同任务，而无需重新训练整个模型，使得模型在面对变化环境时具有更高的灵活性。 总的来说，参数高效微调技术为大语言模型在垂直领域的应用提供了一种高效、低成本的解决方案，能够在保证性能的同时，显著降低微调的计算和存储成本。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:4:1","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"参数附加方法\r参数附加方法（Additional Parameter Methods）是参数高效微调（PEFT）中的一类重要技术，通过在模型中添加新的、较小的可训练模块来实现高效微调。这类方法的核心思想是冻结原始模型的参数，仅微调新加入的模块，从而显著减少需要更新的参数量。根据附加参数的位置不同，参数附加方法可以分为三类：加在输入、加在模型以及加在输出。每一类方法都有其独特的实现方式和优势。 加在输入\r加在输入的方法主要通过在模型的输入嵌入中引入额外的可训练参数来实现微调。最具代表性的方法是 Prompt-tuning。Prompt-tuning 在模型的输入中引入可微分的连续张量，通常称为 软提示（Soft prompt）。软提示作为输入的一部分，与实际的文本数据一起被送入模型。在微调过程中，仅软提示的参数会被更新，其他参数保持不变，从而实现参数高效微调。 具体实现步骤如下： 输入嵌入：给定一个包含 $ n $ 个 token 的输入文本序列 ${w_1, w_2, \\ldots, w_n}$，首先通过嵌入层将其转化为输入嵌入矩阵 $ X \\in R^{n \\times d} $，其中 $ d $ 是嵌入空间的维度。 软提示嵌入：新加入的软提示参数被表示为软提示嵌入矩阵 $ P \\in R^{m \\times d} $，其中 $ m $ 是软提示长度。 拼接输入：将软提示嵌入与输入嵌入矩阵拼接，形成一个新的矩阵 $[P; X] \\in R^{(m+n) \\times d} $，然后输入到 Transformer 模型中进行处理。 训练过程：通过反向传播最大化输出概率似然进行模型训练，仅更新软提示参数 $ P $。 优势： 内存效率高：Prompt-tuning 显著降低了内存需求。例如，T5-XXL 模型对于特定任务的模型需要 11B 参数，但经过 Prompt-tuning 的模型只需要 20480 个参数（假设软提示长度为 5）。 多任务能力：可以使用单一冻结模型进行多任务适应。传统的模型微调需要为每个下游任务学习并保存任务特定的完整预训练模型副本，而 Prompt-tuning 只需要为每个任务存储一个特定的小的任务提示模块。 缩放特性：随着模型参数量的增加，Prompt-tuning 的性能会逐渐增强，并且在 10B 参数量下的性能接近全参数微调的性能。 加在模型\r加在模型的方法通过在预训练模型的隐藏层中插入额外的参数或模块来实现微调。代表性的方法包括 Prefix-tuning、Adapter-tuning 和 AdapterFusion。 Prefix-tuning\rPrefix-tuning 与 Prompt-tuning 类似，但它不仅将软提示添加到输入嵌入中，还将一系列连续的可训练前缀（Prefixes，即 Soft-prompt）插入到输入嵌入以及 Transformer 注意力模块中。具体来说，Prefix-tuning 引入了一组可学习的向量 $ P_k $ 和 $ P_v $，这些向量被添加到所有 Transformer 注意力模块中的键 $ K $ 和值 $ V $ 之前。 优势： 参数效率：只有前缀参数在微调过程中被更新，显著减少了训练参数量。 任务适应性：前缀参数可以针对不同的下游任务进行定制，微调方式灵活。 保持预训练知识：由于预训练模型的原始参数保持不变，Prefix-tuning 能够保留预训练过程中学到的知识。 Adapter-tuning\rAdapter-tuning 向预训练语言模型中插入新的可学习的神经网络模块，称为 适配器（Adapter）。适配器模块通常采用瓶颈结构，即一个上投影层、一个非线性映射和一个下投影层组成的全连接模块。适配器被插入到 Transformer 的每一个多头注意力层和全连接层之后。 优势： 参数效率：适配器模块的参数数量远小于原始模型的参数量，显著减少了微调时的计算和存储需求。 灵活性：适配器模块可以设计得更为复杂，例如使用多个投影层或不同的激活函数和参数初始化策略。 AdapterFusion\rAdapterFusion 是一种两阶段学习的方法，先学习多个任务，对每个任务进行知识提取；再“融合”来自多个任务的知识。具体步骤如下： 知识提取：对每个任务分别训练适配器模块，用于学习特定任务的知识。 知识组合：将不同任务的适配器模块进行融合，以实现知识组合。 优势： 任务泛化：通过融合多个任务的适配器模块，AdapterFusion 能够实现任务泛化，提升模型在新任务上的表现。 加在输出\r加在输出的方法主要通过在模型的输出端进行调整来实现微调，代表性的方法是 代理微调（Proxy-tuning）。代理微调提供了一种轻量级的解码时算法，允许在不直接修改大语言模型权重的前提下，通过仅访问模型输出词汇表预测分布，来实现对大语言模型的进一步定制化调整。具体来说，代理微调通过计算专家模型和反专家模型之间的 logits 分布差异，然后将其加到代理模型的下一个词预测的 logits 分布中。 优势： 计算成本低：通过代理微调，可以将较小模型中学习到的知识迁移到更大的模型中，大幅节省了计算成本。 黑盒模型适用：由于仅需要获取模型的输出分布，而不需要原始的模型权重，因此该方法对于黑盒模型同样适用。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:4:2","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"参数选择方法\r参数选择方法（Parameter Selection Methods） 是参数高效微调（PEFT）中的另一类重要技术，其核心思想是 选择性更新模型中的部分参数，而冻结其余参数。与参数附加方法不同，参数选择方法不需要向模型添加额外的参数，从而避免了在推理阶段引入额外的计算成本。参数选择方法通常分为两类：基于规则的方法 和 基于学习的方法。 基于规则的方法\r基于规则的方法通过人工经验或预定义的规则来选择需要更新的参数。最具代表性的方法是 BitFit。 BitFit\rBitFit 是一种简单但有效的参数选择方法，它仅优化神经网络中的每一层的 偏置项（Biases） 以及任务特定的分类头。由于偏置项在模型总参数中所占比例极小（约 0.08%~0.09%），BitFit 具有极高的参数效率。 优势： 参数效率高：BitFit 仅微调少量参数，但在 GLUE Benchmark 等任务上表现与全量微调相当，甚至在某些任务上表现更好。 优化稳定性：BitFit 允许使用更大的学习率，优化过程更加稳定。 局限性： 小模型验证：BitFit 目前仅在小模型（如 BERT、RoBERTa 等）上进行了验证，在更大模型上的性能表现尚不明确。 其他基于规则的方法\r除了 BitFit，还有一些其他基于规则的方法通过仅微调特定的 Transformer 层来提高参数效率。例如： Lee 等人 提出仅对 BERT 和 RoBERTa 的最后四分之一层进行微调，便能实现全量微调 90% 的性能。 Pafi 选择具有最小绝对值的模型参数作为可训练参数。 基于学习的方法\r基于学习的方法在模型训练过程中 自动选择可训练的参数子集。最具代表性的方法是 Child-tuning。 Child-tuning\rChild-tuning 通过梯度掩码矩阵策略实现仅对选中的子网络进行梯度更新，而屏蔽子网络梯度以外的梯度，从而实现对微调参数的选择。具体来说，Child-tuning 引入了一个与模型参数矩阵同维度的 0-1 掩码矩阵 $ M_t $，用于选择每一轮迭代的子网络 $ C_t $，仅更新该子网络的参数。 变体： Child-tuning$_F$：任务无关的变体，通过伯努利分布随机生成子网络掩码。 Child-tuning$_D$：任务驱动的变体，利用费舍尔信息矩阵（FIM）估计特定任务相关参数的重要性，选择具有最高费舍尔信息的参数作为子网络。 优势： 减少计算负担：通过梯度屏蔽减少了计算负担，同时减少了模型的假设空间，降低了过拟合的风险。 任务适应性：Child-tuning 能够改善大语言模型在多种下游任务中的表现，尤其是在训练数据有限的情况下。 局限性： 计算代价：子网络的选择需要额外的计算代价，特别是在任务驱动的变体中，费舍尔信息的计算十分耗时。 其他基于学习的方法\r除了 Child-tuning，还有一些其他基于学习的参数选择方法： FishMask：使用费舍尔信息来计算掩码，并在每个训练周期动态重新计算。 LT-SFT：根据参数重要性，选择在初始微调阶段变化最大的参数子集形成掩码。 SAM：通过二阶逼近方法解析求解优化函数，帮助决定参数掩码。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:4:3","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"低秩适配方法\r低秩适配方法（Low-rank Adaptation Methods） 是参数高效微调（PEFT）中的一类重要技术，其核心思想是通过 低秩矩阵近似原始权重更新矩阵，并仅微调低秩矩阵，从而大幅减少微调时的参数量和内存开销。低秩适配方法的理论基础是 低维固有维度假设，即过参数化模型的固有维度很低，因此可以通过低秩矩阵来近似全参数更新。 LoRA\rLoRA（Low-Rank Adaptation） 是最经典的低秩适配方法，它通过将参数更新矩阵分解为两个低秩矩阵来实现高效微调。 方法实现\r参数分解：给定一个密集神经网络层，其参数矩阵为 $ W_0 \\in R^{d \\times k} $，LoRA 将参数更新矩阵 $ \\Delta W $ 分解为两个低秩矩阵 $ B \\in R^{d \\times r} $ 和 $ A \\in R^{r \\times k} $，其中 $ r \\ll \\min\\{d, k\\} $。 更新公式：更新后的权重矩阵为： $ W = W_0 + \\alpha BA $ 其中，$ \\alpha $ 是缩放因子，用于控制 LoRA 权重的大小。 训练过程：在训练过程中，固定预训练模型的参数，仅微调 $ B $ 和 $ A $ 的参数。 优势： 参数效率高：LoRA 仅微调部分低秩参数，显著减少了训练参数量。例如，在 LLaMA2-7B 模型中，LoRA 微调的参数量不到全量微调的千分之一。 推理延迟低：LoRA 不会增加推理延迟，且具有可插拔性，训练后可以将 LoRA 参数与模型参数分离。 任务泛化能力强：LoRA 插件可以组合使用，实现跨任务泛化。 参数效率分析\r内存使用：LoRA 显著减少了显存使用。例如，在 LLaMA2-7B 模型上，全量微调需要约 60GB 显存，而 LoRA 仅需约 23GB 显存。 训练速度：由于涉及到的参数计算减少，LoRA 的反向传播速度比全量微调快 1.9 倍。 LoRA 相关变体\r尽管 LoRA 在许多下游任务上表现良好，但在复杂任务（如数学推理）上，LoRA 与全量微调之间仍存在性能差距。为了弥补这一差距，许多 LoRA 变体被提出，主要从以下三个角度进行改进： 打破低秩瓶颈：LoRA 的低秩更新特性限制了模型记忆新知识和适应复杂任务的能力。一些方法通过增加 LoRA 的秩或引入高秩更新机制来打破低秩瓶颈。 ReLoRA：通过周期性地将 LoRA 模块合并到大语言模型中，并重新初始化 LoRA 模块和优化器状态，允许模型通过多次低秩更新累积成高秩状态。 动态秩分配：不同层的权重重要性可能不同，因此需要为每个层分配不同的秩。 AdaLoRA：通过奇异值分解（SVD）动态调整不同层中 LoRA 模块的秩，并根据梯度权重乘积大小的移动平均值构造奇异值的重要性得分，对不重要的奇异值进行剪枝。 训练过程优化：LoRA 的收敛速度较慢，且对超参数敏感，容易过拟合。一些方法通过优化训练过程来提升 LoRA 的性能。 DoRA：将预训练权重分解为方向和大小两个组件，并仅对方向组件施加 LoRA 进行参数化，以增强训练稳定性。 基于 LoRA 插件的任务泛化\rLoRA 的可插拔特性使其能够封装为插件，用于多任务学习和任务泛化。它是一个多 LoRA 组合的框架，通过将已学习的 LoRA 模块进行加权组合，实现跨任务泛化。具体步骤如下： 组合阶段：将多个 LoRA 模块通过逐元素线性加权组合为单一模块： $ \\hat{m} = (w_1A_1 + w_2A_2 + \\cdots + w_NA_N) (w_1B_1 + w_2B_2 + \\cdots + w_NB_N) $ 其中，$ w_i $ 是第 $ i $ 个 LoRA 模块的权重。 适应阶段：通过无梯度方法（如 Shiwa）自适应地学习权重组合，以完成对新任务的适应。 优势： 任务泛化能力强：LoRAHub 能够将多个任务的能力迁移到新任务上，提供了一种高效的跨任务学习范式。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:4:4","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["AI"],"content":"实践与应用\r参数高效微调（PEFT） 技术在实际应用中展现了强大的潜力，尤其是在资源受限的环境下，能够显著提升大语言模型在特定任务上的性能。本节将介绍 PEFT 的实践框架及其在不同垂直领域中的应用案例。 PEFT 实践\r在实际应用中，PEFT 技术的实施和优化至关重要。本小节将详细介绍 PEFT 的主流框架、使用方法及相关技巧。 PEFT 主流框架\r目前最流行的 PEFT 框架是由 Hugging Face 开发的开源库 HF-PEFT，它集成了多种先进的微调技术，如 LoRA、Adapter-tuning、Prompt-tuning 和 IA3 等。 HF-PEFT 的优势： 高效灵活性：支持与 Hugging Face 的其他工具（如 Transformers、Diffusers 和 Accelerate）无缝集成，适用于从单机到分布式环境的多样化训练和推理场景。 易用性：提供了详细的文档、快速入门指南和示例代码，帮助用户快速上手。 社区支持：拥有活跃的社区支持，鼓励用户贡献代码和改进。 HF-PEFT 框架使用\r使用 HF-PEFT 框架进行模型微调的步骤如下： 安装与配置：首先安装 HF-PEFT 框架及其依赖项（主要是 Hugging Face 的 Transformers 库）。 选择模型与数据：根据任务需求，挑选合适的预训练模型，并准备相应的训练数据集。 确定微调策略：选择适合任务的微调方法，例如 LoRA 或适配器技术。 模型准备：加载预训练模型并为选定的微调方法进行配置，包括任务类型、推理模式、参数值等。 模型训练：定义完整的训练流程，包括损失函数、优化器设置，并执行训练，同时可应用数据增强和学习率调度等技术。 PEFT 相关技巧\rHF-PEFT 提供了多种参数高效微调技术，以下是一些常用的参数设置方法： Prompt Tuning： num_virtual_tokens：表示软提示的长度，通常设置在 10-20 之间。 prompt_tuning_init：表示 prompt 参数的初始化方式，可以选择随机初始化或文本初始化。 Prefix Tuning： num_virtual_tokens：与 Prompt Tuning 类似，表示构造的 virtual tokens 的数量。 encoder_hidden_size：表示用于 Prefix 编码的多层感知机（MLP）层的大小。 LoRA： r：秩的大小，通常选择较小的值（如 4、8、16）。 lora_alpha：缩放因子，用于控制 LoRA 权重的大小。 lora_dropout：LoRA 层的 dropout 比率，用于正则化防止过拟合。 target_modules：指定模型中 LoRA 要应用的模块，如注意力机制中的 query、key、value 矩阵。 PEFT 应用\rPEFT 技术在多个垂直领域中展现了强大的应用潜力，尤其是在表格数据处理和金融领域的 Text-to-SQL 生成任务中。 表格数据查询\r表格数据查询是处理和分析表格数据的关键步骤，通常需要编写复杂的 SQL 代码。为了降低 SQL 编写的门槛，Text-to-SQL 技术应运而生，它能够将自然语言文本自动翻译成 SQL 代码。 FinSQL 是一个面向金融领域的 Text-to-SQL 框架，其核心思想是通过 PEFT 技术对基座模型进行微调，提升模型在少样本场景下的性能。FinSQL 的框架包括以下三个部分： 提示构造：通过不同策略增强原始数据，并构造高效检索器，检索与用户查询相关的表格和字段。 参数高效微调：采用 LoRAHub 融合多个 LoRA 模块，提高模型在少样本场景下的性能。 输出校准：修正生成 SQL 中的语法错误，并使用 Self-Consistency 方法选择一致性 SQL。 优势： 性能提升：FinSQL 显著提升了金融领域 Text-to-SQL 任务的准确性和效率。 复杂查询处理：FinSQL 在处理复杂金融查询时展现了强大的能力，为金融领域的数据分析和决策支持提供了技术支撑。 表格数据分析\r表格数据的特点（如缺乏局部性、包含多种数据类型）使得传统的深度学习方法难以直接应用。大语言模型的参数中编码了大量先验知识，能够有效弥补表格数据特征不足的问题。 TabLLM 是一个基于大语言模型的少样本表格数据分类框架，其核心思想是将表格数据序列化为自然语言字符串，并附上分类问题的简短描述来提示大语言模型。在少样本设置中，TabLLM 使用 LoRA 对模型进行微调，显著提升了模型在表格数据分类任务中的性能。 优势： 少样本学习能力强：TabLLM 在多个基准数据集上超过了基于深度学习的表格分类基线方法，甚至在少样本设置下超过了梯度提升树等传统基线方法。 稳健性高：由于 PEFT 只微调部分参数，TabLLM 在少量数据上进行微调时不易过拟合，性能更加稳健。 ","date":"2025.1.17","objectID":"/blog/posts/ai/large_language_model/:4:5","tags":["AI"],"title":"Large Language Model","uri":"/blog/posts/ai/large_language_model/"},{"categories":["Static Analysis"],"content":"记录一下对于数据流分析的理解。 注意事项\r下面的内容大部分来自于 数据流分析及其应用，它是对 《软件分析》 课程中相关内容的总结，同时添加 《软件分析技术》 课程的相关内容。 命令式程序的基本构成单元是命令，而程序本身则是通过 顺序、选择和循环 三种控制结构来组织这些命令。在 数据流分析框架 中，主要关注如何对这些控制结构进行抽象，以便于理解和优化程序的行为。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:0:0","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"数据流分析概述\r数据流分析\r简单来说，程序可以看成是状态（数据）和状态之间的转移（控制）两部分，因为状态转移的条件都被忽略了，核心分析的部分是状态数据在转移过程中的变化，所以叫做数据流分析。 上述就是对于数据流分析的简单定义，但是数据流分析在处理复杂程序时面临挑战，因为直接追踪数据流动既复杂又耗时。为了简化这一过程，通常采用近似方法，即对程序进行适当的修改，以便得到一个更易于分析的版本，同时确保分析结果与原始程序保持一致。这种近似方法主要包含两种手段：忽略条件判断和提前合并。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:1:0","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"近似方案\r条件判断\r忽略条件判断\r忽略条件判断的核心思想是进行非确定性抽象，即假设所有分支条件都可能成立，从而生成一个程序行为的超集，确保原始程序的结果包含在抽象程序的结果中。 这种处理方式能够分析程序的所有可能路径，并确保分析结果的正确性。具体来说，有两种常见的抽象方式。 一种是对于分支条件的抽象。对于 if-else 语句，忽略条件判断，直接将分支抽象为非确定性选择。这里的 nondet_choice 表示程序可能执行 stmt1 或 stmt2，从而形成包含所有可能路径的抽象超集。 if (condition) { stmt1; } else { stmt2; } nondet_choice stmt1; nondet_choice stmt2; 另一种是对于循环条件的抽象。对于循环结构，将循环条件抽象为非确定性路径。这种处理方式消除了控制流图中的环结构，保证程序一定能终止。 while (condition) { body; } nondet_choice body; nondet_choice skip_loop; 上述两种语句的抽象本质上是构造一个超集。对于只有一条执行路径的原始程序，抽象后将存在多条执行路径，其中一定包含原始程序的执行路径。 提前合并\r提前合并\r提前合并的核心思想是不在路径末尾做合并，而是在控制流的汇合点提前进行合并。换句话说，对于忽略条件判断的结果，不是在计算每条路径的结果之后再进行汇总，而是首先考虑每条路径都会遍历，因此在多条路径的汇聚点就进行分支结果的合并，最终得到一个稳定的值，它就是程序的最终结果。 这种处理方式能够避免路径末尾合并带来的冗余计算，通过提前合并，可以加速分析过程并简化结果的计算。具体来说，没有提前合并，那么每条分支就都需要维护整个分支上的所有信息，直到最后才进行处理，这无疑会加重信息的存储，同时末尾过多信息的处理也会拖慢分析速度。 上述两种近似方案，对程序控制部分的分析进行了优化，但是对于程序数据部分的分析没有什么帮助。而数据的分析优化就是下面 符号抽象 的用途。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:1:1","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"符号抽象\r注意事项\r下面的内容大部分来自于 静态分析概述。该文章介绍的是抽象的方法，但是我觉得这个和 《软件分析技术》 中介绍的符号分析方法简直一模一样，所以就将二者进行组合介绍。 在静态分析中，重点在于程序的特定性质，而非每个细节。抽象就是从程序中提取与研究特性相关的部分，忽略无关细节。这种方法不仅简化了问题，还能高效分析复杂程序的潜在问题。例如，分析除零错误（Zero Division Error）时，只需判断值是否为 0，而无需关心具体数值大小。通过将程序的 具体值集（Concrete Domain） 映射到 抽象值集（Abstract Domain），问题得以简化，同时保证分析结果仍涵盖原程序的所有可能性。 符号抽象\r对于程序 P 的 具体值集（Concrete Domain） $D_C$，静态分析 S 基于要研究的性质 Q 设计 抽象值集（Abstract Domain） $D_A$，并建立映射关系 $f_{D_C \\rightarrow D_A} \\ (f \\subseteq D_C \\times D_A)$，这个过程称为 S 对 P 关于 Q 的 抽象（Abstraction）。 通常 $|D_A| \u003c |D_C|$，因为抽象的目的是简化问题。而在 $D_A$ 中，常见两个特殊值： $\\top$ 为 未确定值（Unknown） $\\bot$ 为 未定义值（Undefined） $\\top \\in D_A \\land \\bot \\in D_A$，它们需要额外定义运算规则以适应表达式计算。 这里的 $D_C$ 包含程序中变量的所有具体值，如 $\\{-2,-1,0,1,2,3,\\ldots\\}$，而 $D_A$ 是一个抽象化的集合，如 $\\{\\text{+},\\text{-},\\text{零}\\}$。$f_{D_C \\rightarrow D_A}$ 则是其 映射函数，将程序中的具体状态或值转化为适合静态分析的抽象状态或值。其中关系 $f \\subseteq D_C \\times D_A$ 表示映射 $f$ 是具体值域 $D_C$ 和抽象值域 $D_A$ 笛卡尔积的一个子集，即 $f = \\{(c, a) \\ | \\ c \\in D_C, a \\in D_A\\}$。这种表示方式允许 $f$ 是一个多对一的关系，即多个具体值可以映射到同一个抽象值，上述 $D_C$ 中的所有正数都可以映射为 $D_A$ 中的 $+$。 上述是对于单个变量而言的抽象，而程序中不止变量，还存在表达式，所以下面需要对于表达式进行处理。 状态转移\r假设 $f_1 = f_{D_C \\rightarrow D_A}$。对于程序 P 的二元操作集 op，定义映射： $$ f_2 = f_{op \\times D_A \\times D_A \\rightarrow D_A} \\ (f_2 \\subseteq op \\times D_A \\times D_A \\times D_A) $$ 之后就可以通过 $f_2 \\circ f_1$，将 $D_C$ 相关的表达式值映射到 $D_A$。其中： $f_1$为 状态函数（State Function），定义如何将具体值转化为抽象值 $f_2$为 转移函数（Transfer Function），定义如何基于抽象值解析表达式 这里的 $f_2$ 定义了如何基于抽象值解析二元操作，这是对于具体值集中函数的抽象，使得经过符号抽象的变量可以通过抽象后的函数进行运算。其中 $op$ 是程序种的操作符集合（例如加法 +、减法 -、逻辑与 \u0026\u0026 等），$op \\times D_A \\times D_A \\rightarrow D_A$ 表示两个抽象值通过一个操作符生成新的抽象值。而这里的 组合 $f_2 \\circ f_1$ 表示了对于变量的所有处理操作，先通过 $f_1$ 将具体值转换为抽象值，再通过 $f_2$ 对这些抽象值进行操作，得到新的抽象结果。 那么对于上述符号抽象过程，可以使用研究程序中变量的正负性来举例，则若是 $D_A = \\{+,-,0,\\top,\\bot \\}$，那么 $f_1 = f_{D_C \\rightarrow D_A}$ 为 $$ \\forall x \\in D_C, \\quad f_1(x) = \\begin{cases} +, \u0026 \\text{if } x \u003e 0 \\\\ 0, \u0026 \\text{if } x = 0 \\\\ -, \u0026 \\text{if } x \u003c 0 \\end{cases} $$ $f_2 = f_{op \\times D_A \\times D_A \\rightarrow D_A}$ 就是 $$ f_2 = \\{(+, +, +, +), (+, -, -, -), (+, +, -, \\top), (+, 0, 0, 0), \\\\ (/, +, +, +), (/, -, -, +), (\\top, 0, \\bot, \\bot), (/, +, -, -), \\ldots \\} $$ 可以使用 Sound、过近似的分析原则分析下面的代码： x = 1; if input then y = 10; else y = -1; z = x + y; 这里会发现在进入 2-5 行的条件语句时，y 的值可能为 10，也可能为 -1。于是最终会认为 y 的抽象值为 $\\top$，最终 z 的抽象值也就为 $\\top$。这样的分析就是尽可能全面的，虽然它并不精准。这里 y 的抽象值会为 $\\top$ 是根据 常量传播 的规则来的，在下面常量传播的板块会进行规则说明。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:1:2","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"基本概念\r数据流分析\r数据流分析（Data Flow Analysis） 是指分析数据在程序中是怎么流动的，具体而言，其分析的对象是基于抽象的应用特定型数据；行为是数据的流动；方式是安全近似，根据安全性需求选择过近似还是欠近似；基础是控制流图。而在数据的流动中，场景只有两个： 在 CFG 的点内流动，即程序基本块内部的数据流。 在 CFG 的边上的流动，即由基本块间控制流触发的数据流。 由此可以看出不同的数据流分析应当有： 不同的数据抽象； 不同的流安全近似策略 — 过近似或者欠近似； 不同的 转移函数 和 控制流处理方法； 上述对于转移函数进行了定义，那么下面还需要对于控制流处理方法进行介绍。 数据流值\r定义程序 P 的 抽象数据状态（Abstract Data State，ADS），也即 数据流值 为程序中所有数据抽象值的整体，由此： 称每一个 IR 语句 s 执行之前，pre(s) 执行之后的数据流值为 s 的输入状态，记为 $IN[s]$，这里 pre(s) 为控制流中 s 的前驱的集合 称每一个 IR 语句 s 执行之后，suc(s) 执行之前的数据流值为 s 的输出状态，记为 $OUT[s]$，这里 pre(s) 为控制流中 s 的后继的集合 而对于数据流值，还存在另外一种表示方式： 程序点\r对于程序 P 的 IR 语句集 S，定义集合 $$ PP = \\{ (s_i, s_j) | s_i, s_j \\in S \\land (s_i \\in pre(s_j) \\lor s_j \\in suc(s_i) ) \\} $$ 中的每一个元素为程序 P 的一个 程序点（Program Point）。 顾虑到 $i, j$ 是对于平面的 IR 语句的标记，其差值大小不好定义，但是这里的程序点是用控制流中 相邻语句的顺序二元组 来进行表示的。这里每一个程序点都对应了一个数据流值，而每一个不同的数据流值都会有一个或者多个程序点与之对应。简单而言，数据流值就是当前未执行语句 $s$ 前，所有变量状态的整体。它类似于寄存器组，存储着当前未执行指令前的寄存器状态。 交汇\r定义数据流值在控制流推动下融合时的运算为 交汇（meet），用符号 $\\wedge$ 表示，其含义由具体的数据流分析决定。一般对于集合类型的数据流值，在可能性分析下，$\\wedge$ 常定义为并集、必然性分析下，$\\wedge$ 则常定义为交集。 那么对于正向分析，每个语句 s 的输入状态，就是其前驱语句输出状态的交汇，即： $$ IN[s] = \\bigwedge_{s_i \\in pre(s)} OUT[s_i] $$ 而对于逆向分析，每个语句 s 的输出状态，就是其后继语句输入状态的交汇，即： $$ OUT[s] = \\bigwedge_{s_i \\in suc(s)} IN[s_i] $$ 以正向分析为例，上述公式有如下 3 种常见的应用场景： 交汇场景\r由上面的一系列定义，可以总结数据流分析的工作： 数据流分析的核心任务是为程序中的每个 程序点 关联一个 数据流值，用以表征在该程序点可能观察到的所有程序状态。这一过程可以形式化为寻找一个从程序点集合到数据流值集合的满射 $f_{PP \\to D}$，这里程序的定义域为 D。在分析过程中，每个程序点的数据流值都可以通过程序语句的输入状态 $IN[s]$ 和输出状态 $OUT[s]$ 表达，这使得数据流分析可以等价地描述为对所有程序点输入输出状态集合的求解问题。 数据流分析需要满足一系列约束条件，统称为 安全近似导向型约束（SAOC）。这些约束主要来源于两个方面：一是语句的语义约束，即通过 状态转移方程 描述语句对程序状态的具体影响；二是 控制流约束，由程序控制流关系决定，确保不同语句之间输入输出状态的一致性。通过满足这些约束，数据流分析能够在安全的前提下对程序状态进行合理的近似和推断。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:1:3","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"约束记号\r状态转移\r在数据流分析中，状态转移方程（State Transfer Function） 用于描述程序语句对数据流值的映射，它表示为 $f_{D \\rightarrow D}$ 的映射。语句 s 的状态转移方程就是 $f_s$，它的作用就是根据控制流方向将输入状态映射为输出状态，或者反向从输出状态推导输入状态。 状态转移方程和上述定义的转移函数本质是一样的，都是 Transfer Function，只不过应用场景不一样，原相和相的集合不一样而已。后者是基于抽象值解析表达式，使其可以形成原来函数的超集；而前者则是基于数据流值进行的处理。 数据流方向\r控制流都是从开始到结束的，而数据流由于分析场景的不同，存在两种不同的方向。定义顺控制流方向的数据流分析为 正向分析（Forward Analysis），则 $f_s$ 满足 $OUT[s] = f_s(IN[s])$；定义逆控制流方向的数据流分析为 逆向分析（Backward Analysis）， $f_s$ 满足 $IN[s] = f_s(OUT[s])$。 最后根据上述信息，就可以定义控制流中的约束记号。 基本块状态\r对于基本块 $B = \\{s_1,s_2,\\ldots,s_n\\}$，其中 $\\forall i = 1, 2, \\dots, n - 1; IN[s_{i + 1}] = OUT[s_i]$。那么基本块 $B$ 的输入状态为 $IN[B] = IN[s_1]$，基本块 $B$ 的输出状态为 $OUT[B] = OUT[s_n]$。 状态转移方程\r对于上述基本块 $B = \\{s_1,s_2,\\ldots,s_n\\}$，$s_i$ 的状态转移方程为 $f_{s_1}$。 在正向分析中，基本块 $B$ 的状态转移方程为 $f_B = f_{s_n} \\circ \\ldots \\circ f_{s_2} \\circ f_{s_1}$，满足 $$ OUT[B] = f_B(IN[B]) $$ 其中，$IN[B] = \\bigwedge_{P \\in pre(B)} OUT[P]$。 在逆向分析中，基本块 $B$ 的状态转移方程为 $f_B = f_{s_1} \\circ \\ldots \\circ f_{s_{n-1}} \\circ f_{s_n}$，满足 $$ IN[B] = f_B(OUT[B]) $$ 其中，$OUT[B] = \\bigwedge_{S \\in suc(B)} IN[S]$。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:1:4","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"定义可达性分析\r注意事项\r在下面对于四种算法的定义中，不讨论方法调用和别分分析，这些会在之后的 过程间（Inter-Procedural）分析 中进行讨论。而这里的情况只考虑 过程内（Intra-Procedural）分析 的 CFG。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:2:0","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"问题描述\rReaching Definition Analysis\r程序中变量 v 的 定义（Definition） 是指对 v 赋值的语句。在程序点 p 的一个定义 到达（Reach） 程序点 q，如果存在一条从 p 到 q 的 控制流路径，且在这条路径上，定义 d 未被后续的重新定义所 覆盖（Kill）。这种分析每个程序点处定义是否可达的过程称为 定义可达性分析（Reaching Definition Analysis）。 定义可达性分析\r从上述定义可以看出，“定义可达性” 实际上描述了一个定义可能的最长 生存期（Lifetime）。因为只要存在一条路径能够从定义点到达目标点，就认为该定义是可达的。这是一种 可能性分析（May Analysis），采用了 过近似（Over-Approximation） 的原则。 定义可达性分析可以应用于检测程序中可能存在的未定义变量。例如，可以在 数据流图（DFG） 的入口处为每个变量 v 赋予一个 伪定义（Dummy Definition）。如果在程序中的某个点 p 使用了变量 v，并且 v 的伪定义能够到达程序点 p，则可以推测出变量 v 可能在定义之前被使用，即程序可能存在变量未定义的错误。 这里为什么是 可能存在 而不是 一定存在 呢？原因在于定义可达性分析采用的是可能性分析的策略：只要有任意一条控制流能够将定义传播到某个程序点，就认为定义是可达的。然而，在实际程序执行时，只会有唯一的一条控制流被真正执行，这条控制流未必与分析过程中得出结论的路径相符。因此只能推测出可能存在未定义变量的问题，而不能确保其一定发生。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:2:1","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"问题分析\r在这一小节中，会应用静态分析和数据流分析的基本思路，分析定义可达性问题。 数据抽象\r首先，需要定义程序的 抽象数据状态。在此问题中，关注的数据流值是每个变量的定义，因此，程序的抽象数据状态可以用 定义的集合 表示。假设程序 P 中的所有定义为 $D = \\{d_1, d_2, \\dots, d_n\\}$，则在定义可达性问题下，D 的幂集（Power Set）即为程序 P 的定义域（Domain）。每个程序点的数据流值可以表示为 D 的子集，描述能够到达该点的定义集合。也即确定 $f_{P \\to D}$，为每个程序点关联一个数据流值。 在实际实现中，由于全集 D 是固定的，且 $|D| = n$，因此可以用 n 位的位向量（Bit Vector） 表示 D 的所有子集，即所有可能的抽象数据状态。位向量的第 i 位表示定义 $d_i$ 是否可达： 第 i 位为 0 表示 $d_i$ 不可达； 第 i 位为 1 表示 $d_i$ 可达。 用位向量表示全集确定的集合是常见做法。假设全集的势为 n，则其子集有 $2^{n}$ 个，而 n 位的位向量也有 $2^n$ 种可能，二者之间存在一一对应关系。除了位向量外，集合还可以用其他方式表示，例如哈希表或红黑树。但由于位向量在数据流分析中应用广泛，这里仅作介绍。为了更具一般性，接下来的分析仅基于集合的抽象，而不依赖其具体实现。 约束分析\r完成数据抽象后，进行估计分析。采用的策略是 安全近似（过近似）和 正向分析。估计的时候需要考虑两种约束：语义约束和控制流约束。 语义约束指程序语句的影响，例如，语句 D: v = x op y 生成了关于变量 v 的新定义 D，并覆盖程序中其他对 v 的定义，但不会影响后续其他定义覆盖该定义。赋值语句是定义的一种形式，定义还可以包括其他形式，例如引用参数。同时这里分析时以基本块为粒度。基本块 $B$ 中： 生成的新定义记为集合 $gen_B$。 覆盖掉的定义记为集合 $kill_B$。 例如： 举例\r$$ \\begin{aligned} gen_{B_1} \u0026 = {d_1, d_2, d_3}, \\qquad kill_{B_1} = {d_4, d_5, d_6, d_7} \\\\ gen_{B_2} \u0026 = {d_4, d_5}, \\qquad\\quad\\ kill_{B_2} = {d_1, d_2, d_7} \\\\ gen_{B_3} \u0026 = {d_6}, \\qquad\\qquad\\ \\ kill_{B_3} = {d_3} \\\\ gen_{B_4} \u0026 = {d_7}, \\qquad\\qquad\\ \\ kill_{B_4} = {d_1, d_4} \\end{aligned} $$ 在静态程序中，$gen_B$ 和 $kill_B$ 是固定的。基于此，可以得到基本块 $B$ 的 转移方程： $$ OUT[B] = gen_B \\cup (IN[B] - kill_B) $$ 控制流约束采用过近似原则：某定义达到程序点，只需存在至少一条路径可达即可。因此，定义 交汇操作符（Meet Operator） 为集合并操作：$\\wedge = \\cup$。控制流约束 表示为： $$ IN[B] = \\bigcup_{P \\in pre(B)} OUT[P] $$ ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:2:2","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"问题求解\r由上述分析，可以得到该算法的相关信息： 定义可达性分析\r对程序中任意程序点，分析在该点处每个变量的定义可能由哪些语句生成。要求上近似，即最后得到的返回值包括所有可能的定义。 分析方向：正向分析 半格元素：从变量到定义集合的函数 $X \\rightarrow 2^{D}$，其中 $X$ 是程序中所有变量的集合，$D$ 是程序中所有定义的集合。（格的概念下文会进行陈述） 合并运算（控制流约束）：对应集合的并 $IN[B] = \\bigcup_{P \\in pre(B)} OUT[P]$ 最小元：映射任何变量到空集 $\\bot$ 输入值：最小元 转换函数（状态方程）： $OUT[B] = gen_B \\cup (IN[B] - kill_B)$，其中 对于基本块 $B$ 中给变量 $x$ 赋值的语句，$kill_B$ 表示所有给 $x$ 赋值的语句的行号，而 $gen_B$ 表示当前语句的行号； 而对于基本块 $B$ 中的其他语句，$kill_B$ 和 $gen_B$ 都是 $\\emptyset$； 注意这里分析的都是基本块中的单个语句，所以直接对于 $kill_B$ 等表示变量相关的集合进行操作，而没有使用 $kill^{x}_B$ 进行描述。但是对于整个基本块而言，最后的结果必然会包括对于其余变量的操作。 这里的基础就是 半格元素，之所以使用半格而不是全格，是因为全格需要存在最小上界和最大下界，同时要支持并操作和交操作。而半格就相对结构简单，只需要前面一半即可。同时，静态分析中的算法由于上近似（May）和下近似（Must）的原因，所以只需要并操作和交操作中的一种即可，因此这里使用半格。 算法设计\r根据上面的分析，可以设计定义可达性问题的求解算法。**** 可达性分析算法\r可以通过下面的例子直观感受算法。 例子\r算法分析\r上述算法是经典的静态分析迭代算法，用于解决定义可达性问题。初始化阶段将入口（ENTRY）的输出集合 $OUT[\\text{ENTRY}]$ 设置为空集，其他基本块的输出集合也初始化为空集。这种分开初始化的方式是一种通用模式，因为某些情况下 ENTRY 和其他基本块的初始值可能不同。 该算法的核心是通过反复更新每个基本块的输入集合 $IN[B]$ 和输出集合 $OUT[B]$ 直到达到一个不动点（Fixed Point），即所有的 $OUT[B]$ 不再发生变化。在每次迭代中，$IN[B]$ 是其所有前驱节点输出集合的并集，而 $OUT[B]$ 是生成集合 $gen_B$ 与幸存者集合 $IN[B] - kill_B$ 的并集。 该算法可以停止。这是因为 $OUT[B]$ 在每次迭代中要么保持不变，要么增长，而定义的总集合 $D$ 是固定的，且 $OUT[B] \\subseteq D$ 。因此，迭代最多进行 $|D| \\times |B|$ 次，其中 $|B|$ 是基本块的总数。最终，当所有 $OUT[B]$ 都不再变化时，算法达到固定点。 具体分析过程中，$gen_B$ 和 $kill_B$ 是固定的，因为程序 $P$ 本身不改变。当新的定义到达基本块 $B$ 时，这些定义要么被 $kill_B$ 覆盖，要么幸存下来进入 $OUT[B]$ 。一旦定义被加入 $OUT[B]$ ，无论通过 $gen_B$ 还是幸存者集合，都会永久保留。因此，集合 $OUT[B]$ 具有单调性，并最终收敛。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:2:3","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"活跃变量分析\r","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:3:0","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"问题描述\rLive Variable Analysis\r在程序点 p 处，某个变量 v 的变量值可能在之后的某条控制流中被用到，称变量 v 是程序点 p 处的 活跃变量（Live Variable），否则，称变量 v 为程序点 p 处的 死变量（Dead Variable）。分析在各个程序点处所有的变量是死是活的分析，就是 活跃变量分析（Live Variable Analysis）。这里程序点 p 处的变量 v 是活跃变量，当且仅当在 CFG 中存在某条从 p 开始的路径，在这条路径上变量 v 被使用了，并且在 v 被使用前，v 未被重新定义，即使用前没有被重新赋值。 活跃变量分析\r这里判断活跃变量的标准不是这个变量之后有没有可能用到，而是 这个变量当前所储存的值在之后有没有可能被用到。 活跃变量分析可以应用在 寄存器分配（Register Allocation） 中，可以作为编译器优化的参考信息。比如说，如果在某个程序点处，所有的寄存器都被占满了，又需要用一个新的寄存器，那么就要从已经占满的这些寄存器中选择一个去覆盖它的旧值，这时应该更青睐于去覆盖那些储存死变量的寄存器。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:3:1","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"问题分析\r数据抽象\r同样按照上面理解定义可达性分析的流程理解这里的活跃变量分析，进行数据抽象。在活跃变量分析问题中，相比于之前关注每个变量的定义，这里关注的就是哪些是活跃变量。所以可以使用 变量的集合 作为数据流值。因此假设程序 P 中的所有变量为 $V = \\{v_1, v_2, \\dots, v_n\\}$，则数据流分析的定义域为 $V$ 的幂集，即 $D = 2^V$。 每个程序点处的抽象程序状态就是 $V$ 的一个子集，表示这个程序点处所有的活跃变量的集合，而活跃变量分析的任务就是确定每个程序点的抽象程序状态。同时和定义可达性分析一样，因为全集确定，所以子集的实现可以使用位向量来表示 约束分析\r在某个程序点处的变量是否存活，判断标准是这个程序点之后，该变量的值是否还有被使用的可能。换句话来说，如果在某个程序点处，一个变量被使用了，那么在它之前的一些程序点上，该变量就是活的。这里的一些的标准是到某个变量的定义为止，因为这里判断活跃变量看的是变量的值，而不是变量本身，变量一旦被重新赋值，那么就可以把它视为死变量了（这里对于新的赋值而言就是新的活跃变量）。 由此可以看出，活跃变量分析适合从后往前进行分析，也就是逆向分析。因为若是从前往后进行分析，判断一个变量是否活跃需要正向搜索查看后续是否存在被使用的部分，效率低下；而从后往前分析，那么只需要在某程序点处找到了变量的使用，那么就可以证明在此之前任意可达此点且定义了该变量的程序点处，该变量都是活跃的。同时由上述对于活跃变量分析的定义可知，只要存在一条路径使得程序点 p 处的变量 v 的值被用到，那么就认为 v 在 p 处是活跃的，因此采用过近似的方式。 之后就是考虑 语义约束。在基本块 $B$ 中，需要考虑 $OUT[B]$ 如何被转化为 $IN[B]$，也就是 $IN[B]$ 中的活跃变量从何而来？ 第一种就是在 $OUT[B]$ 中已经存活的，并且在基本块 $B$ 中没有被重新定义； 第二种就是在基本块 $B$ 的表达式中，且是在基本快之前定义的变量（也就是说，如果基本块 $B$ 内部先定义一个变量，然后再使用这个变量，那么这个变量在 $IN[B]$ 处还是死变量） 对于下方的 CFG，假设 $OUT[B] = \\{v\\}$，考虑 $B$ 中具有代表性的六种语句，则 $IN[B]$ 的结果如右所示： 举例\r这里可以记在基本块 B 中被定义的变量集合为 $def_B$，在基本块 $B$ 中定义前使用的变量集合为 $use_B$。而被使用但未被定义也属于在定义前使用，也就是说上述的 2、4、6 都满足 $v \\in use_B$。而在情况 5 中，$v \\notin use_B$，因为它是在被重定义之后才使用的，不符合上述约束分析的要求，无法确定 $IN_B$ 处的变量是否活跃。 于是，可以得到语义约束下的 状态方程 为： $$ IN[B] = use_B \\cup (OUT[B] - def_B) $$ 考虑控制流约束，由于采用过近似的方式，所以交汇操作应当定义为交集，即 $\\wedge = \\cup$，又是因为逆向分析，因此控制流约束为： $$ OUT_B = \\bigcup_{S \\in suc(B)} IN[S] $$ ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:3:2","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"问题求解\r由上述分析，可以得到该算法的相关信息： 活跃变量分析\r给定程序中的某条语句 s 和变量 v，如果在语句 s 执行前保存在 v 中的值在后续执行中还会被读取，则称 v 是语句 s 处的 活跃变量。最后需要得到的是所有可能的活跃变量。 分析方向：逆向分析 半格元素：变量集合 合并运算：集合的并 $OUT_B = \\bigcup_{S \\in suc(B)} IN[S]$ 最小元：空集 输出值：空集 转换函数：$IN[B] = use_B \\cup (OUT[B] - def_B)$ 对于给 $x$ 赋值的语句（x = 1），$def_B = \\{x\\}$；对于其他语句，$def_B = \\emptyset$； $use_B$ 是基本块 $B$ 中在定义之前使用的所有变量的集合； 算法设计\r根据上面的分析，可以设计活跃变量分析问题的求解算法。 活跃变量分析算法\r可以通过下面的例子直观感受算法。 举例\r算法分析\r这里的算法和定义可达性分析基本上一致，都是可能性分析（May Analysis）的迭代算法，区别就在于定义可达性算法为正向分析，初始化先初始输出状态（也是由分析顺序决定初始化状态），而活跃变量分析算法为逆向分析，初始化先初始输入状态。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:3:3","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"可用表达式分析\r","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:4:0","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"问题描述\rAvaliable Expression Analysis\r称一个表达式 x op y 在程序点 p 处是 可用的（Avaliable） 需要满足后续条件：如果所有从程序入口到程序点 p 的路径都必须经过（使用） x op y 表达式，并且在最后一次 x op y 的使用之后，没有 x 或者 y 的重定义。这里对于程序中每个程序点处的可用表达式的分析，称之为 可用表达式分析（Avaliable Expression Analysis）。 这里说一个表达式是可用的，指的是这个表达是的值肯定已经被计算过了，可以直接复用之前的结果，没必要再算一遍，也就是说，这个表达式不需要忙碌于计算。可以使用下面的例子进行说明： if a - b \u003e c then c = a - b; d = a - b; if d \u003e c then c = d; 上面两个代码功能性上是等价的，但是在案例一中， a - b 被重复计算了两次，而案例二中， a - b 只被计算了一次，因此案例二的效率是更高的。在案例一的第 2 行， a - b 就是一个可用表达式，在之前肯定已经被计算过，因此可以对程序进行优化，通过一个变量或者是寄存器储存之前的计算结果，从而在之后不需要进行重复的计算。 可用表达式的相关信息还可以被用来检测 全局的公共子表达式（Global Common Subexpression）。 从定义中不难看出，可用表达式分析是一种必然性分析。因为在上述表达式优化的应用场景中，可以不优化每一个表达式，但不可以优化错误（也就是说一旦决定优化某个表达式，这个表达式就必须必然是可用表达式）。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:4:1","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"问题分析\r数据抽象\r在这个问题中，考虑程序中所有表达式的集合，即 $E = \\{e_1,e_2,\\ldots e_n \\}$，其中 $e_i$ 是程序中的表达式。那么，每个程序点处的抽象程序状态，也就是说数据流值为 $E$ 的一个子集，整个分析的定义域 $D = 2^E$。 约束分析\r上面的定义已经指明从程序的入口进行分析，所以这里为正向分析，同时又由于所有路径都可用才是可用，因为这里为必然性分析（Must Analysis）。 对于语义约束，基于上述定义，这里需要从 $IN[B]$ 中加入 $B$ 中产生的新表达式，删除 $IN[B]$ 中被 $B$ 重定义变量的表达式。因此定义 $gen_B$ 为基本块 $B$ 中所有表达式的集合，$kill_B$ 为 程序中所有变量被 $B$ 重新定义的表达式 的集合。 $kill_B$：表示程序中所有可能被基本块 $B$ 重定义的表达式集合，而不是 $IN[B]$ 中被 $B$ 重定义的表达式集合。 这是因为 $gen_B$ 和 $kill_B$ 需要在数据流分析算法执行前静态计算，以避免在每次迭代中重复计算。此外，由于后续操作是取差集，$kill_B$ 取更大的集合不会影响最终结果。 于是得到基本块 $B$ 的 状态转移方程 为： $$ OUT[B] = gen_B \\cup (IN[B] - kill_B) $$ 考虑控制流约束，由于采用过近似的方式，所以交汇操作应当定义为交集，即 $\\wedge = \\cap$，又是因为逆向分析，因此控制流约束为： $$ IN[B] = \\bigcap_{P \\in pre(B)} OUT[P] $$ ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:4:2","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"问题求解\r由上述分析，可以得到该算法的相关信息： 活跃变量分析\r给定程序中某个位置 p，如果从入口到 p 的所有路径都对表达式 exp 求值，并且最后一次求值后该表达式的所有变量都没有被修改，则 exp 称作 p 的一个 可用表达式。最后的结果就是当前的可用表达式。要求下近似。 分析方向：正向分析 半格元素：表达式的集合 合并运算：集合的交 $IN[B] = \\bigcap_{P \\in pre(B)} OUT[P]$ 最小元：全集 输出值：空集 转换函数：$OUT[B] = gen_B \\cup (IN[B] - kill_B)$ 对于给 $x$ 赋值的语句，$kill_B$ 为所有包含 $x$ 的表达式，$gen_B$ 为当前求值语句中不含 $x$ 的表达式； 对于其他语句，$kill_B$ 为 $\\emptyset$，$gen_B$ 为当前语句中求值的表达式； 算法设计\r基于上述分析，可以设计可用表达式分析算法如下： 可用表达式分析\r这里的 $U$ 和 $I$ 常在集合论中表示全集。 可以通过下面的例子来直观感受一下上面的算法。 举例\r算法分析\r这里的算法和定义可达性分析算法基本一致，不同的之处在于： 初始化的时候，除了程序入口之外的其他基本块 $B$ 的 $OUT[B]$ 都初始化为了全集，因为在最开始的时候，所有的表达式都是可用的。只有在分析过程中发现表达式中的某个变量被重定义的时候，表达式才会变得不可用（需要重新计算）。 控制流约束部分变成了交集，因为这里是必然性分析。 这里注意在分析初始状态的时候，需要通过语义以及对算法执行过程的影响来分析到底是空集还是全集。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:4:3","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"格理论基础\r","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:5:0","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"基本概念\r偏序\r对于数据流分析的基础知识，可以从数学的角度去探索，这个产物就是格。而了解格需要先知道偏序是什么。 偏序关系\r定义 偏序集（Poset） 为 $(P, \\preccurlyeq)$，其中 $\\preccurlyeq$ 为一个二元关系，这个二元关系在 $P$ 上定义了 偏序关系，并且 $\\preccurlyeq$ 具有如下性质： 自反性：$\\forall x \\in P, x \\preceq x$； 反对称性：$\\forall x, y \\in P, x \\preceq y \\land y \\preceq x \\Rightarrow x = y$； 传递性：$\\forall x, y, z \\in P, x \\preceq y \\land y \\preceq z \\Rightarrow x \\preceq z$； 例如 $(Z, \\preccurlyeq)$ 是偏序集，$(2^S, \\subseteq)$ 也是偏序集。并且，偏序意味着集合中可能存在某两个元素，它们之间是不可比的。任意两个元素都可比的偏序集称为全序集。 上界与下界\r对于偏序集 $(P, \\preceq)$ 及其子集 $S \\subseteq P$，如果 $\\forall x \\in S, \\ x \\preceq u$，则称 $u \\in P$ 是 $S$ 的一个 上界（Upper Bound）；同时定义 $S$ 的 最小上界（Least Upper Bound，LUB） 为 $\\bigvee S$，它使得对于 $S$ 的任意一个上界 $u$，有 $\\bigvee S \\succeq u$。如果 $\\forall x \\in S, \\ l \\preceq x$，则称 $l \\in P$ 是 $S$ 的一个 下界（Lower Bound）；定义 $S$ 的 最大下界（Greatest Lower Bound, GLB） 为 $\\bigwedge S$，它使得对于 $S$ 的任意一个下界 $l$，有 $l \\preccurlyeq \\bigwedge S$。 通常，如果 $S$ 只包含两个元素 $a$ 和 $b$，即 $S = \\{a, b\\}$，则 $\\bigvee S$ 可写作 $a \\vee b$，称为 $a$ 和 $b$ 的 联合（Join）；$\\bigwedge S$ 可写作 $a \\wedge b$，称为 $a$ 和 $b$ 的 交汇（Meet）。 并不是每一个偏序集都有最小上界或者最大下界，比如说 $(Z,\\leq)$，子集 $Z_+$ 就没有最小上界。同时如果一个偏序集存在最小上界，则这个偏序集最小上界是唯一的；如果一个偏序集存在最大下界，则这个偏序集的最大下界也是唯一的。 格\r格就是在偏序的基础上定义的，相关理论如下所示： 格\r考虑偏序集 $(P, \\preccurlyeq)$，如果 $\\forall a, b \\in P$，$a \\vee b$ 和 $a \\wedge b$ 都存在，则我们称 $(P, \\geq)$ 为 格（Lattice）。 简单理解，格是每对元素都存在最小上界和最大下界的偏序集。 比如说 $(Z, \\leq)$ 是格，其中 $\\vee = \\max$，$\\wedge = \\min$；$(2^S, \\subseteq)$ 也是格，其中 $\\vee = \\cup$，$\\wedge = \\cap$。 半格\r考虑偏序集 $(P, \\preccurlyeq)$， 如果 $\\forall a, b \\in P$，$a \\vee b$ 存在，则称 $(P, \\preccurlyeq)$ 为 联合半格（Joint Semilattice）; 如果 $\\forall a, b \\in P$，$a \\wedge b$ 存在，则称 $(P, \\preccurlyeq)$ 为 交汇半格（Meet Semilattice）。 联合半格和交汇半格统称为 半格（Semilattice）。 如果 $\\forall S \\subseteq P$，$\\bigvee S$ 和 $\\bigwedge S$ 都存在，则称 $(P, \\preccurlyeq)$ 为 全格（Complete Lattice)。每一个有限格 $(P, \\preccurlyeq)$（$P$ 是有限集）都是一个全格。 简单理解，全格的所有子集都有最小上界和最大下界。由于 $(Z, \\leq)$ 的子集 $Z_+$ 没有最小上界，因此它不是全格；与之不同的 $(2^S, \\subseteq)$ 就是一个全格。 顶部和底部\r每一个全格 $(P, \\preceq)$ 都有一个序最大的元素 $\\top = \\bigvee P$ 称作 顶部（Top），和一个序最小的元素 $\\perp = \\bigwedge P$ 称作 底部 （Bottom）。 积格\r考虑偏序集 $L_1 = (P_1, \\preceq_1), \\ L_2 = (P_2, \\preceq_2),\\ \\ldots,\\ L_n = (P_n, \\preceq_n)$，其中 $L_i = (P_i, \\preceq_i), \\ i = 1, 2, \\ldots, n$ 的 LUB 运算为 $\\bigvee_i$，GLB 运算为 $\\bigwedge_i$，定义积格（Product）为 $L^n = (P, \\preceq)$，满足： $P = P_1 \\times P_2 \\times \\ldots \\times P_n$ $(x_1, x_2, \\ldots, x_n) \\preceq (y_1, y_2, \\ldots, y_n) \\Leftrightarrow (x_1 \\preceq y_1) \\wedge (x_2 \\preceq y_2) \\wedge \\ldots \\wedge (x_n \\preceq y_n)$ $(x_1, x_2, \\ldots, x_n) \\wedge (y_1, y_2, \\ldots, y_n) = (x_1 \\wedge y_1, x_2 \\wedge y_2, \\ldots, x_n \\wedge y_n)$ $(x_1, x_2, \\ldots, x_n) \\vee (y_1, y_2, \\ldots, y_n) = (x_1 \\vee y_1, x_2 \\vee y_2, \\ldots, x_n \\vee y_n)$ 这里积格是格。全格的积格还是全格。 不动点\r上述算法都是达到不动点就结束了，这就涉及到不动点理论。 单调函数\r这里称一个函数 $f_{L \\rightarrow L}$（$L$ 是格）是 单调的（Monotonic），具有 单调性（Monotonicity），如果 $\\forall x, y \\in L, x \\preceq y \\Rightarrow f(x) \\preceq f(y)$。 这里格和格所在的那个集合一般是不严格区分的，也就是说，$L$ 既可以表示格，也可以表示定义格的那个集合。 相关知识\r如果 $f(\\alpha) = \\alpha$，则称 $\\alpha$ 是一个函数 $f$ 的 不动点（Fixed Point）。 一个格的高度 $h$ 为从底部到顶部的最长路径。 不动点定理\r考虑一个全格 $(L, \\preceq)$，如果 $f_{L \\rightarrow L}$ 是单调的且 $L$ 是有限集，那么序最小的不动点（Least Fixed Point）可以通过如下的迭代序列找到： $$ f(\\perp), f(f(\\perp)), \\ldots, f^{h+1}(\\perp) $$ 序最大的不动点 (Greatest Fixed Point) 可以通过如下的迭代序列找到： $$ f(\\top), f(f(\\top)), \\ldots, f^{h+1}(\\top) $$ 其中，$h$ 是 $L$ 的高度。 上述可以通过鸽笼原理来进行证明。同时由此知道一个格上的单调函数一定能够对端点元素（$\\top$ 和 $\\perp$）迭代出不动点，并且上述 交汇 $\\wedge$ 和 联合 $\\vee$ 操作都是单调的。但是这里还不能因此说明迭代算法也有这样的性质，除非之后可以将迭代算法和不动点定理关联起来。 分配性\r分配性\r由上述定义可知，定义在格 $(L, \\preceq)$ 上的单调函数 $f(x)$ 满足： $$ f(x) \\vee f(y) \\preceq f(x \\vee y), \\quad f(x \\wedge y) \\preceq f(x) \\wedge f(y) $$ 而如果 $f(x \\vee y) = f(x) \\vee f(y)$，$f(x \\wedge y) = f(x) \\wedge f(y)$，那么称定义在格 $(L, \\preceq)$ 上的函数 $f(x)$ 满足分配性（Distributive）。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:5:1","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"单调框架\r迭代算法\r本文对于常见的四种算法进行了描述，但是其他类型的数据流分析也由类似的迭代算法，它们都很相似。于是可以使用一种具有通用性的数据流分析算法来提供一种通用的数据流分析的解决方案。可以从另外一种角度，也就是上面说的数学角度来看待迭代算法： 给定一个具有 $k$ 个结点的程序流程图（CFG），对于 CFG 中的每个结点 $n$，迭代算法的每一次迭代都会更新 $OUT[n]$。 设数据流分析的定义域为 $V$，那么可以定义一个 $k$ 元组（k-tuple）来表示每次迭代后的分析值： $$ (OUT[n_1], OUT[n_2], \\ldots, OUT[n_k]) \\in V \\times V \\times \\ldots \\times V = V^k $$ 每次迭代可以视为将 $V^{k}$ 中的某个值映射为 $V^{k}$ 中的另一个值，通过状态转移方程和控制流约束式，这个过程可以抽象为一个函数 $F_{V^{k} \\rightarrow V^{k}}$。 算法输出一系列的 $k$ 元组，直到某两个连续输出的 $k$ 元组完全相同时算法终止。 因此算法的过程可以表示为下面的过程，其中 $v_i^{j}$ 表示第 $i$ 个结点第 $j$ 次迭代后的数据流值。 步骤 表达式 初始化 $(\\bot, \\bot, \\ldots, \\bot) = X_0$ 第 1 次迭代 $(v_1^{1}, v_2^{1}, \\ldots, v_k^{1}) = X_1 = F(X_0)$ 第 2 次迭代 $(v_1^{2}, v_2^{2}, \\ldots, v_k^{2}) = X_2 = F(X_1)$ … … 第 i 次迭代 $(v_1^{i}, v_2^{i}, \\ldots, v_k^{i}) = X_i = F(X_{i-1})$ 第 i+1 次迭代 $(v_1^{i}, v_2^{i}, \\ldots, v_k^{i}) = X_{i+1} = F(X_i) = X_i$ 这里通过上述数学角度来审视迭代算法，因此在 i+1 此迭代时，也会因为鸽笼原理到达不动点。由此在上述过程中，在 $X_{i+1} = F(X_i) = X_i$ 时，抵达了不动点，算法也就是停止了，由此 $X_i$ 是函数 $F$ 的一个不动点。通过这个角度，可以将数据流分析中的迭代算法转化为对于格值的分析，但是还是存在一些疑问： 算法是否保证终止？不动点一定存在吗？ 不动点是否唯一？算法终止的不动点是否是最优的？ 算法何时到达不动点？ 这里问题的解决就需要使用到下面的单调框架的知识。 框架解释\r数据流分析中的单调框架是一种用于确保数据流分析算法的安全性、终止性和收敛性的通用框架。这个框架允许通过配置不同的参数来导出各种类型的数据流分析算法。 数据流分析框架\r一个 数据流分析框架（D，L，F） 由以下三个部分组成： D（Direction）：数据流的方向，表示正向或者逆向； L（Lattice）：一个包含值集 V 的域（即 V 的幂集）的格和一个交汇或者联合操作符； F（Function Family）：一个从 V 到 V 的转移函数族； 数据流与格\r对于每一个结点而言，数据流分析就是在某个格的值上迭代应用转移函数和交汇/联合操作的过程。因为这里结点的定义域是值集 $V$ 的幂集，它形成一个天然的全格 $(L, \\subseteq)$，所以可以将数据流分析和格进行关联。 而对于整个控制流图（CFG）而言，数据流分析可以被视为在所有结点的格的积格上迭代应用转移函数和交汇/联合操作。假设有 $k$ 个结点，结合上述偏序集的概念，可以把每次迭代视为 $F_{L^{k} \\rightarrow L^{k}}$，其中 $L^k$ 是一个全格。这里也就是指每次迭代，其实是所有结点对应的格值进行一次转换的过程。这里 $F$ 由两部分组成： 一个部分是状态转移方程 $f_{L \\rightarrow L}$，上述描述的三种算法都是单调的，因此在进行其他分析而设计状态转移方程时，需要保证其单调性。也就是说，一个设计糟糕的状态转移方程可能是不单调的，从而导致迭代算法无法终止，或者无法求出符合预期的结果。 另一个部分是交汇/联合操作函数 $f_{L \\times L \\rightarrow L}$，上述也指出了这两个操作都是单调的。 由此可以确定函数 $F$ 是单调的，因此其也满足了不动点定理的条件。那么根据不动点定理，已经证明了这里的 $F$ 是单调的，就可以回答上述的三个问题。 迭代算法一定会达到不动点。由不动点定理，在有限的格中，，迭代过程必然会在有限步骤内达到不动点； 不动点不一定是唯一的，但是通常关注的是序最小不动点（对于正向分析）或序最大不动点（对于逆向分析），这取决于分析的类型。这里不动点定理保证了最小不动点的存在，并且通过迭代可以找到这个不动点。 算法会在迭代过程中不断更新 $X_i$ 直到 $X_{i+1} = F(X_i) = X_i$，此时 $X_i$ 是 $F$ 的一个不动点。迭代次数最多为格的高度 $h$，最坏情况下，迭代次数为 $k \\cdot h + 1$（最坏情况是在每次迭代中，k 个结点的格中只有一个格变化了一次，并且直到 $\\top$ 才找到不动点，加一的最后一次用于确认所有的数据流值都不会发生变化了），其中 $k$ 是 CFG 的结点个数，$h$ 是定义域格的高度。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:5:2","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"常量传播分析\r","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:6:0","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"问题描述\rConstant Propagation Analysis\r对于程序点 p 处的变量 x，分析 x 在 p 处是否为一个常量就是 常量传播分析（Constant Propagation Analysis）。 这里如果知道了某程序点处的某些变量一定是一个常量的话，就可以直接优化，将这个变量视为常量，从而减少内存的消耗（可以在编译的时候就完成一部分计算，并且有些常量并不需要分配储存它的内存空间）。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:6:1","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"问题分析\r数据抽象\r这里可以使用一个有序对 $(x,v)$ 的集合表示每个程序点处的抽象程序状态，其中 $x$ 是变量名，$v$ 是变量的常数值，$v$ 的取值可能为某个常数，UNDEF（Undefined）或 NAC（Not A Constant）。 控制流约束\r基于上述数据抽象，可以定义值之间的交汇操作： $(x,v) \\land (x,NAC)=(x,NAC)$：变量和任意常量交汇还是变量。因为这里采用必然性分析策略，只有当变量值确定为常量时，才认为它是常量； $(x,\\text{UNDEF}) \\land (x,v)=(x,v)$：未初始化的变量 UNDEF 不在关注的范围内，出现 Undefined 错误应由 Reaching Definitions 负责，因此在交汇时可忽略 UNDEF； $(x,v_1) \\land (x,v_2) = \\begin{cases}(x,v_1), \u0026\\text{if }v_1=v_2\\\\(x,\\text{NAC}),\u0026\\text{otherwise}\\end{cases}$ 若是 $v_1 = v_2$，结果为 $(x,v_1)$，表明在该程序点处，变量 $x$ 可以被视为常量 $v_1$； 若是 $v_1 \\not ={v_2}$，结果为 $(x, \\text{NAC})$，说明两个不同的值汇聚到某一点，该变量不是常量； 状态转移方程\r考虑语句 $s: x = …$，定义其状态转移方程为： $$ F: OUT[s]=gen_s \\cup (IN[s] - \\{ (x ,\\_ ) \\}) $$ 其中，$(x,\\_)$ 是一个通配符，表示所有的以 $x$ 作为第一个元素的有序数对，也就是变量名为 $x$ 的变量。为了表示方便，下面用 $val(x)$ 来表示变量 $x$ 的值（它可能是常数，UNDEF或者NAC），定义 $gen_s$ 如下： 如果 $s: x = c$（$c$ 是常量），则 $gen_s=\\{(x,c)\\}$； 如果 $s: x = y$，则 $gen_s = \\{(x,val(y))\\}$； 如果 $s: x = y \\ op \\ z$，则 $gen_s = \\{(x,f(y,z))\\}$，其中， $$ f(y,z) = \\begin{cases} val(y) \\ op \\ val(z),\u0026\\text{if }y \\text{ and }z \\text{ are constants} \\\\ \\text{NAC},\u0026\\text{if }y \\text{ or }z \\text{ is NAC} \\\\ \\text{UNDEF},\u0026\\text{otherwise} \\end{cases} $$ 如果 $s$ 不是赋值语句，则 $OUT[s] = IN[s]$； 关于上述 $y \\ op \\ z$ 的操作，认为 NAC / 0 和 NAC mod 0 的结果都为 UNDEF。不过这些对于常量优化不重要，因为这些是程序逻辑错误，不是常量优化可以解决的。 总结\r由上述分析，可以得到该算法的相关信息： 活跃变量分析\r对于程序点 p 处的变量 x，分析 x 在 p 处是否为一个常量就是 常量传播分析。如果知道某程序点处的某些变量一定是常量，则可以直接优化，将该变量视为常量，从而减少内存消耗（可以在编译时完成部分计算，并且某些常量不需要分配存储空间）。 分析方向：正向分析 半格元素：有序对 $(x, v)$ 的集合，其中 $x$ 是变量名，$v$ 是变量的常数值（可能为常数、UNDEF 或 NAC）。 合并运算：上述控制流约束中的三种情况 最小元：所有变量映射到 UNDEF，表示初始状态下变量的值未知。 输出值：所有变量映射到 UNDEF，输入值是最小元，表示初始状态下没有任何信息。 转换函数：$OUT[s]=gen_s \\cup (IN[s] - \\{ (x ,\\_ ) \\})$ 对于赋值语句，更新变量的值。具体查阅上面的状态转移方程内容。 对于非赋值语句，变量的值保持不变。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:6:2","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"算法精度分析\r上述的四种数据流分析，前三者的转移方程满足分配性，而常量传播的转移方程不满足分配性，因此它们迭代算法的精度也不同。这里就涉及到算法的精度分析： ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:7:0","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"MOP 算法\r路径概念\r对于从程序入口沿着某条控制流到某个程序点 $s_i$ 处所经过的所有语句，可以记为 $P = \\text{ENTRY} \\to s_1 \\to s_2 \\to … \\to s_i$ ，称序列 $s_1s_2…s_i$ 是到程序点 $(s_i, s_{i + 1})$ 的一条 路径（Path）。而若该路径上每个语句的状态转移方程为 $f_{s_i}$，则路径 $P$ 的状态转移方程为 $$ F_P = f_{s_i} \\circ f_{s_{i - 1}} \\circ … \\circ f_{s_2} \\circ f_{s_1} $$ MOP\r数据流分析 全路交汇（Meet-Over-All-Paths） 方法用于计算程序点 $(s_i, s_{i+1})$ 处的数据流值 $MOP[s_i]$，其计算步骤如下： 确定从程序入口至 $s_i$ 的所有路径 $P$，并获取每条路径对应的状态转移方程 $F_P$，将这些路径的集合记作 $Paths(ENTRY, s_i)$； 对 $Paths(ENTRY, s_i)$ 中各路径的状态转移方程 $F_P$ 应用联合或交汇操作，以求得这些路径数据流值的最小上界或最大下界。其形式化表达为： $$ MOP[s_i] = \\bigvee_{\\forall P \\in Paths(ENTRY, s_i)} F_P(OUT[ENTRY]) $$ 或 $$ MOP[s_i] = \\bigwedge_{\\forall P \\in Paths(ENTRY, s_i)} F_P(OUT[ENTRY]) $$ 由于 CFG 中并不是每一条控制流都会被执行的，所以 MOP 算法并不是完全精确的。并且由于 MOP 算法需要考虑所有的路径，因此它也是不现实的。故而实际并不会使用 MOP 算法，而更多的把 MOP 当作是用于比较分析其他算法精度的标尺。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:7:1","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"MOP 与迭代算法\r这里通过一个例子比较两个算法 算法比较举例\r如果采用迭代算法进行分析，最终得到： $$ IN[s_4] = f_{s_3}(f_{s_1}(OUT[ENTRY]) \\vee f_{s_2}(OUT[ENTRY])) $$ 如果采用 MOP 算法，最终得到： $$ IN[s_4] = f_{s_3}(f_{s_1}(OUT[ENTRY])) \\vee f_{s_3}(f_{s_2}(OUT[ENTRY])) $$ 从上述例子可以发现，迭代算法求的是 $F(x\\vee y)$ ，而 MOP 算法求的是 $F(x) \\vee F(y)$（如果是交汇操作也是类似的）。简单而言，迭代算法就是先进行交汇，然后再使用状态转移方程；而 MOP 算法是先把每条路径采用状态转移方程计算，最后再进行交汇操作。根据格上的分配性定理，可以看出 $F(x) \\vee F(y) \\preceq F(x\\vee y)$ ，那么以 May Analysis 为例，迭代算法的精度不如MOP。 不过，当 $F(x)$ 满足分配性的时候，迭代算法的精度和 MOP 是一样的。上述四种数据流分析的前三种的状态转移方程就是满足分配行的，也就是说，在这些情景下，迭代算法可以达到 MOP 算法的精度，但是它的实现要比 MOP 简单得多。其实对于许多可以通过 $gen/kill$ 的视角来解决的问题，其状态转移都是满足分配性的。但是对于最后一种数据流分析，也就是常量传播分析，它的状态转移方程就不满足分配性，下面就是常量传播的一个例子： 对比举例\r对于上图，可以得到下面两个结果： $F(X \\wedge Y) = \\{(a, NAC), (b, NAC), (c, NAC)\\}$ $F(X) \\wedge F(Y) = \\{(a, NAC), (b, NAC), (c, 10)\\}$ 这里虽然 $F(X \\wedge Y) \\preceq F(X) \\wedge F(Y)$，与单调性是契合的。但是 $F(X \\wedge Y) \\ne F(X) \\wedge F(Y)$，它不满足分配性。因此其迭代算法的精度达不到 MOP 精度。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:7:2","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"工作表算法\r迭代算法\rWorkList 算法\r上述都是正向可能性分析的算法，但是后者工作表算法是对于前者迭代算法的优化。具体的优化就是使用一个集合存储下一次遍历中会发生变化的基本块，从而避免对已经达到不动点的基本块进行重复遍历。同时该 $WorkList$ 需要采用去重实现，防止出现重复的基本块导致不必要的重复计算。 此外，算法的最后一行仅将输出状态发生变化的基本块的后继（即下一轮输入状态会发生变化的基本块）加入工作表。这是因为只有输入状态发生变化时，输出状态才会相应变化，而 $gen_B$ 和 $kill_B$ 是预先计算的，它们固定不变。 本质上，该算法是图的广度优先遍历算法的变体，融入了剪枝逻辑，每轮仅遍历可能发生变化的节点，而将不发生变化的节点提前从遍历流程中剔除。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:8:0","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"Widening \u0026 Narrowing\r注意事项\r下面的内容大部分来自于 数据流分析基础，它是对 《软件分析技术》 课程的相关内容的总结。 标准数据流分析有可能收敛得很慢，甚至有可能因为半格的高度无限导致不收敛。为了让结果收敛得更快，可以使用 加宽（Widening） 的方法。加宽之后结果也会随之变得不精确，这时候可以使用 变窄（Narrowing） 的方法，进一步让结果变精确。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:9:0","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"区间分析\r这里使用区间分析作为例子分析 Widening 和 Narrowing。 区间分析\r区间分析是一种静态分析技术，用于确定程序中每个变量的取值范围。其核心思想是通过分析程序中的语句，逐步推导出每个变量的可能取值区间。区间分析通常采用正向分析的方式，从程序的入口开始，沿着控制流的方向逐步推导变量的区间，采用上近似。相关的信息如下： 分析方向：正向分析 半格元素：程序中每个变量的区间 合并运算：每个变量的区间对应求并，区间的并 定义为 $$ [a,b] \\sqcup [c,d] = [\\min(a,c), \\max(b,d)] $$ 最小元：每个变量都映射到 $L$ 上（$L$ 表示未定义或未知的区间，通常为 $[-\\infty, +\\infty]$），它表示初始状态下变量的取值范围未知 输入值：根据具体分析任务的输入上下界确定 转换函数：根据程序语句对区间进行计算 a = 0; for (int i = 0; i \u003c b; i ++){ a = a + 1; } return a; 对于上面的程序，其结果就是 $a : [0, +\\infty]$。根据这个程序可以知道，如果正向进行，那么变量的范围应当是逐渐扩大的。根据上述合并运算，该区间会发生变化。例如添加一个变量： $$ [a, b] + [c, d] = [a + c, b + d] \\\\ [a, b] - [c, d] = [a - d, b - c] $$ 但是这样构成的半格高度是无限的，而且操作也不能保证单调性，因此提出了改进策略 —— 人为指定上下界。如果下界大于类型的最大值，那么为空值，程序异常；如果下界小于最大值，那么更新下界和上界，其中最大值之和与类型最大值取其中小者。 $$ \\begin{cases} [a, b] + [c, d] = \\emptyset \u0026 a + c \u003e \\text{int\\_max}\\\\ (a + c, \\min(b + d, \\text{int\\_max})) \u0026 a + c \\leq \\text{int\\_max} \\end{cases} $$ ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:9:1","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"基础 Windening\r但是在数据流分析中，某些情况下状态空间可能非常大，导致分析过程收敛缓慢。例如，在区间分析中，变量的区间可能无限扩大。而 基础 Widening 是一种用于加速数据流分析收敛的技术。它的核心思想是通过 降低结果的精度 来减少分析的状态空间，从而加快分析的收敛速度。具体来说，Widening 通过引入一个 单调函数 $w$，将原本的半格映射为一个新的、高度更低的格，从而减少分析过程中的状态数，即 $f \\leftarrow w \\circ f$。例如在上述区间分析中可以预定范围： 定义有限集合：$B = \\{ -\\infty, 10, 20, 50, 100, +\\infty \\}$； 定义映射函数：$w: w([l, h]) = \\big[ \\max \\{i \\in B \\mid i \\leq l \\}, \\min \\{i \\in B \\mid h \\leq i \\} \\big]$，它将原始的区间映射为 $B$ 中的区间； 这样就可以把区间 $[15, 75]$、$[11, 90]$ 等映射为 $[10, 100]$ 了，即 $w([15, 75]) = [10, 100]$。此时由于状态数大幅度减少，所以很大程度上加快了收敛速度。 再对于下面的代码对比是否应用基础 Windening 的区别： y = 0; while(input){ x = 7; x = x + 1; y = y + 1; } 这里有限集合为：$\\{−\\infty, 0, 1, 7, +\\infty \\}$，那么每次循环 while(input) 处语句的状态如下所示： 结果对比\r这里基础 Widening 的安全性指通过 Widening 技术得到的结果不会丢失原始分析的正确性，具体表现为 Widening 的结果必须是原始结果的 上近似，即满足 $w(x) \\sqsubseteq x$，其中 $x$ 是原始分析的结果，$w(x)$ 是 Widening 后的结果。安全性得以保证的原因是 Widening 函数 $w$ 是单调的，即如果 $x \\sqsubseteq y$，则 $w(x) \\sqsubseteq w(y)$，因此 Widening 后的结果不会比原始结果更精确，但也不会丢失原始结果的正确性。例如，在区间分析中，若原始结果为 $[15, 75]$，Widening 后的结果为 $[10, 100]$，则 $[10, 100]$ 包含了 $[15, 75]$，从而确保了正确性。此外，Widening 的收敛性是指通过 Widening 技术，分析过程能够在有限的步骤内收敛到一个固定点。这是因为 Widening 函数 $w$ 将状态空间映射为一个有限的集合，且 $w$ 是单调的，避免了分析过程中的振荡或无限循环，从而保证了分析过程必然会在有限的步骤内收敛。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:9:2","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"一般 Windening\r一般 Widening 是对基础 Widening 的改进，旨在在 精度 和 收敛速度 之间取得更好的平衡。与基础 Widening 不同，一般 Widening 不仅参考更新前的值，还参考更新后的值，从而更智能地猜测最终收敛的值。其核心思想是通过自定义的 Widening 算子 $\\nabla$，在保证安全性和收敛性的同时，提高分析的性能。 定义 Widening 算子 $\\nabla$，满足：$\\text{DATA}_v \\leftarrow \\text{DATA}_v \\nabla f_v(\\text{MEET}_v)$。其中： $\\text{DATA}_v$ 是节点 $v$ 的当前值； $f_v$ 是节点 $v$ 的转换函数； $\\text{MEET}_v$ 是节点 $v$ 的前驱节点的交汇结果。 Widening 算子 $\\nabla$ 的具体规则由用户定义，通常基于对程序行为的经验或启发式规则。 之后对于同样的代码： y = 0; while(input){ x = 7; x = x + 1; y = y + 1; } 就可以更改节点的交汇规则，这里的 $\\top$ 表示格中最高层的元素。 $$ [a, b] \\nabla \\top = [a, b] \\\\ \\top \\nabla [c, d] = [c, d] \\\\ [a, b] \\nabla [c, d] = [x, y] \\quad \\text{where} \\\\ x = \\begin{cases} a \u0026 c \\geq a \\\\ -\\infty \u0026 c \u003c a \\end{cases} \\\\ y = \\begin{cases} b \u0026 d \\leq b \\\\ +\\infty \u0026 d \u003e b \\end{cases} $$ 由此可以将这些结果进行比对： 结果比对\r这里一般 Widening 的安全性通过确保 Widening 算子 $\\nabla$ 满足 $(x \\nabla y) \\sqsubseteq x$ 且 $(x \\nabla y) \\sqsubseteq y$ 来保证，即 Widening 的结果是原始结果的上近似，因此不会丢失正确性。例如，在区间分析中，若 $x = [10, 20]$，$y = [15, 25]$，则 $x \\nabla y$ 的结果可能是 $[10, 25]$，这包含了 $x$ 和 $y$ 的所有可能值。然而，一般 Widening 的收敛性难以通用保证，因为 Widening 算子 $\\nabla$ 的行为与转换函数 $f_v$ 的趋势密切相关，且不一定在半格上保持单调性，可能导致分析过程振荡或无法收敛。特别是在多条路径交汇时，如果交汇操作是逐个进行的，处理前驱节点的顺序可能影响最终结果，进一步增加了不收敛的风险。尽管如此，通过合理设计 Widening 算子 $\\nabla$，一般 Widening 在实践中通常能够在精度和收敛速度之间取得较好的平衡，从而提高数据流分析的性能。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:9:3","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"Narrowing\rNarrowing 是一种用于提高 Widening 结果精度的技术。Widening 通过扩大区间来加速收敛，但可能导致结果过于粗略；而 Narrowing 则通过再次应用原始转换函数对 Widening 的结果进行修正，从而缩小区间，提高精度。Narrowing 的核心思想是在某些步骤中不再使用 Widening，而是直接应用原始转换函数，从而逐步逼近更精确的结果。 定义 Narrowing 算子 $\\Delta$，满足： $\\text{DATA}_v \\leftarrow \\text{DATA}_v \\Delta f_v(\\text{MEET}_v)$。其中： $\\text{DATA}_v$ 是节点 $v$ 的当前值。 $f_v$ 是节点 $v$ 的原始转换函数。 $\\text{MEET}_v$ 是节点 $v$ 的前驱节点的交汇结果。 Narrowing 算子 $\\Delta$ 的具体规则由用户定义，通常用于缩小区间。 对于下面的代码： y = 0; x = 7; x = x + 1; while(input){ x = 7; x = x + 1; y = y + 1; } Narrowing 就是在某一步就不采用 Widening 的方法，而是用原始的函数。例如下图 x 的值的区间在第 5 步突然缩小了： Narrowing 结果\r而根据上面算子的定义，可以得到下面的规则。这里的含义就是已经收敛到的整数不进行变动，只重新计算被 Widening 扩展到无穷大的情况。 $$ [a, b] \\Delta [c, d] = [x, y], \\text{ where} \\\\ x = \\begin{cases} a \u0026 a \\neq -\\infty \\\\ c \u0026 a = -\\infty \\end{cases} \\\\ y = \\begin{cases} b \u0026 b \\neq +\\infty \\\\ d \u0026 b = +\\infty \\end{cases} $$ 这里 Narrowing 的安全性通过确保 Narrowing 算子 $\\Delta$ 满足 $x \\Delta y \\sqsubseteq y$ 来保证（$x$ 是 Narrowing 前的值，$y$ 是 Narrowing 后的值），即 Narrowing 的结果是原始结果的下近似，因此不会丢失正确性。例如，在区间分析中，若 Widening 后的结果是 $[10, 100]$，Narrowing 后的结果可能是 $[20, 80]$，这仍然包含了实际的可能值。具体推导可以参考下图： Narrowing 的安全性\r然而，Narrowing 的收敛性无法保证，即使收敛，也不能保证快速收敛。这是因为 Narrowing 的结果依赖于 Widening 的不动点，而 Widening 的不动点可能已经丢失了一些精度；同时，Narrowing 算子 $\\Delta$ 的行为与转换函数 $f_v$ 的趋势密切相关，可能导致分析过程振荡或无法收敛。尽管如此，Narrowing 在 Widening 的基础上能够逐步缩小区间，提高精度，从而在精度和收敛速度之间取得更好的平衡。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:9:4","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"其余参考资料\r参考\r静态分析概述 数据流分析-应用 数据流分析-基础 （二）数据流分析基础 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/data-flow-analysis/:10:0","tags":["Static Analysis"],"title":"03 Data Flow Analysis","uri":"/blog/posts/staticanalysis/data-flow-analysis/"},{"categories":["Static Analysis"],"content":"记录一下对于控制流分析的理解。 注意事项\r下面的内容大部分来自于 程序分析与优化 - 2 控制流图，它是对 《DCC888》 课程中相关内容的总结。重点围绕 LLVM 中端优化器（Optimizer）的优化技术进行阐述与介绍。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:0:0","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"控制流分析\r注意事项\r以下内容主要参考了 程序的中间表示 的内容，主要说明控制流图的构建方法。 控制流分析（Control Flow Analysis, CFA） 通常是指构建 控制流图（Control Flow Graph，CFG） 的过程。上篇文章使用 Soot 和 LLVM 得到了中间表示 IR，之后使用 IR 转化为了控制流图，而下图也是这样的过程，转化的方法就是划分基本块。 控制流图构建\r","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:1:0","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"基本块\rCFG 是静态分析的基础，而在上图可以看出，将 IR 按照相应规则划分为数个指令快，之后采用箭头显示控制就构成了 CFG。这里的指令块就被称为基本块。 基本块\r记一个程序 P 在 IR 表示下 的指令序列为 $P = \\{a_1, a_2, …, a_n\\}$，这里 P 是一个有序集，那么 $IN_i = \\{a_j | next(a_j) = a_i\\}$，其中 $next(a_j)$ 表示控制流中 $a_j$ 的下一条指令； $OUT_i = \\{a_j \\mid prev(a_j) = a_i\\}$，其中 $prev(a_j)$ 表示 $a_j$ 的上一条指令。 如果连续的指令序列 $a_p, a_{p+1}, a_{p+2}, …, a_q$ 满足如下性质： $$ \\forall i \\in [p+1, q], IN_i = {a_{i-1}} \\land \\forall i \\in [p, q-1], OUT_i = {a_{i+1}} $$ 并且 $a_{p-1}, a_p, …, a_{q-1}, a_q$ 和 $a_p, a_{p+1}, …, a_q, a_{q+1}$ 都不满足上述性质，则称 $\\{a_p, a_{p+1}, …, a_q\\}$ 为 基本块（Basic Block）。 简单而言，基本块就是满足以下性质的最长指令序列： 程序的控制流只能从首指令进入，不能存在跳转指令跳入执行基本块中的某行指令。 程序的控制流只能从尾指令流出，只有最后一条指令允许包含离开基本快的分支或者挂机指令。 由上面的定义，可以知道跳转指令会将一个完成的程序切割为几个基本快，所以只需要将基本快的分割点找到，那么整个程序就可以按照分割点划分为数个基本块了。而这个分割点就是 基本块的首领（leader）。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:1:1","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"基本块的首领\r基本块首领\r对于 IR 表示下的程序 $P = \\{a_1, a_2, …, a_n\\}$，考虑某个基本块 $B = \\{a_p, a_{p+1}, …, a_{q-1}, a_q\\}$，称 $a_p$ 为 $P$ 的基本块 $B$ 的 首领（Leader），而程序 $P$ 中所有的首领组成的集合为 $L$，则 $$ L = \\{a_1\\} \\cup \\{a_i \\mid type(a_i) = jump \\land target(a_i) \\} \\cup \\{a_{i+1} \\mid type(a_i) = jump\\} $$ 其中，$type(a_i)$ 表示指令 $a_i$ 的类型，$jump$ 类型是跳转指令，包括 条件跳转（Conditional Jump） 和 无条件跳转（Unconditional Jump）。 $target(a_i)$ 仅用于 $a_i$ 是跳转指令的时候，表示 $a_i$ 的目标指令。 简单而言，首领就是每个基本块的首指令，其可以分为三种类型： 整个程序的首指令。 跳转指令（包括条件跳转和无条件跳转）的目标指令。 紧邻跳转指令（包括条件跳转和无条件跳转）的下一条指令。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:1:2","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"基本块的界定\r基本块的界定\r对于程序 $P = \\{a_1, a_2, …, a_n\\}$ 而言，其所有的首领构成的集合为 $L$，则 $$ a_p \\in L \\land a_{q+1} \\in L \\land \\forall a_i (i \\in [p+1, q]), a_i \\notin L \\iff \\{a_p, a_{p+1}, …, a_{q-1}, a_q\\} $$ 得到的指令序列就是一个基本块。 简单而言，基本块的首领就是基本块之间的分割线，从一个基本块的首领到紧接着的下一个基本块的首领之前的所有指令组成了一个基本块。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:1:3","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"局部优化\r在 Basic Block 作用域内部的优化被称为 局部优化（Local Optimization），也就是将优化限制在单个基本块的上下文中，例如 基于 DAG 的优化（DAG based optimizations）、窥孔优化（Peephole optimizations）、局部寄存器分配（Local register allocation）。而基于整个程序的控制流图进行的优化被称为 全局优化（Global Optimization），静态分析的优化大部分为全局优化，这里就对局部优化进行介绍。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:1:4","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"基于 DAG 的优化\r代码优化技术是需要直接分析一整个程序的 CFG，所以一般不怎么使用 DAG 的方式来进行数据结构的优化。但是下面介绍的两种优化方法不仅仅只是使用在 DAG 优化中，而在静态分析中也有很大的作用，所以这里只对优化方式进行简单介绍。这里 DAG 就是指 有向无环图（Directed Acyclic Graph），在图论中，如果一个有向图从任意顶点触发无法经过若干条边回到该点，则这个图是一个有向无环图，DAG 如下图所示： 有向无环图\r而对于上述程序而言，DAG 的构造涉及程序指令的含义： 每个输入值对应 DAG 中的一个结点，例如 a = b + c，这里的 b，c 就是输入值，需要作为结点出现。 基本块中的每行指令生成一个结点，例如 a = b + c，会产生 +,a 这样的结点。 如果指令 $S$ 用到了指令 $S_1, \\ldots ,S_n$ 中的变量，则需要一条从 $S_1，i \\in \\{1, \\ldots, n\\}$ 到 $S$ 的边。 基本块中定义但未在基本块中使用的变量称为输出值，这些值可能会被后续基本块使用，因此为输出值。 a = b + c b = a - d c = b + c d = a - d 对于上面的基本快指令，会生成下面的 DAG： 程序的 DAG\r","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:2:0","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"公共子表达式消除\r对于上述程序，可以直观看到 a - d 被利用了两次，并且其返回的结果不变，因此可以将其视为公共子表达式，从而进行 公共子表达式消除（Common subexpression elimination）。它们的值相同，那么其赋值对象 b 和 d 就是别名关系，于是可以直接使用上面计算的结果 b 直接赋值给 d，不必再进行表达式的计算。而 b + c 就不可以再次利用，因为两个表达式之间存在 b = a - d，它对表达式的其中一个变量进行了重新赋值，因此不是相同表达式。 在实际应用过程中，采用 值标记 方法来计算相同子表达式： 对于 DAG 的每个结点关联一个 签名（$lb, v_1, \\ldots, v_n$），其中 $lb$ 是该结点的标签，$v_i(1 \\leq i \\leq n)$ 是该节点的所有子结点。 将签名中的子结点序列作为 hash 函数的 key； hash 函数的值就是该变量的值标记 当有新的结点加入 DAG 时，先根据它的所有子结点计算出一个 hash 值， 如果已经存在，直接返回该 hash 值对应的索引； 如果找不到，则创建该结点。 根据上面的方法，生成的值标记的 hash 表如下，可以看出最后一列显然是不必要的： 表达式 b c d a = b + c b = a - d c = b + c d = a - d hash key b c d (+,1, 2) (-,4,3) (+,5, 2) (-,4,3) value number 1 2 3 4 5 6 5 总的而言，上述方法就是在构建 DAG 的每一条语句的结点时，如果语句之前未出现过，则构建新的结点，如果是出现过相同 hash 的结点，则直接将新的结点指向之前那个相同hash 的结点。因此就可以把下图中的 -,d 结点直接删除，然后标识 d 变量的数据就和 -,b 结点的数据一样。 公共子表达式消除\r为了找到更多的 CSE（Common SubExpressions），需要指定更多的定理： 交换律：对 $+$ 运算符，$x + y$ 和 $y + x$ 等同。 特性转换：$x\u003cy$ 一般转换成 $t = x - y; \\ t \u003c 0$。 结合律: 对 $a = b + c;$，$t = c + d;$，$e = t + b;$ 等同于 $a = b + c;$，$e = a + d;$。 算术特性转换：$x + 0 = 0 + x = x;$，$x * 1 = 1 * x = x;$，$x - 0 = x;$，$x / 1 = x;$。 计算强度降维转换：$x^2 = x * x;$，$2 * x = x + x;$，$x / 2 = x * 0.5$。 常量折叠：在编译阶段计算表达式的值，并将表达式替换成对应的值。这个也是后面数据流分析中主要介绍的方法。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:2:1","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"死代码消除\r死代码（Dead Code） 是指程序中不可达的代码，即不会被执行的代码，或者是执行结果永远不会被其他计算过程用到的代码。对于 DAG 而言，满足下面两个消除条件就可以将相应代码进行删除： 结点没有后继结点，即没有父结点，例如 root node。 该结点不是输出结点。 那么由上面的条件，如果下图中的 -,d 不是输出值，那么就可以进行删除。同时该清除过程也是多轮迭代，直至不动点才停止的。如果下面 +,c 不是输出值，那么它也会被删除，然后在后面的迭代中，会发现 -,b 也满足清除条件。它也会被清除，再次迭代后发现和上次没有变化，视为达到了不动点，迭代停止，获得了死代码清除的结果。 死代码消除\r","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:2:2","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"窥孔优化\r窥孔是指程序中存在的一个小的滑动窗口，窥孔优化（Peephole Optimization） 是指优化器在分析指令序列时，每次只检查一个固定大小的滑动窗口内的指令。随着窗口不断滑动，如果发现符合某种优化模式的指令序列，则执行优化。常见的优化模式包括冗余指令消除、控制流优化、代数简化、以及机器特定指令的替换等。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:3:0","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"消除冗余加载和存储指令\rload R0, m store m, R0 在同一个基本块中，如果上述加载和存储指令相连且没有对值进行任何修改，则可以直接删除整个指令序列，因为它们对程序状态没有实际影响，从而避免无意义的操作，提升执行效率。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:3:1","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"控制流优化\r# original goto L1 ... L1: goto L2 # optimized goto L2 ... L1: goto L2 # faulty optimization goto L2 ... 在上述的情况中，L1 只存在一条跳转语句，所以开始的跳转语句可以直接跳转到最终的标签语句处。而后面直接将 L1 标签语句删除的做法不被窥孔优化支持，因为窥孔只能看一个滑动窗口内的指令，它不能保证是否其他地方会使用这个 L1 标签，所以它不会被优化删除。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:3:2","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"消除级联跳转指令\r# original if debug == 1 goto L1 goto L2 L1: ... goto end L2: ... end: ... # optimized if debug != 1 goto L2 L1: ... goto end L2: ... end: ... 对于上述不在同一个基本块的情况，也可以应用窥孔优化。从代码中可以看出，if 语句后对 L1 和 L2 添加了跳转语句，但实际上可以将满足 if 条件时执行的语句直接放在 if 语句之后，无需额外的跳转，从而简化控制流逻辑。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:3:3","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"代数化简和强度消减\r代数恒等式可以简化 DAG，同时也可以被窥孔优化器用来消除一些语句。如下就将一些冗余的操作直接优化为简洁的表达。 x = x + 0; x = x + 1; 同时窥孔优化支持强度消减，也就是把代价较高的运算替换为目标机器上代价较小的等价运算。例如，将除数为常数的浮点除法替换为常量为倒数的乘法，将幂函数替换为移位运算符。运算强度排序为：除法 \u003e 乘法 \u003e 减法 \u003e 移位/加法。 使用机器习语\r许多计算机体系结构中，都有一些常见操作的有效指令，使用机器特有的指令可以显著减少运行时间。例如自增或自减指令。 add1 $1, %edi ;加 1 的操作可以替换为下面的自增指令 inc1 %edi ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:3:4","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"局部寄存器优化\r寄存器是处理器中存取速度最快的存储单元，但数量有限，例如在 32 位 x86 架构中，只有 8 个可用寄存器。由于寄存器运算的速度远快于内存操作，现代处理器的运行速度通常比内存快一个数量级以上，因此，将程序中使用最频繁的变量直接映射到寄存器中，可以显著提升性能。但是这种优化方式通常需要编译器对整个函数或程序进行全局分析，才能实现高效的寄存器分配。 allocate(Block b) { for (Inst i : b.instructions()) { for (Operand o : i.operands()) { if (o is in memory m) { r = find_reg(i) assign r to o add \"r = load m\" before i } } for (Operand o : i.operands()) { if (i is the last use of o) { return the register bound to o to the list of free registers } } v = i.definition r = find_reg(i) assign r to v } } 上述就是一个进行局部寄存器分配优化的伪代码，其主要目的是通过将变量映射到寄存器来提高程序的执行效率。它首先由 if (o is in memory m) 语句判断当前指令是否需要从内存中加载数据，如果是，那么就执行 find_reg(i) 来查找寄存器 r，用来存储当前操作数 o。这个函数会根据当前指令和上下文来选择一个适合的寄存器。之后在当前指令 i 前插入一条 r = load m 的指令，用于将内存中的数据加载到寄存器 r 中。 而后面的循环则是通过 if (i is the last use of o) 判断当前操作数 o 是否是其最后一次使用。如果是，那么当前寄存器分配的寄存器 r 可以释放，即将寄存器 r 归还给空闲寄存器列表，表明该寄存器可以重新使用。 最后的语句则是使用 find_reg(i) 来查找寄存器，从而将寄存器 r 分配给当前指令定义的值 v，使得该值的结果存储在寄存器中。 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:4:0","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"Spilling\rfind_reg(i) { if (there is free register r) { return r } else { // spilling let v be the latest variable to be used after i, that is in a register } if (v does not have a memory slot) { let m be a fresh memory slot } else { let m be the memory slot assigned to v } add \"store v m\" right after the definition of v return r } 上图就是 find_reg(i) 函数的实现，可以看出它根据指令 i 的需求来选择一个合适的寄存器来存储变量或操作数。如果没有可用的寄存器，它会寻找最合适的寄存器或内存位置来存储数据，并返回一个寄存器或内存位置供指令使用。在这里包含一个操作 Spilling，它是一个变量映射到内存的过程。因为物理机器的寄存器的个数有限，所以当寄存器不够用时，就需要将之前保存在寄存器里面的内容映射会内存。 对于上述代码，首先检查是否存在空闲寄存器，若是存在就直接返回这个寄存器。否则就是执行 spilling 操作，进行寄存器的回收。这里的 v 是指当前指令 i 执行之后，存储在寄存器中且最晚会被使用的变量。对于这个寄存器的回收采用的就是 Belady’s Algorithm，也就是 最远未来使用（LRU） 策略。 之后进行后续操作，回收寄存器的变量如果没有内存空间，那么就创建一个内存空间分配给回收寄存器后的变量 v。最后之后插入 store v m 指令，表示把变量 v 存储到了 m 的内存空间。这样，变量 v 的值就从寄存器中存回内存，确保它的值不会丢失，并且寄存器可以被其他变量使用。 上文提起 局部寄存器分配（Local Register Allocation） 通常需要编译器对整个函数或程序进行全局分析，才能实现高效的寄存器分配。这是因为在分配时，需要关注一个变量是否会在一个基本块之后使用，因为一个基本块不能包含一整个函数，所以寄存器的分配起码需要在函数范围上进行使用才能真正实现优化。 a = load m0 b = load m1 c = a + b d = 4 * c e = b + 1 f = e + a g = d / e h = f - g ret h 对于只有两个寄存器的机器，实现上述的指令运算的过程如下图所示： 寄存器分配示例\r","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:4:1","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["Static Analysis"],"content":"其余参考资料\r参考\rLecture02 Coutrol Flow Graphs Enflame编译优化培训(基于DCC888) 第二课 ","date":"2024.12.22","objectID":"/blog/posts/staticanalysis/control-flow-analysis/:5:0","tags":["Static Analysis"],"title":"02 Control Flow Analysis","uri":"/blog/posts/staticanalysis/control-flow-analysis/"},{"categories":["programming"],"content":"记录一下 java 的学习。 ","date":"2024.12.20","objectID":"/blog/posts/programming/java/:0:0","tags":["programming"],"title":"Some about Java","uri":"/blog/posts/programming/java/"},{"categories":["programming"],"content":"基础知识\r相关基础知识可以从 菜鸟教程，二哥的Java进阶之路，cs61b 中进行学习，这些都很详细。对于 cs61b 可以直接看 相关笔记1，相关笔记2。下面主要记录 cs61b 中的相关结构源码与知识点辨析。 ","date":"2024.12.20","objectID":"/blog/posts/programming/java/:1:0","tags":["programming"],"title":"Some about Java","uri":"/blog/posts/programming/java/"},{"categories":["programming"],"content":"List and Deque\r对于 java 的列表和队列不太了解，所以这里进行相关记录。 ","date":"2024.12.20","objectID":"/blog/posts/programming/java/:2:0","tags":["programming"],"title":"Some about Java","uri":"/blog/posts/programming/java/"},{"categories":["programming"],"content":"SLList\rSLList 就是 Singly Linked List，即单向链表，代码如下所示。可以看出： 内部类 Node：对于链表的单个结构进行封装的结果，其类型为 private，保证了封闭性。 哨兵 sentinel：用来维护下面 addFirst 和 addLast 的一致性。正是因为这个哨兵节点的存在，他们可以对所有节点保持一样的操作，而不必考虑一开始没有节点的情况。 列表大小 size：记录当前列表的大小，因为在每次操作都会记录，所以可以直接通过 size() 获取大小。这样添加一个变量定理，遏制了列表从头遍历获取长度的操作，使长度获取更快速。 同时由下可以看出，列表对于最后一个元素的添加和获取速度很慢，但是其可以动态分配内存、插入和删除效率高，无连续内存要求。 public class SLList\u003cItem\u003e { private class Node { public Item item; public Node next; public Node(Item i, Node n) { item = i; next = n; } } /* The first item (if it exists) is at sentinel.next. */ private Node sentinel; private int size; /** Creates an empty timingtest.SLList. */ public SLList() { sentinel = new Node(null, null); size = 0; } public SLList(Item x) { sentinel = new Node(null, null); sentinel.next = new Node(x, null); size = 1; } /** Adds x to the front of the list. */ public void addFirst(Item x) { sentinel.next = new Node(x, sentinel.next); size += 1; } /** Returns the first item in the list. */ public Item getFirst() { return sentinel.next.item; } /** Adds x to the end of the list. */ public void addLast(Item x) { size += 1; Node p = sentinel; /* Advance p to the end of the list. */ while (p.next != null) { p = p.next; } p.next = new Node(x, null); } /** returns last item in the list */ public Item getLast() { Node p = sentinel; /* Advance p to the end of the list. */ while (p.next != null) { p = p.next; } return p.item; } /** Returns the size of the list. */ public int size() { return size; } } ","date":"2024.12.20","objectID":"/blog/posts/programming/java/:2:1","tags":["programming"],"title":"Some about Java","uri":"/blog/posts/programming/java/"},{"categories":["programming"],"content":"AList\rAList 就是 Array List，即数组列表，代码如下所示。可以看出： 列表大小 size：记录当前列表的大小，因为在每次操作都会记录，所以可以直接通过 size() 获取大小。 扩容 resize：进行扩容的函数，当 addLast 时发现大小不够时，就通过该函数对数组大小进行扩容处理。但是该操作是创建一个新数组，然后将原本的内容进行复制处理，所以对于大量数据速度会很慢。 使用数组的列表结构，可以快速随机访问，能直接通过索引进行访问，并且简单直观。但是扩容成本高，内存一开始开辟，填不满浪费空间，同时也不适合平凡的插入和删除。 public class AList\u003cItem\u003e { private Item[] items; private int size; /** Creates an empty list. */ public AList() { items = (Item[]) new Object[100]; size = 0; } /** Resizes the underlying array to the target capacity. */ private void resize(int capacity) { Item[] a = (Item[]) new Object[capacity]; System.arraycopy(items, 0, a, 0, size); items = a; } /** Inserts X into the back of the list. */ public void addLast(Item x) { if (size == items.length) { resize(size * 2); } items[size] = x; size = size + 1; } /** Returns the item from the back of the list. */ public Item getLast() { return items[size - 1]; } /** Gets the ith item in the list (0 is the front). */ public Item get(int i) { return items[i]; } /** Returns the number of items in the list. */ public int size() { return size; } /** Deletes item from back of the list and * returns deleted item. */ public Item removeLast() { Item x = getLast(); items[size - 1] = null; size = size - 1; return x; } } ","date":"2024.12.20","objectID":"/blog/posts/programming/java/:2:2","tags":["programming"],"title":"Some about Java","uri":"/blog/posts/programming/java/"},{"categories":["programming"],"content":"LinkedListDeque\r下面的代码实现了双端队列，他就是基于上面 SLList 的基础上进行改进的。 哨兵 sentinel：这里的哨兵和上文的作用一样，但是这里采用了更为巧妙的方法。这里对于双端队列，只设置一个哨兵，这样通过哨兵关联头尾，形成了一个环，这样就能保证双端队列的添加一致性。 这里双端队列相比于上文的单向链表，对于尾端的插入迅速，不必再遍历整个链表。 import java.util.Iterator; public class LinkedListDeque\u003cT\u003e implements Iterable\u003cT\u003e { // define the basic struct private class Node { T item; Node prev; Node next; Node(T item, Node prev, Node next) { this.item = item; this.prev = prev; this.next = next; } } // the first item (if it exits) is at sentinel.next // and the last item is at sentinel.prev private Node sentinel; private int size; public LinkedListDeque() { sentinel = new Node(null, null, null); sentinel.next = sentinel; sentinel.prev = sentinel; size = 0; } @Override public void addFirst(T item) { Node tempItem = new Node(item, sentinel, sentinel.next); sentinel.next.prev = tempItem; sentinel.next = tempItem; size += 1; } @Override public void addLast(T item) { Node tempItem = new Node(item, sentinel.prev, sentinel); sentinel.prev.next = tempItem; sentinel.prev = tempItem; size += 1; } @Override public int size() { return size; } @Override public void printDeque() { Node p = sentinel; while (p.next != sentinel) { System.out.print(p.item + \" \"); p = p.next; } System.out.println(); } @Override public T removeFirst() { if (size == 0) { return null; } T value = sentinel.next.item; sentinel.next.next.prev = sentinel; sentinel.next = sentinel.next.next; size -= 1; return value; } @Override public T removeLast() { if (size == 0) { return null; } T value = sentinel.prev.item; sentinel.prev.prev.next = sentinel; sentinel.prev = sentinel.prev.prev; size -= 1; return value; } // get the item at the given index @Override public T get(int index) { if (index \u003c 0 || index \u003e= size) { return null; } Node p = sentinel.next; for (int i = 0; i \u003c index; i++) { p = p.next; } return p.item; } // same as get, but uses recursion public T getRecursive(int index) { if (index \u003c 0 || index \u003e= size) { return null; } return recursiveHelper(sentinel.next, index); } private T recursiveHelper(Node p, int index) { if (index == 0) { return p.item; } return recursiveHelper(p.next, index - 1); } // Implemente Iterator @Override public Iterator\u003cT\u003e iterator() { return new DLLListIterator(); } private class DLLListIterator implements Iterator\u003cT\u003e { private Node current; private DLLListIterator() { current = sentinel.next; } @Override public boolean hasNext() { return current != sentinel; } @Override public T next() { T returnItem = current.item; current = current.next; return returnItem; } } @Override public boolean equals(Object o) { if (this == o) { return true; } if (!(o instanceof Deque\u003c?\u003e)) { return false; } Deque\u003c?\u003e deque = (Deque\u003c?\u003e) o; if (size != deque.size()) { return false; } for (int i = 0; i \u003c size; i++) { if (!get(i).equals(deque.get(i))) { return false; } } return true; } } ","date":"2024.12.20","objectID":"/blog/posts/programming/java/:2:3","tags":["programming"],"title":"Some about Java","uri":"/blog/posts/programming/java/"},{"categories":["programming"],"content":"ArrayDeque\r下面的代码基于上述的 AList，使用数组来实现双向队列。 nextFirst：这个变量一直指向第一个节点的上一个位置。当添加第一个节点时，就是添加数据到这个变量指向位置的下一个地址，然后这个变量就会继续指向前面一格的位置。 nextLast：这个变量则是一直执行最后一个节点的后一个位置。添加最后一个节点就是添加到它指向位置的前面，然后它会往后移动。 size 是当前数组中存在的真实数据的个数，数组中空的元素不算在里面，而 item.length 就是整个数组一开始开辟的大小，它是一个固定的值。 checkExpand，checkEmptySpace 和 resize 就是进行队列扩容缩放的操作，前二者就是对于当前队列大小进行判断的函数，然后在插入前和删除后进行相关判断。他们调用的就是 resize，它通过创建一个新的数组，然后将原来的数据复制进行来实现扩容缩放。 import java.util.Iterator; public class ArrayDeque\u003cT\u003e implements Iterable\u003cT\u003e { // define the basic struct private T[] items; private int size; // the number of items in the array // items.length -\u003e the size of the position in the array, and it is the capacity private final int INIT_CAPACITY = 8; // init size private final int MAX_CAPACITY = 16; // max size // nextFirst points to the position before the first item. private int nextFirst; // nextLast points to the position after the last item. private int nextLast; public ArrayDeque() { items = createArray(INIT_CAPACITY); size = 0; nextFirst = INIT_CAPACITY - 1; nextLast = 0; } // update the index private int nextIndex(int index) { return (index + 1) % items.length; } private int prevIndex(int index) { return (index - 1 + items.length) % items.length; } // check if it needs expand. private void checkExpand() { if (size != items.length) { return; } resize(size * 2); } // check if empty space exists. private void checkEmptySpace() { if (items.length \u003e= MAX_CAPACITY \u0026\u0026 size \u003c 0.25 * items.length) { resize(items.length / 2); } } @SuppressWarnings(\"unchecked\") private T[] createArray(int capacity) { return (T[]) new Object[capacity]; } private void resize(int newCapacity) { T[] newItems = createArray(newCapacity); int first = nextIndex(nextFirst); // if it not is a circular if (first \u003c= prevIndex(nextLast)) { System.arraycopy(items, first, newItems, 0, size); } else { int firstPart = items.length - first; System.arraycopy(items, first, newItems, 0, firstPart); System.arraycopy(items, 0, newItems, firstPart, nextLast); } items = newItems; // since the copy happens within the range from 0 to size, // so the nextFirst points of the last position of the newCapacity. nextFirst = newCapacity - 1; // the nextLast points the position right the previous size. nextLast = size; } @Override public void addFirst(T item) { checkExpand(); items[nextFirst] = item; nextFirst = prevIndex(nextFirst); size += 1; } @Override public void addLast(T item) { checkExpand(); items[nextLast] = item; nextLast = nextIndex(nextLast); size += 1; } @Override public int size() { return size; } @Override public void printDeque() { int index = nextIndex(nextFirst); for (int i = 0; i \u003c size; i++) { System.out.print(items[index] + \" \"); index = nextIndex(index); } System.out.println(); } @Override public T removeFirst() { if (size == 0) { return null; } // nextIndex(nextFirst) points the first item int firstIndex = nextIndex(nextFirst); // cache calculation results T item = items[firstIndex]; items[firstIndex] = null; // the original position of first item becomes the position for the next addFirst insertion. nextFirst = firstIndex; size -= 1; checkEmptySpace(); return item; } @Override public T removeLast() { if (size == 0) { return null; } int lastIndex = prevIndex(nextLast); T item = items[lastIndex]; items[lastIndex] = null; nextLast = lastIndex; size -= 1; checkEmptySpace(); return item; } // get the item at the given index @Override public T get(int index) { if (index \u003c 0 || index \u003e= size) { return null; } int actualIndex = (nextFirst + 1 + index) % items.length; return items[actualIndex]; } @Override public Iterator\u003cT\u003e iterator() { return new ArrayDequeIterator(); } private class ArrayDequeIterator implements Iterator\u003cT\u003e { private int index; private ArrayDequeIterator() { index = nextIndex(nextFirst); } @Override public boolean hasNext() { return index != nextLast; } @Override public T next() { T returnItem = items[index]; index = nextIndex(index); return returnItem; } } @Override public boolean equals(Object o) { if (this","date":"2024.12.20","objectID":"/blog/posts/programming/java/:2:4","tags":["programming"],"title":"Some about Java","uri":"/blog/posts/programming/java/"},{"categories":["Static Analysis"],"content":"记录一下对于静态分析的理解。 前提说明\r由于我的知识面受限，所以关于当前系列文章做出以下解释： 在计算机科学中，程序分析 是指自动分析一个程序的包括正确性、健壮性、安全性和活跃性等特征的过程。程序分析主要研究两大领域：程序的优化和程序的正确性。前者研究如何提升程序性能并且降低程序的资源占用，后者研究如何确保程序完成预期的任务。同时程序分析可以在不执行程序的情况下进行（静态程序分析），也可以在执行时进行（动态程序分析），或结合二者。这里都是来自 wiki 的介绍，而本文的相关术语也遵从上面的介绍，静态分析就是程序分析的一个方面，全称为静态程序分析。 在我的学习中，感觉静态分析的相关知识和编译器优化的内容存在很大程度上的重叠，所以就将 DCC888 的内容也补充进来。后来发现该课程主页的名称为 Static Program Analysis，拿过来记录也名正言顺。所以本系列文章就是基于 《软件分析》，《软件分析技术》，《DCC888》 三门课程与其余参考资料所写的笔记。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:0:0","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"静态分析概览\r注意事项\r下面的内容大部分来自于 沉浸式《程序分析》教材，但是在那篇文章中，对于程序分析的描述为本文一开始提到的静态程序分析。所以在这篇文章中，采用静态分析替代那篇文章中的程序分析。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:1:0","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"静态分析定义\r静态分析\r静态分析（Static Analysis） 是一种在实际运行程序 P 之前，通过分析静态程序 P 本身来推测程序的行为，并判断程序是否满足某些特定的 属性（Property） Q 的方法。 上述中静态程序指的就是不运行程序的状态，它也被称为 “静态” 或 “编译时”，它与程序的 “动态” 和 “运行时” 相对应。由此可以看出，静态分析就是对于给定的程序代码进行自动化扫描、分析，而不必运行程序。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:1:1","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"静态分析用途\r静态分析的用处有很多，主要可以分为下面的内容： 程序可靠性（Program Reliability）：空指针异常而导致的程序崩溃会影响程序的可靠性，诸如这样的 bug 还存在很多，但是他们中很多都可以在没有运行的状态下被静态分析检测出来。同时导致程序不响应的程序缺陷，例如内存泄漏，也会被静态分析检测出来。 程序安全性（Program Security）：对于程序中的可能引起注入攻击等的缺陷代码，静态分析也可以进行识别。 编译优化（Compiler Optimization）：在将源码编译为目标平台程序的过程中，静态分析可以在中间环节对中间代码进行优化。其中 Dead code elimination 可以避免永远执行不到的代码最终编译到目标平台程序中；Code motion 可以将循环中的某些计算不变式语句提取到循环外部，进行避免冗余计算，提高程序运行速度。 程序理解（Program Understanding）：IDE 提供的不止有代码编辑功能，还有程序理解功能。比如 IDE 可以提示代码的调用关系、继承关系、声明类型等信息，这些关于程序的诸多信息的提取很多都是通过静态分析技术来完成的。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:1:2","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"静态分析定位\r静态分析属于程序设计语言的一部分，而程序设计语言主要分为三类研究内容： 理论：设计一款程序设计语言一般是从其语法、语义的设计开始，也包括选择什么类型系统，支持什么语言特性等问题。一般情况下，这类理论研究一般可以自证，即可以将语言的语法、语义、类型系统等形式化，然后在其形式化基础上用理论方法证明该语言的诸多属性，这也就是为什么很多 PL 的论文并没有实现实验部分。 环境：程序设计语言有了理论设计，在实际中想要运行的起来，必须要有支撑它的环境系统，这主要包括编译系统和运行时系统两个部分。编译系统强调语法的解析（如果是静态语言还会有类型检查等），运行时系统强调语义的解释执行（比解释器更复杂的运行时系统也会负责垃圾回收等内存问题）。PL 的环境系统往往避免不了在实现细节上做很多脏活累活，使得语言在实际中真能好用起来。 应用：有了理论与环境的支撑，语言就能跑起来了。然而，一个工业级的程序设计语言通常是一个非常复杂的系统，如何保障该复杂系统的可靠性、安全性、高性能等需求，是需要一系列方法来支撑的，这些方法（如静态分析、程序验证、程序合成等）通常要以语言的理论部分为基础（如语法、语义），结合不同的数学理论来完成各自应用的目标。在 PL 应用中，最具代表性的技术就是静态分析。 虽然程序设计语言数量繁多，但是无非属于以下三大类（称为 程序设计语言范式，Programming Paradigm）： 命令式程序设计语言（Imperative Programming Languages，IP）：在 IP 中，指令一个一个给出，用条件、循环等来控制逻辑（指令执行的顺序），同时这些逻辑通过程序变量不断修改程序状态，最终计算出结果。尽管 IP 现在都是高级语言了，但是本质上并没有脱离那种 “类似汇编的，通过读取、写入等指令操作内存数据” 的编程方式。国内高等教育中接触的绝大多数编程语言都是 IP 的，比如 Java、C、C++ 等。 函数式程序设计语言（Functional Programming Language，FP）：在 FP 中，逻辑（用函数来表达）可以像数据一样抽象起来，复杂的逻辑（高阶函数）可以通过操纵（传递、调用、返回）简单的逻辑（低阶函数）和数据来表达，没有了时序与状态，隐藏了计算的很多细节。不同的逻辑因为没有被时序和状态耦合在一起，程序本身模块化更强，也更利于不同逻辑被并行的处理，同时避免因并行或并发处理可能带来的程序故障隐患，这也说明了为什么 FP 语言如 Haskell 在金融等领域（高并发且需要避免程序并发错误）受到瞩目。 逻辑式程序设计语言（Logic Programming Language，LP）：LP 抽象的能力就更强了，计算细节干脆不见了，把想表达的逻辑直观表达出来就好了。如今，在数据驱动计算日益增加的背景下，LP 中的声明式语言，如 Datalog 作为代表开始崭露头角，在诸多专家领域开拓应用市场。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:1:3","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"不完备性\r由上文静态分析的定义可知，静态分析是通过分析程序的代码而推理出程序在动态运行时可能的行为，然后判断程序是否满足关注的一些属性的一种方法。这里关注的信息可能是： 该程序是否会泄漏私有信息？ 该程序是否会引用空指针？ 该程序中所有 cast 操作都是安全的吗？ 该程序的这块代码是否是死代码？ …… 可以看出，静态分析可以判断的属性有很多，而且有些至关重要。所以如果静态分析可以准确无误地判断上述程序的所有属性，那么程序就不愁还会存在可靠性和安全性的问题了，只需要关注于静态分析的优化即可。但是显示情况却不是这样的，这就需要下面一系列理论的支持了。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:2:0","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"不可判定\r可判定问题是指：对于回答是或否的问题，如果存在一个算法，使得对于该问题的每一个实例都能给出 是/否 的答案，那么这个问题就是可判定问题。而将这个问题转移到程序上面，就是说对于一个程序或者代码而言，只看其初始状态，而不运行这个程序，那么是否可以判断其是否会停机？答案就是它是不可判定的。 而要知道它为什么是不可判定问题，就需要知道哥德尔不完备定理和图灵停机问题的相关知识，因此可以从 停机悖论三句话就能证明不完备性定理？ 来进行了解。在观看之后就可以明白一个概念，不存在一个算法能够回答停机问题，它是不可判定的。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:2:1","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"莱斯定理\r现在关注 静态分析是否可以准确无误地判断上述程序的所有属性问题，那么就需要引入莱斯定理。 Rice 定理\r对于使用 递归可枚举（Recursively Enumerable） 的语言描述的程序，其任何 非平凡（Non-trivial） 的属性都是不可判定的。 这里的 递归可枚举 就可以理解为图灵完备语言；而对于 非平凡属性，定义一个属性是平凡的，那么这个属性要么对任何一个递归可枚举语言编写的所有程序为真，要么为假，否则它就是非平凡的。由此可以把这里的非平凡属性理解为和程序运行时行为相关的属性，它体现的是一种语义相关而不是语法相关的属性。例如，一个程序是否存在 while 循环、是否存在左右括号等类似的就是和 语法相关 的属性，而这里讨论的是否会泄漏私有信息等就是和 语义相关 的属性。因此可以进一步将莱斯定理理解为下面的表述： Rice 定理\r一个程序的任何语义（运行时行为）相关的属性都是不可判定的。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:2:2","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"莱斯定理验证\r既然成功理解了莱斯定理，知道其含义，那么就需要知道它为什么是正确的，这样就涉及到上面提起的不可判定了。这里的证明思路需要和图灵机停机进行联系，也就是将证明这个问题（非平凡属性）是否可判定 规约 到是否可以判定图灵机停机问题上。如此一来，如果该问题可判定，那么就可以得到图灵停机问题也可以判定的结果，继而根据已知事情反证出来该问题不可判定。下面就是使用 python 来定义一个 Halt 函数： def Halt(p, i): def trick(k): p(i) return k * k * k return is_cube(trick) 这里 Halt 函数用来判定在给定程序 p 和输入 i 的条件下，p 是否会停机。之后再内部定义一个函数 trick，它的输入是 k，首先执行输入为 i 的程序 p，之后返回输入 k 的立方。如果 p(i) 不停机，也就是一直处于运行状态，那么这个 trick 函数就不会进行返回操作；而如果 p(i) 能够停机，那么这个函数就会返回给定参数的立方值。 现在假设 Halt 函数中的 is_cube(trick) 是可以确切无误地判断 给定函数 trick 是否可以计算立方值问题 的函数（这里可以把计算立方值替换为任何非平凡的属性），那么就可以得到下面的两种情况： p(i) 会停机，之后 trick 就会返回立方值，又因为 is_cube(trick) 可以确切判断出 trick 可以计算立方值，因此 is_cube(trick) = Halt(p, i) = true，也就是判定了程序可以停止。 p(i) 不会停机，之后 trick 不会返回立方值，而此时 is_cube(trick) 可以确切判断出 trick 不可以计算立方值，因此 is_cube(trick) = Halt(p, i) = false，也就是判定了程序不可以停机。 从上述可以知道，如果真存在一个可判定非平凡属性是否为真的算法（即这里的 is_cube），那么就可以得到一个可判断图灵提及的算法（即这里的 Halt），即可以通过调用 is_cube(trick) 的结果准确的判断程序 p 是否会停机。但是又因为图灵停机问题是不可判定的，因此就不存在一个可判定非平凡属性的算法，那么莱斯定理就成立了。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:2:3","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"静态分析类型\r","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:3:0","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"完美的静态分析\r完美的静态分析\r如果一个静态分析 S 能够对于程序的某个非平凡性质 Q 给出确切的答案，就称 S 是 P 关于 Q 的 完美静态分析（Perfect Static Analysis）。定义程序 P 关于 Q 的真实性为 真相（Truth），那么完美静态分析有两层含义： 完全性（soundness）：真相一定包含在 S 给出的答案中； 正确性（completeness）：S 给出的答案一定包含在真相中； 那么记这个静态分析程序给出的答案集合 A，真相集合为 T，则完美的静态分析满足： $$ T \\subseteq A \\land A \\subseteq T \\iff A = T $$ 其中，$T \\subseteq A$ 体现了 soundness，$A \\subseteq T$ 体现了 completeness。 根据上述的定义可知，一个完美的静态分析得到的结果是正确的，也是全面的，但是由上文的莱斯定理可以知道并不存在一个完美的方法可以准确判断任意程序的非平凡属性是否为真，也就是说不存在一个既能 Sound 又能 Complete 的静态分析方法。那么这里的静态分析是否就没用了呢？其实并不是，sound 和 complete 之间就是进行探索的领域。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:3:1","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"妥协的静态分析\rsoundness \u0026 completeness\r因为完美的静态分析并不存在，所以通常需要做一些妥协，也就是在 $T⊆A$ 和 $A⊆T$ 中只尽力满足其中一个条件，妥协另一个条件。于是，就得到了两种妥协后的静态分析类型。 过近似\r记程序 P 关于性质 Q 的静态分析 S 为 Sound 的静态分析，当且仅当 S 给出的答案集合 A 和 P 关于 Q 的真相集合 T 之间满足： $$ T \\subseteq A $$ 这种分析策略也成为 过近似（Over-approximation）。 这里 Sound 的静态分析保证了 soundness，妥协了 completeness，会 过近似（Overapproximate） 程序的行为，因此会出现 假阳性（False Positive） 的现象，即判定为阳性，但实际是阴性的。反映在现实场景中即为 误报问题。比如真实的行为（Truth）给定程序存在 5 处空指针引用，那么一个 Sound 的结果就是它至少包含这 5 处空指针引用，即不存在漏报。但是它可能报出 9 处空指针引用，多余的这 4 处空指针引用就是误报，也就是假阳性的情况。但是这样的妥协理论上不但可以帮助找出所有的程序缺陷漏洞，还可以使得静态分析可以用于编译优化和程序验证等应用。这里对于编译优化而言，静态分析就是在判断该优化是否是正确的，所以需要 Sound 的分析来对所有情况进行考虑。 False Positive\r欠近似\r记程序 P 关于性质 Q 的静态分析 S 为 Complete 的静态分析，当且仅当 S 给出的答案集合 A 和 P 关于 Q 的真相集合 T 之前满足： $$ A \\subseteq T $$ 这种分析策略也成为 欠近似（Under-approximation）。 这里 Complete 的静态分析保证了 completeness，妥协了 soundness，会 欠近似（Underapproximate） 程序的行为，因此会出现 假阴性（False Negative） 现象，即判定为阴性，但实际是阳性。反映在现实场景中即为 漏报问题。比如真实的行为（Truth）给定程序存在 5 处空指针引用，那么一个 Complete 的结果就是它最多包含这 5 处空指针引用，即不存在误报。但是它可能报出 3 处空指针引用，缺少的 2 处空指针引用就是漏报，也就是假阴性的情况。而这样的妥协不影响分析应用的有效性，例如利用静态分析查找程序缺陷和安全漏洞，虽然因为漏报没有找到所有的 bug，但是找到一个 bug 都是有益的。 False Negative\r","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:3:2","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"现实的静态分析\r而现在，无论是怎样的分析应用，静态分析的设计都是在努力追求更好的 soundness。因为这样的分析会覆盖到更多的程序行为，对于像编译优化和程序验证这种应用，静态分析必须是 Sound 的，否则它会影响优化和验证的正确性。对于 Sound 可以通过下面的例子进一步理解： if input then x = 1; else x = 0; output x; 对于上面的代码，存在两种 Sound 的静态分析： 当 input 为 true，x 是 1，当 input 为 false， x 是 0 ； x 是 0 或 1 这两种都保证了 soundness，不过前者更精确，代价也更高，分析速度慢；后者不那么精确，但相应代价也会更低一点，分析速度快。前者精确，但是需要维护条件分支的信息，需要额外的开销，如果针对一定规模的程序，这个开销将占据非常多的资源，很大程度上会拖累分析的速度；相比之下，后者不那么精准，但是因为不用维护额外的信息，分析速度会很快。由此可以对现实世界中的静态分析进行总结： 现实中的静态分析\r现实中的静态分析需要在确保（尽可能接近）soundness 的前提下，在速度和精度之间做一个合适的权衡。 上述中尽可能接近 soundness，是因为在不少实际应用场景中，取得完全的 soundness 是困难的。例如分析 java 程序，那么 java 语言的反射特性、动态类加载特性以及 native 代码调用，都会在很大程度上影响分析的 soundness。在这种情况下，就是需要分析尽可能地接近 soundness 了。当下鉴于 soundness 对普遍应用的重要性，以及对部分应用的必要性，为 方便理解，如无特殊说明，静态分析都应做 Overapproximate 以获取 soundness。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:3:3","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"编译基础\r注意事项\r在最初的前提说明中，我提到自己认为静态分析与编译器优化（尤其是编译的中后端内容）之间存在较大的重叠。因此，我觉得有必要了解一些编译相关的知识，并记录了相关内容。以下内容主要参考了 （一）初始软件分析，对编译基础进行一个概括性的介绍。详细内容可以参见 compile 中的相关内容。 编译器（Compiler） 是一种计算机程序，它会将用某种编程语言写成的源代码（原始语言），转换成另一种编程语言（目标语言）。一般来说编译器的内部包含了如下的工作步骤： 词法分析； 语法分析； 语义分析； 生成中间表示（intermediate representation，IR）； 优化； 代码生成； 编译器工作步骤\r在这里，编译器的前端对接各种编程语言，对编程语言本身进行语法和语义分析，中端则负责代码优化，后端对接各种硬件架构，负责生成对应硬件下的可执行文件。而对于静态分析而言，关注的就是中间代码上的优化部分。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:4:0","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"词法分析\r词法分析（lexical analysis） 的主要工作是将源代码的字符流转换成 Token（词法单元） 流，在这里，词法分析器会根据给定的规则把输入的源代码构建成 Token。例如对于 int a = 0;，就会产生 $int, \\ a, \\ =, \\ 0, \\ ;$ 这几个 Token。具体而言，Token 分为种别码和属性值，种别码是一个标识符，用于区分不同类型的 token，编译器就是通过它来识别和分类 token 的关键信息；而属性值就是 token 的具体信息了，它会在后续的语法分析和语义分析阶段被用于构建语法树和进一步的处理。下面就是种别码的不同类型，可以看出它是程序的关键字部分。 种别码类型\r下面就是一个简单的词法分析过程，输出的内容就是输入的字符流进行转化的结果。第一列就是字符，后面使用 \u003c \u003e 包围的就是转换后的 Token 了。 词法分析\r词法分析器可以读入字符流，然后转化为 Token，但是词法分析器的产生过程是不容易的。下图就展示了词法分析器代码产生的过程。 词法分析流程\r词法分析器 的产生始于对语言模式的定义，即需要知道对于某个字符，应该进行怎样的处理。而正则表达式（RE）正是描述语言词法规则的一种简洁而有效的工具。例如，正则表达式 [a-zA-Z_][a-zA-Z0-9_]* 可以定义标识符的格式，而 [0-9]+ 则可以描述整数常量。然而，正则表达式只是对模式的抽象描述，并不能直接用于字符流的解析。为了将正则表达式转化为实际可执行的结构，首先需要构造非确定性有限自动机（NFA）。NFA 是正则表达式的一种等价形式，它通过状态和状态间的转移来表示语言的匹配过程。NFA 的一个显著特点是它允许非确定性迁移，即在某个状态下，读取一个字符可以有多个可能的转移，这为复杂的模式解析提供了灵活性。NFA 的构造通常采用 Thompson 构造法，将简单的正则表达式片段逐步拼接为完整的自动机结构。 如下图就是 RE 向 NFA 的转化过程，上面的公式就是正则表达式的匹配公式。可以看出，随着对于正则表达式的不断解析，NFA 的构建也逐渐完善。这里的圆圈就是状态，随着中间箭头上字符的输入，状态进行不同的转移，从而匹配相应的正则表达式规则，即语法规则。 NFA 结构\r然而，NFA 的非确定性也导致了解析效率的下降，因为在解析时可能需要同时跟踪多个状态，例如下图中 NFA 的 1 状态到 2、4 状态。为了解决这个问题，需要将 NFA 转换为确定性有限自动机（DFA）。DFA 通过子集构造法将 NFA 的多个状态集合映射为一个确定的状态，从而消除了非确定性。DFA 的每个状态都可以看作是 NFA 的一个状态集合，因此保留了与 NFA 等价的语言匹配能力，但解析效率得到了显著提高，因为在 DFA 中，每次读取一个字符只会有唯一的状态迁移。 DFA 结构\r在得到 DFA 后，接下来的任务是将其转化为实际的词法分析器代码。DFA 本质上是一个状态迁移表，可以直接映射到代码实现中。词法分析器通过模拟 DFA 的运行过程，从字符流中逐个读取字符，根据 DFA 的状态迁移规则移动到下一个状态。当到达终止状态时，词法分析器便生成对应的 Token，并继续解析后续的字符。如果某个字符无法匹配任何状态迁移，则报告词法错误。通过这种方式，词法分析器实现了从正则表达式到自动机、再到代码的完整转换流程，从而能够高效地将源代码的字符流解析为词法单元。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:4:1","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"语法分析\r语法分析（parsing） 就是从词法分析器生成的 Token 流出发，根据特定语言的 形式文法 规则对 Token 进行结构化分析，从而确定输入文本的语法结构。形式文法（如上下文无关文法）定义了语言的语法规则，可以描述合法句子的结构和嵌套关系。语法分析的目标是验证这些规则是否被满足，同时将 Token 组织成有意义的结构。可以将这一过程类比为从单词（Token）构造语句：词法分析生成了“单词”，而语法分析负责根据语言规则将这些单词组合成语法正确的“句子”。 为了完成这一目标，语法分析器需要根据形式文法生成语法树，这种树形结构反映了 Token 在语法规则中的嵌套关系。例如，下图展示了从 Token 到语法树的构造过程，语法树的层次关系表示了不同语法规则的应用： 语法分析过程\r语法分析器（parser） 的实现通常涉及两种主要的解析方法：自顶向下解析和自底向上解析。自顶向下解析以语言的起始符号为根，根据文法规则尝试生成整个输入的结构化表示，例如递归下降法；而自底向上解析则从输入的 Token 开始，逐步将其归约为更高层次的语法结构，例如使用 LR 分析方法。 在构造语法树时，语法分析器将通过匹配 Token 和文法规则来完成 Token 的层次化组织。例如，对于以下伪代码的辗转相除法： while b != 0: if a \u003e b: a := a - b else: b := b - a return a 语法分析器将根据文法规则对 Token 流进行处理，最终生成以下的 抽象语法树（AST）： 抽象语法树\r抽象语法树是一种高层次的表达方式，删除了文法中不必要的细节，仅保留了表达程序结构和逻辑的核心信息。这种结构不仅便于程序的后续分析（如语义检查或优化），还能够作为解释器或编译器的输入。 通过这样的流程，语法分析实现了从 Token 到语法结构的转化，验证了输入代码的语法正确性，同时生成了表达程序逻辑的层次化数据结构。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:4:2","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"语义分析\r语义分析（semantic analysis） 建立在语法分析生成的抽象语法树（AST）之上，旨在验证程序的逻辑正确性，确保程序的各部分含义符合语言的语义规则。语法分析关注的是形式结构是否符合文法，而语义分析则进一步检查这些结构是否具有合法的意义。例如，在变量赋值中，语法分析确保了赋值表达式的结构正确，而语义分析则要验证变量是否已声明、数据类型是否匹配等。 语义分析的核心任务可以分为以下几个方面： 类型检查：验证表达式、操作符及赋值操作是否符合语言的类型规则。例如，在赋值语句 a = b + c; 中，语义分析需确保 b 和 c 的类型是数值类型，且 b + c 的结果可以赋值给 a 的类型。类似地，函数调用时参数类型需与函数声明的参数类型一致。 作用域检查：确认符号的引用是否在其合法作用域内。通过符号表，语义分析会在变量或函数被引用时查找它们的声明位置，若在当前作用域及其父作用域中均未找到匹配声明，则会报错。例如，若在函数内部使用一个未定义的变量，则语义分析会捕捉到这一错误。 符号表管理：符号表记录程序中每个符号的相关信息，包括变量名、数据类型、作用域、初始值、存储位置等。语义分析会动态更新符号表，例如在变量声明时插入记录，在变量使用时查找匹配条目。 属性计算：为语法树的每个节点附加语义信息，例如变量的类型、数组的维度、函数的返回值等。这些信息既用于当前的语义分析，也为后续阶段（如中间代码生成和优化）提供支持。例如，在表达式 a + b 中，属性计算需要确认 a 和 b 的类型，并推导出整个表达式的类型。 int a = 5; int b = 0; while (b != 0) { if (a \u003e b) { a = a - b; } else { b = b - a; } } return a; 对于上述代码，语义分析的任务包括： 符号表检查：变量 a 和 b 在声明后被插入符号表。随后，语义分析会在使用 a 和 b 时在符号表中查找它们的定义并验证是否符合上下文需求。 类型检查：表达式 b != 0 会被验证是否返回布尔值；减法操作 a - b 会检查操作数是否都是整型，并确保结果能赋值给变量 a 或 b。 作用域管理：验证 a 和 b 的作用域合法性，确保它们在 while 和 if-else 的嵌套语句中可见。 返回值验证：return a; 的返回值类型必须与函数声明的返回类型一致。 语义分析通过对抽象语法树（AST）的遍历标注和符号表的交互，将类型检查、作用域验证等任务贯穿于整个代码分析过程。当语义分析的所有检查均通过时，程序被认为在逻辑上是正确的。与语法分析的形式验证相比，语义分析更关注程序的逻辑合理性。例如，语法分析可能允许 int a = \"string;\" 形式正确。但语义分析会根据类型规则报告错误。 下图就是对于抽象语法树进行标注而生成的 注释语法分析树，可以看出其与抽象语法树相比，标明了数据类型，初始值等信息。根据这些标明的信息，语义分析就可以对语法分析的结果进行进一步的审查，以确保程序的各部分含义符合语言的语义规则。 注释语法分析树\r语义分析的结果是为后续阶段（如中间代码生成或优化）提供更为具体和完整的语义信息，同时确保输入代码的逻辑正确性。借助语义分析，编译器能够更准确地捕获语义错误，例如类型不匹配、未定义变量使用或非法操作，从而提高程序的健壮性和可靠性。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:4:3","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"中间表示\r注意事项\r这部分内容主要来自课程笔记 程序的中间表示，也就是 《软件分析》 课程讲述的内容。 由上文可以知道编程语言转换为目标机器代码的过程中，会经历词法分析、语法分析、语义分析的前端阶段，然后得到了抽象语法树。之后 翻译器（Translator） 会将抽象语法树翻译成 中间表示（Intermediate Representation, IR），也就是中间代码，IR 的出现解耦了编译器的机器相关部分和机器无关部分，在这一步之前都是和机器无关的，也就是说这几个阶段可以在不同架构的机器上几乎不加改动地复用，而后面的目标代码生成，对于不同架构的机器需要进行不同的处理。 源代码转化为机器码\r上图就是将源代码转化为机器码的过程，其中翻译器和代码生成中间的产物 IR 就是静态分析的目标。而下图也简洁的展示了静态分析关注的是编译器的中端，也就是生成中间代码和优化的部分。 编译器结构\rAST 和 IR\rdo i = i + 1; while (a[i] \u003c v); 对于上面的代码，可以转化为相应的抽象语法树和中间表示： AST 和 IR\rAST IR 层次更高，和语法结构更接近 低层次，和机器代码相接近 通常是依赖于具体的语言类的 通常和具体的语言无关，主要和运行语言的机器（物理机或虚拟机）有关 适合快速的类型检查 简单通用 缺少和程序控制流相关的信息 包含程序的控制流信息 通常作为静态分析的基础 三地址码\r三地址码\r将形如 $f(a_1, a_2, \\ldots, a_n)$ 的指令称为 n 地址码（N-Address Code）。其中，每一个 $a_i$ 是一个地址，既可以通过 $a_i$ 传入数据，也可以通过 $a_i$ 传出数据，$f$ 是从地址到语句的一个映射，其返回值是某个语句 s，s 中最多包含输入的 n 个地址。这里，定义某编程语言 L 的语句 s 是 L 的操作符、关键字和地址的组合。 如上图采用的就是三地址码的形式，之后章节使用就是三地址码的 IR，这是因为一方面经典的分析算法是以三地址码作为 IR 的所以这种表示方法就一直沿用着；另一方面是因为它表示上更加简洁方便且表达能力完备。 对于三地址码而言，其地址可能存在下面几种类型： 名字（Name），包括变量（Variable），用于只是程序位置、方便跳转指令书写的标签（Label） 字面常量（Literal Constant） 编译器生成的临时量（Compiler-Generated Temporaty） x = y bop z x = uop y x = y goto L if x goto L if x rop y goto L 对于上述三地址码形式进行分析： x, y, z 是变量的地址。 bop 是双目操作符（Binary Operator），可以是算数运算符，也可以是逻辑运算符。 uop 是单目操作符（Unary Operator），可能是取负、按位取反或者类型转换。 L 是标签（Label），是标记程序位置的助记符，本质上还是地址。 rop 是关系运算符（Relational Operator），运算结果一般为布尔值。 goto 是无条件跳转， if ... goto 是条件跳转。 三地址码类似于汇编，但是比汇编的结构更为简单，同时需要注意无论是三地址码还是汇编，其指令类型不取决于具体的语言，而取决于运行这个语言的机器的 指令集体系结构（Instruction Set Architecture，ISA）。 静态单赋值\r静态单赋值（Static Single Assignment，SSA） 是另一种IR的形式，它和三地址码的区别是，在每次赋值的时候都会创建一个新的变量，也就是说，在 SSA 中，每个变量（包括原始变量和新创建的变量）都只有唯一的一次定义。下图就是三地址码转化为的 SSA 形式。 中间表示比对\r在没有控制语句的情况下，三地址码可以很简单地转化为 SSA 形式。但是在存在控制语句的情况下，对于 SSA 就需要额外处理。下图就展示了存在控制语句 if 的情况，在这个情形下，y = x + 7 的取值就收到上方不同分支中的 $x_0$ 和 $x_1$ 的影响。 merge 操作\r而对于这种控制流汇合（Merge）的情况，就需要使用特殊的 $\\phi$ 函数来表示汇合的操作： $$ x_3 = \\phi(x_1, x_2) = \\begin{cases} x_1, \u0026 \\text{if true} \\\\ x_2, \u0026 \\text{otherwise} \\end{cases} $$ 上面的公式表示了对于两条路径的选择，但是在现实数据流分析中，要求 Sound 的分析不会放弃任何一条路径的结果。所以这个 $\\phi$ 函数在某些情况下可能会采用其他形式处理，例如将上面的公式表示为 $x_3 = \\{ x_1, x_2 \\}$，以此说明最终结果同时包含了两条路径的结果。 现实中的 IR\rSoot\r在现实中，对于不同的编程语言，采用的 IR 一般不同。例如对于 java(Android) 的静态分析而言，最多使用的就是 Soot 这个 Java 字节码分析工具，它提供了多种字节码分析和变换功能，通过它可以进行过程内和过程间的分析优化，以及程序流图的生成，还能通过图形化的方式输出，让用户对程序有个直观的了解。尤其是做单元测试的时候，可以很方便的通过这个生成控制流图然后进行测试用例的覆盖，显著提高效率。它同时也是一个字节码优化框架，内部对于同一个程序存在四种中间表示： Baf：精简的字节码表示，操作简单 Jimple：适用于优化的 3-address 中间表示 Shimple：Jimple 的 SSA 变体 Grimple：适用于反编译和代码检查的 Jimple 汇总版本。 这里最为关键的就是 Jimple 的中间表示，它是三地址码形式，并且存在类型，可以用来做各种优化需要的分析，比如类型推测（需调用优化）、边界检查消除、常量分析、公共子串分析等。同时该中间表示操作简单，表达简洁，所以常被用来充当 java 静态分析中的中间表示。关于 Soot 的使用可以参照下面的命令，环境为 jdk1.8，Soot 的版本为 sootclasses-trunk-jar-with-dependencies.jar。 # .java -\u003e .class javac hello.java # .class -\u003e .jimple java -cp sootclasses-trunk-jar-with-dependencies.jar soot.Main -f J -pp -cp . hello # .class -\u003e .dot java -cp sootclasses-trunk-jar-with-dependencies.jar soot.tools.CFGViewer -pp -cp . Hello # .dot -\u003e .svg dot -Tsvg -o Hello.svg '.\\sootOutput\\Hello void main(java.lang.String[]).dot' 采用上面的命令，先将下面的代码文件 hello.java 转化为 hello.class 的字节码文件，之后就可以使用 Soot 来产生对应的 Hello.jimple 文件了。 public class Hello { public static void main(String[] args) { int a = 1; int b = a + 1; a = b * 2; int c = 4; if (a \u003e 3) { a += c; } else { a -= c; } System.out.println(\"the value of a is: \" + a); } } 产生的 Jimple 如下所示，这里首先排除 $ 开头的内置变量、label2 的 println 操作和 init 部分的初始化操作，然后主要关注的就是 main 函数的剩余部分。可以清晰的看出 Jimple 采用的就是三地址码形式，同时也没有使用 SSA 的模式，因为存在 i4 = i3 + 4; 和 i4 = i3 - 4; 这两条语句，对于同一个变量进行了多次赋值操作。 public class Hello extends java.lang.Object { public void \u003cinit\u003e() { Hello r0; r0 := @this: Hello; specialinvoke r0.\u003cjava.lang.Object: void \u003cinit\u003e()\u003e(); return; } public static void main(java.lang.String[]) { int i0, i3, i4; java.lang.StringBuilder $r0, $r2, $r3; java.io.PrintStream $r1; java.lang.String $r4; java.lang.String[] r5; r5 := @parameter0: java.lang.String[]; i0 = 1 + 1; i3 = i0 * 2; if i3 \u003c= 3 goto label1; i4 = i3 + 4; goto label2; label1: i4 = i3 - 4; label2: $r1 = \u003cjava.lang.System: java.io.PrintStream out\u003e; $r0 = new java.lan","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:4:4","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"优化和代码生成\r优化阶段 以中间表示（IR）为输入，目标是提高程序的运行效率和资源利用率。优化分为两大类：机器无关优化和机器相关优化。前者专注于 IR 的逻辑结构，与具体的目标机器无关，因此具有跨平台复用的优势。例如，常量传播 通过分析程序的静态属性将计算结果提前确定，死代码消除 移除无意义的代码块，公共子表达式 消除检测并复用重复的计算结果，循环优化 则通过提取循环不变代码或展开循环体减少分支跳转，提高程序执行效率。静态分析在其中扮演了重要角色，通过控制流图和数据流图的构建，为优化提供了重要的基础信息。而这些优化措施，都会在之后的笔记中进行着重讲解。 相比之下，机器相关优化则深入目标架构的特性，结合硬件层面的需求对 IR 进行调整。例如，寄存器分配 利用目标机器的寄存器资源，优先分配热点变量，减少对内存的访问；流水线优化 则通过调整指令顺序避免数据冒险，提高指令并行执行的效率。这些优化措施往往与代码生成阶段密切结合，优化结果直接影响最终的目标代码质量。 代码生成阶段 以优化后的 IR 为基础，将其逐步转换为目标机器的可执行代码。首先进行 指令选择，将抽象的 IR 操作映射为具体的目标机器指令，例如在 x86 架构中，ADD 指令可用于加法运算。接着，通过 寄存器分配，将高频变量优先分配至寄存器中，避免频繁的内存访问；若寄存器不足，则需通过溢出处理将变量存放至内存。最后，指令排布 通过调整指令顺序减少流水线冲突，进一步提升执行效率。 通过优化阶段与代码生成的紧密配合，编译器在语义正确的基础上，生成了高效、可靠的目标代码，从而实现了性能与可移植性的平衡。 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:4:5","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["Static Analysis"],"content":"其余参考资料\r参考\r程序分析与优化 - 1 导论 静态分析基础教程 程序分析研究进展 程序分析与优化 - 2 控制流图 ","date":"2024.12.19","objectID":"/blog/posts/staticanalysis/understanding/:5:0","tags":["Static Analysis"],"title":"01 Understanding Static Analysis","uri":"/blog/posts/staticanalysis/understanding/"},{"categories":["compile"],"content":"这里记录 riscv、isel、ra 的相关知识。 ","date":"2024.12.7","objectID":"/blog/posts/compile/codegen-riscv-isel-ra/:0:0","tags":["compile"],"title":"05 Codegen Riscv Isel Ra","uri":"/blog/posts/compile/codegen-riscv-isel-ra/"},{"categories":["compile"],"content":"riscv\r这里主要就是讲述了 riscv 的汇编语法，通过写汇编语言并执行来加强对于 riscv 的熟悉。因此这里没什么笔记，需要的时候再去查找 riscv 的相关汇编语法。 贴一下 相关手册。 ","date":"2024.12.7","objectID":"/blog/posts/compile/codegen-riscv-isel-ra/:1:0","tags":["compile"],"title":"05 Codegen Riscv Isel Ra","uri":"/blog/posts/compile/codegen-riscv-isel-ra/"},{"categories":["compile"],"content":"代码转换\r下面就是从中间语言到目标平台代码的翻译过程，其中 MC 指 Machine Code。 转换流程\r下图上面部分就是在转换为目标代码过程中的各个中间产物，而下面就是具体流程。白色块就是 pass，主要是进行优化，因此我们的关注点就是黑色的转换块。 这里首先把 LLVM IR 转化为 DAG（有向无环图） 形式，然后在这个形势下做指令选择。之后 Instruction scheduling 就是对于前面的无环图进行排序，转换为线性结构，这样才能给后面进行处理。然后进行 Register allocation，把之前使用 SSA 的 IR 转换为有限寄存器的结构。 LLVM 转换\r","date":"2024.12.7","objectID":"/blog/posts/compile/codegen-riscv-isel-ra/:2:0","tags":["compile"],"title":"05 Codegen Riscv Isel Ra","uri":"/blog/posts/compile/codegen-riscv-isel-ra/"},{"categories":["compile"],"content":"指令选择\r","date":"2024.12.7","objectID":"/blog/posts/compile/codegen-riscv-isel-ra/:3:0","tags":["compile"],"title":"05 Codegen Riscv Isel Ra","uri":"/blog/posts/compile/codegen-riscv-isel-ra/"},{"categories":["compile"],"content":"介绍\r指令选择介绍\r指令选择的介绍如上图所示，下面就是可以采用的不同的算法。 指令选择算法\r下面就是 LLVM IR 生成的相关 DAG 图，之后我们需要做的就是对于这个 DAG 图进行优化，删除改进合并相关细节。 DAG 生成\r优化之后的 DAG 如下左图所示。之后我们就需要从 DAG 中找到和指令相关的元素，然后采用树结构来进行匹配，从而获取下图最终的结果。 选择指令\r结构转换\r","date":"2024.12.7","objectID":"/blog/posts/compile/codegen-riscv-isel-ra/:3:1","tags":["compile"],"title":"05 Codegen Riscv Isel Ra","uri":"/blog/posts/compile/codegen-riscv-isel-ra/"},{"categories":["compile"],"content":"树匹配\r定义\r上面就是我们需要进行匹配的定义，下面则是我们相关的匹配规则。1 匹配规则 1\r匹配规则 2\r由上述匹配规则，我们自底向上进行匹配，若是遇到更大的匹配规则，那么采用更大的覆盖小的，这样就可以得到右侧的相关指令。 匹配结果\r对于上述匹配是理解匹配，我们需要转换为相关算法，所以采用之前语法分析的流程。将树进行前缀表达，语法的推导也转换形式，这样就可以形成推导算法了。 匹配算法\r但是这也会存在二义性问题，所以面对归约/归约冲突，优先选择较长的归约；面对移入/归约冲突，优先选择移入动作。 ","date":"2024.12.7","objectID":"/blog/posts/compile/codegen-riscv-isel-ra/:3:2","tags":["compile"],"title":"05 Codegen Riscv Isel Ra","uri":"/blog/posts/compile/codegen-riscv-isel-ra/"},{"categories":["compile"],"content":"寄存器分配\r定义\r线性扫描分配算法就是根据活跃变量算法查看不同变量的活跃期间，然后进行寄存器的分配和回收；而对于溢出变量，就是使用 store/load 来存取内存。然后后面就没有声音了，但是有个 笔记 可以看看，包含编译原理的很多内容。 ","date":"2024.12.7","objectID":"/blog/posts/compile/codegen-riscv-isel-ra/:4:0","tags":["compile"],"title":"05 Codegen Riscv Isel Ra","uri":"/blog/posts/compile/codegen-riscv-isel-ra/"},{"categories":["config"],"content":"主要记录 Vim 的配置和使用。 ","date":"2024.12.4","objectID":"/blog/posts/config/vim/:0:0","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"基本模式\r","date":"2024.12.4","objectID":"/blog/posts/config/vim/:1:0","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"Normal 模式\r默认进入的模式，可以进行查看，粘贴，搜索等相关操作。 基本移动\r快捷键 含义 hjkl 左下上右 gg 跳到第一行（类似于 Home 键） G 跳到最后一行（类似于 End 键） \u003cCtrl-u\u003e 往上翻半页（类似 PageUp 键） \u003cCtrl-d\u003e 往下翻半页（类似 PageDown 键） {lineno}gg 跳到第 lineno 行 zz / zt / zb 光标行设置为 屏幕居中/屏幕第一行/屏幕最后一行 这里的 u，d 可以理解为 up，down。同时在 Vim 配置中放弃了 和 这两个翻一页的操作，反而改为 vscode 原本的操作。 基于单词移动\r快捷键 描述 w 代表 word，跳转到下一处单词的开头 b 代表 back，跳转到上一处单词的开头 e 代表 end，跳转到下一处单词的结尾 ge e 的反向版本，跳转到上一处单词的结尾 wbe 大写版本 WBE 对应的单词是连续的非空字符 大小写比对\r基于搜索的移动\r行内搜索： 快捷键 描述 f{char} / t{char} 跳转到本行下一个 char 字符出现处/出现前 ; / , 快速向后/向前重复 ft 查找 F{char} / T{char} 往前搜索而非往后 文件中搜索： 快捷键 描述 /{pattern} 跳转到本文件中下一个 pattern 出现的地方 ?{pattern} 跳转到本文件中上一个 pattern 出现的地方 pattern 可以是正则表达式 * 等价于 /{pattern}，pattern 是当前光标下的单词 n / N 快速重复 / 查找 这里查找之后按 enter 就会跳到查找的 pattern 出现的地方 基于标记的移动\r快捷键 描述 `` 上次跳转前的位置 `. 上次修改的位置 其它移动\r快捷键 描述 ^ / $ 跳转到本行的开始/结尾 % 跳到匹配的配对符（括号等）处 % 的作用比如就是在左花括号使用可以跳到右花括号处。同时三者的顺序为 shift + 456，可以理解为结束、匹配、开始。 ","date":"2024.12.4","objectID":"/blog/posts/config/vim/:1:1","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"Insert 模式\r插入模式，主要用于编辑文本内容。Normal 模式下通过特定命令进入 Insert 模式。ESC 返回 Normal 模式。 快捷键 描述 i 代表 insert，当前光标之前开始输入 a 代表 append，当前光标之后开始输入 o 下方插入新的一行，然后开始输入 s 删除当前光标的字符，然后开始输入 I 在本行的开头开始输入 A 在本行的末尾开始输入 O 上方插入新的一行，然后开始输入 S 删除当前行，然后开始输入 ⼤写字⺟和⼩写字⺟的操作存在关联，可以⼀起记忆 ","date":"2024.12.4","objectID":"/blog/posts/config/vim/:1:2","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"Command 模式\r命令模式，在底部输入命令，对文本进行保存，搜索等。Normal 模式下输入 : 进入 Command 模式。ESC 返回 Normal 模式。 快捷键 描述 :w 保存当前文件 :q 退出 :q! 放弃当前更改，然后退出 :wq 保存当前更改，然后退出 /{pattern} 搜索文本，按 enter 确认，之后使用 n 搜索下一个 这里的搜索会出现高亮，取消高亮就是在命令模式中输入 :noh 即可。这里需要注意，使用 : 进入命令模式高亮就会消失，但是不做任何操作，直接退出的话，高亮还是会在之后出现。 ","date":"2024.12.4","objectID":"/blog/posts/config/vim/:1:3","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"Visual 模式\r可视模式，对文本进行选择。ESC 回到 Normal 模式 Normal 模式下按 v 进入可视模式，之后可以用 Normal 模式下的移动命令选择文本 按 V 进入行可视模式，一次选中一整行，在需要选中多行时很方便。 可视模式下 x / y：剪切/复制；回到 Normal 模式下 p：粘贴 ","date":"2024.12.4","objectID":"/blog/posts/config/vim/:1:4","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"编辑命令\r","date":"2024.12.4","objectID":"/blog/posts/config/vim/:2:0","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"编辑动作\r{operator}{motion}：就是一次编辑动作，这里的 motion 就是对于光标的移动，也就是上面我们在 Normal 和 Visual 模式展示的这些，因此我们可以理解这里的编辑就是对于这次光标到移动后光标之间的内容进行编辑操作。常见的操作符如下： c：change，修改、删除内容并进入插入模式 d：delete，删除 y：yank，复制 v：visual，选中文本，进入可视模式 大部分操作符连续按两次（例如 cc/dd/yy）：将其作用在这一行上，比如 dd 就是删除这一行。 d{char}，说是删除，但是这个动作其实是剪切的行为，会把内容复制到剪切板上，之后就可以使用 p 会进行粘贴。 例子： dgg：删除到第一行 ye：复制到单词结尾 d$：删除到行尾 dt;：删除直到分号为止的内容 ","date":"2024.12.4","objectID":"/blog/posts/config/vim/:2:1","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"重复操作\r.：重复上一次修改，记住是修改，不能重复光标移动操作 u：撤销上一次修改 \u003cCtrl-r\u003e：重做上一次修改 . 命令适合需要多次重复某一个修改动作的场景 ","date":"2024.12.4","objectID":"/blog/posts/config/vim/:2:2","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"批量操作\r{count}{action}：重复 count 次 action 动作，动作可以是移动动作或是编辑动作。 4j：向下移动4行 3dw：删除3个单词 2yy：复制2行 4p：粘贴4次 数字 + 动作是一种重要的批量操作方式，命令直观，语义明确。这里 .命令可以直观地看到每一次地变化，在合适地时候停止，而数字 + 动作则需要预先知道动作的次数。 ","date":"2024.12.4","objectID":"/blog/posts/config/vim/:2:3","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"文本对象操作\r文本对象为文本赋予了结构化的含义，允许我们以一个语义对象作为操作单元。命令就是 [count]{operator}{textobjects}。这里的 {textobjects} 就是语义化的文本片段，格式为 i / a + 对象。常见的对象如下： w / W，s，p：单词、句子、段落 ( / )，[ / ]，{ / }，\u003c / \u003e，' / \"：配对符定义的对象 这里 i 代表 inner，内部；a 额外包括周围的空格或配对符。 图示举例\r例子： diw：删除一个单词 ci(：修改小括号内部 yi{：复制大括号内部 这里配合 . 或 [count] 可以简单完成多次对特定语义对象的操作 ","date":"2024.12.4","objectID":"/blog/posts/config/vim/:3:0","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"其余操作\rgd：跳转到函数定义的地方。 gh：查看函数的帮助信息。 ","date":"2024.12.4","objectID":"/blog/posts/config/vim/:4:0","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"相关配置及插件\r","date":"2024.12.4","objectID":"/blog/posts/config/vim/:5:0","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"Vim 配置\r由下面的 vim.handleKeys 可以看出为了vscode 和 vim 协调，调整了一些快捷键。 { // Vim \"vim.easymotion\": true, \"vim.incsearch\": true, \"vim.useSystemClipboard\": true, \"vim.useCtrlKeys\": true, \"vim.hlsearch\": true, \"vim.leader\": \"\u003cspace\u003e\", \"vim.smartRelativeLine\": true, // 相对行号 \"vim.handleKeys\": { // false 则由 vscode 进行处理 \"\u003cC-a\u003e\": false, // 全选 \"\u003cC-c\u003e\": false, \"\u003cC-x\u003e\": false, \"\u003cC-v\u003e\": false, \"\u003cC-p\u003e\": false, // 打开配置 \"\u003cC-w\u003e\": false, // 关闭当前窗口 \"\u003cC-q\u003e\": false, // 选择侧边栏 \"\u003cC-f\u003e\": false, // 查找 \"\u003cC-b\u003e\": false, // 关闭侧边栏 }, \"vim.insertModeKeyBindings\": [ { \"before\": [\"j\", \"j\"], \"after\": [\"\u003cEsc\u003e\"] } ], \"vim.normalModeKeyBindingsNonRecursive\": [ { \"before\": [\"\u003cleader\u003e\", \"d\"], \"after\": [\"d\", \"d\"] }, { \"before\": [\"\u003cC-n\u003e\"], \"commands\": [\":nohl\"] }, { \"before\": [\"K\"], \"commands\": [\"lineBreakInsert\"], \"silent\": true } ], // To improve performance \"extensions.experimental.affinity\": { \"vscodevim.vim\": 1 }, } ","date":"2024.12.4","objectID":"/blog/posts/config/vim/:5:1","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"easymotion\r这里就是使用 \u003cleader\u003e 来帮助进行操作。也就是采用 \u003cleader\u003e \u003cleader\u003e {char} 可以使操作作用于全局，将对应的位置高亮显示，并修改为字母，之后就可以点击相应字母达成光标移动 ","date":"2024.12.4","objectID":"/blog/posts/config/vim/:5:2","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"vim-surround\r对于上述文本对象操作而言，这里除了 i/a 之外可以再添加一个 s，它表示 a 中除了 i 的部分。下面就是使用说明： 具体使用\r","date":"2024.12.4","objectID":"/blog/posts/config/vim/:5:3","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"注意事项\r","date":"2024.12.4","objectID":"/blog/posts/config/vim/:6:0","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"解决中文输入法\r根据 知乎回复 的解决方案，配置了改进的 AutohotKey 方法。同时按照我的情况更改为如下的代码： ;限定在vscode 程序里面发挥作用 #IfWinActive ahk_exe Code.exe ;vscode 的exe 名字叫做Code.exe global MyVar := 0 ~Esc:: if (MyVar == 0) { sendinput, {Esc} sendinput, {Shift Down} SendInput, {Shift Up} MyVar = 1 } else { sendinput, {Shift Down} SendInput, {Shift Up} MyVar = 0 } return #If 但是仍然存在问题，第一次使用的时候和预期的一样，按下 ESC 就可以切换回 NORMAL 的同时切换回英文模式。但是在之后进行第二次切换时，就只会切换英文模式，而不会切换为 NORMAL 了，怀疑之后信息截断之后没有再发送到 vscode 了。所以现在使用的方案就是手动进行中英文的切换，多进行一步比搞混乱浪费的时间短。 ","date":"2024.12.4","objectID":"/blog/posts/config/vim/:6:1","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"参考\r参考的教程如下： Vim 使用教程 Vim 相关使用 Vscode Vim 快捷键配置 Vim 讲解及配置 ","date":"2024.12.4","objectID":"/blog/posts/config/vim/:7:0","tags":["config"],"title":"Vim 使用记录","uri":"/blog/posts/config/vim/"},{"categories":["config"],"content":"记录一下 vscode 及其相关插件的配置和使用。 ","date":"2024.12.3","objectID":"/blog/posts/config/vscode/:0:0","tags":["config"],"title":"VsCode 配置和使用","uri":"/blog/posts/config/vscode/"},{"categories":["config"],"content":"具体使用\r","date":"2024.12.3","objectID":"/blog/posts/config/vscode/:1:0","tags":["config"],"title":"VsCode 配置和使用","uri":"/blog/posts/config/vscode/"},{"categories":["config"],"content":"快捷方式\r这里只描述 vscode 本体的快捷方式或者后续添加的，与 Vim 相关的操作在具体描述 Vim 的文章中进行讲述。 多功能按键\r\u003cCtrl-Shift-p\u003e：这个按键就是调出设置框，如下图所示。通过这个设置框，我们可以在里面寻找 vscode 的任何操作说明，点击即可进行相关操作。 设置处理\r\u003cCtrl-p\u003e：和上面按键类似，但是这个是对于文件的处理，它可以寻找当前项目的文件，点击即可打开，同时聚焦于新文件。 文件处理\r界面聚焦\r这里的聚焦实际指当前光标的位置，因为 vscode 的区域可以分为侧边栏，编辑区，终端等区域，所以这里记录区域切换的方式。 \u003cCtrl-`\u003e：这个按键用于打开/隐藏终端，但是打开/隐藏的时候会伴随着光标的切换。也就是当打开终端时，会自动聚焦于终端上；隐藏终端时，会自动聚焦于编辑框。由此能把这个按键作为编辑框和终端的光标转换按键，需要终端时使其打开出现，不需要时也可以令其隐藏使光标处于编辑框中。 \u003cCtrl-Shift-E\u003e：这个是光标跳转到左侧边栏的 Explorer 的快捷键，使用它就可以使光标在左侧边栏和离侧边栏最近的编辑框进行跳转了，它类似于上面的 \u003cCtrl-`\u003e，不过这里只是光标的跳转，上面还可以打开/隐藏终端。 \u003cCtrl-count\u003e：count 指的就是数字键，这个组合是对于横向的窗口而言的。比如 \u003cCtrl-0\u003e 就会把光标放在左侧边栏中，这样对于文件栏而言可以使用 enter 进行打开；\u003cCtrl-1\u003e 则是当前编辑框；Ctrl-2 就是第二个编辑框，若是当前没有就会进行创建，然后我们就可以使用 \u003cCtrl-p\u003e 来打开文件了。后面的数字依次类推。 \u003cAlt-count\u003e：上面的 Ctrl 是对于不同窗口而言的，那么这里的 Alt 就是对于不同标签页而言的。使用这个按键组合就可以在当前窗口中切换相应标签页了。这里使用 \u003cCtrl+Tab\u003e 也是切换标签页，不过有一个切换界面。 页面操作\r\u003cCtrl-w\u003e：关闭当前标签页，\u003cCtrl-Shift-w\u003e就是关闭当前项目了。 \u003cCtrl-b\u003e：打开/关闭侧边栏。 侧边栏操作\r配置了下面的快捷方式，使得在侧边栏操作更为方便。 // -------------------------- 侧边栏操作 -------------------------- [ { // Rename file \"key\": \"r\", \"command\": \"renameFile\", \"when\": \"explorerViewletVisible \u0026\u0026 filesExplorerFocus \u0026\u0026 !explorerResourceIsRoot \u0026\u0026 !explorerResourceReadonly \u0026\u0026 !inputFocus\" }, { // New file \"key\": \"a\", \"command\": \"explorer.newFile\", \"when\": \"explorerViewletVisible \u0026\u0026 filesExplorerFocus \u0026\u0026 !explorerResourceIsRoot \u0026\u0026 !explorerResourceReadonly \u0026\u0026 !inputFocus\" }, { // New folder \"key\": \"shift+a\", \"command\": \"explorer.newFolder\", \"when\": \"explorerViewletVisible \u0026\u0026 filesExplorerFocus \u0026\u0026 !explorerResourceIsRoot \u0026\u0026 !explorerResourceReadonly \u0026\u0026 !inputFocus\" }, { // Delete file \"key\": \"d\", \"command\": \"deleteFile\", \"when\": \"explorerViewletVisible \u0026\u0026 filesExplorerFocus \u0026\u0026 !explorerResourceIsRoot \u0026\u0026 !explorerResourceReadonly \u0026\u0026 !inputFocus\" }, { // Copy \"key\": \"y\", \"command\": \"filesExplorer.copy\", \"when\": \"explorerViewletVisible \u0026\u0026 filesExplorerFocus \u0026\u0026 !explorerResourceIsRoot \u0026\u0026 !explorerResourceReadonly \u0026\u0026 !inputFocus\" }, { // Cut \"key\": \"x\", \"command\": \"filesExplorer.cut\", \"when\": \"explorerViewletVisible \u0026\u0026 filesExplorerFocus \u0026\u0026 !explorerResourceIsRoot \u0026\u0026 !explorerResourceReadonly \u0026\u0026 !inputFocus\" }, { // Paste \"key\": \"p\", \"command\": \"filesExplorer.paste\", \"when\": \"explorerViewletVisible \u0026\u0026 filesExplorerFocus \u0026\u0026 !explorerResourceIsRoot \u0026\u0026 !explorerResourceReadonly \u0026\u0026 !inputFocus\" } ] ","date":"2024.12.3","objectID":"/blog/posts/config/vscode/:1:1","tags":["config"],"title":"VsCode 配置和使用","uri":"/blog/posts/config/vscode/"},{"categories":["config"],"content":"配置辨析\r为了更改目录相关的配置，我们可以设置 settings.json 文件实现。在此过程中，我们选择使用 Workspace（工作区）配置，而非全局的 User 配置。这样 vscode 会在当前目录的根目录下生成 .vscode 文件夹，其含有 settings.json 文件，专门用于当前目录的配置。因此，每个独立的目录可以被视为一个单独的工作区。 对于工作区配置，其核心是通过一个专门的配置文件 .code-workspace 来管理多个子项目和工作区的 settings.json 文件。同时，每个子项目的目录下仍可以存在单独的 .vscode 文件夹，用于个性化配置。这种方案的优点是粒度更细，可以分别对工作区和子项目进行配置。但缺点也较为明显：如果需要按之前的配置打开相关项目，就必须通过工作区文件打开多个子项目。此外，工作区文件记录的是绝对路径，若项目地址发生变动，原配置文件就失效了。这对我当前随机分配项目的需求而言并不适用。 至于 Profiles 功能，它允许为不同目录设置独立的配置文件。然而，根据实际尝试，这种方式存在一个问题：每个配置文件需要单独下载插件，并无法共享，这对于插件依赖较多的场景来说非常不便，因此我放弃了这种方案。 最终，我选择将每个目录视为一个独立的工作区，并通过禁用不相关插件来优化体验。尽管目前大多数语言相关插件都支持按需加载（未使用时不加载），且加载后会显示加载时间，但我注意到部分插件在未使用时依然被加载。因此，为了针对不同目录提高效率，我仍倾向于手动禁用无关插件，以实现更精准的控制和优化。 ","date":"2024.12.3","objectID":"/blog/posts/config/vscode/:1:2","tags":["config"],"title":"VsCode 配置和使用","uri":"/blog/posts/config/vscode/"},{"categories":["config"],"content":"配置记录\r记录一下自己的 settings.json 配置。 { // ----------------------------- 编辑器配置 ----------------------------- // workbench 配置，用于设置工作台的外观和行为 \"workbench.editor.enablePreview\": false, \"workbench.startupEditor\": \"none\", \"workbench.colorTheme\": \"Learn with Sumit - Blue Velvet\", \"workbench.iconTheme\": \"material-icon-theme\", \"workbench.layoutControl.enabled\": false , // files 配置，用于设置文件相关的行为 \"files.autoSave\": \"onFocusChange\", \"files.autoGuessEncoding\": true, // editor 配置，用于设置编辑器的外观和行为 \"editor.wordWrap\": \"on\", // 软换行，没有横向滚动条 \"editor.formatOnPaste\": true, \"editor.fontSize\": 13, \"editor.fontFamily\": \"JetBrainsMono Nerd Font Propo\", \"editor.tabCompletion\": \"on\", \"editor.formatOnType\": true, \"editor.acceptSuggestionOnEnter\": \"off\", \"editor.detectIndentation\": false, \"editor.minimap.autohide\": true, \"editor.unicodeHighlight.invisibleCharacters\": false, // 不高亮显示不可见字符 // window 配置，用于设置窗口的行为 \"window.openFoldersInNewWindow\": \"on\", // ----------------------------- 终端配置 ----------------------------- // 配置 Windows 平台终端 \"terminal.integrated.profiles.windows\": { \"PowerShell\": { \"source\": \"PowerShell\", \"icon\": \"terminal-powershell\" }, \"Command Prompt\": { \"path\": [ \"${env:windir}\\\\Sysnative\\\\cmd.exe\", \"${env:windir}\\\\System32\\\\cmd.exe\" ], \"args\": [], \"icon\": \"terminal-cmd\" } }, // 终端通用配置 \"terminal.integrated.defaultProfile.windows\": \"PowerShell\", \"terminal.integrated.defaultProfile.linux\": \"zsh\", \"terminal.integrated.fontFamily\": \"JetBrainsMono Nerd Font Propo\", \"terminal.integrated.enablePersistentSessions\": false, // ----------------------------- Git 配置 ----------------------------- \"git.autofetch\": false, // 禁止自动获取远程仓库的更新 \"git.enableSmartCommit\": false, \"git.confirmSync\": true, // ----------------------------- 扩展配置 ----------------------------- \"extensions.autoUpdate\": false, // SSH \"remote.SSH.showLoginTerminal\": true, // GitHub Copilot \"github.copilot.editor.enableAutoCompletions\": true, \"github.copilot.enable\": { \"*\": true, \"plaintext\": false, \"markdown\": false, \"scminput\": false }, // Git History \"gitHistory.showEditorTitleMenuBarIcons\": true, // PicGo，这里根据自己的配置来设置 \"picgo.customOutputFormat\": \"![${uploadedName}](${url} \\\"${uploadedName}\\\")\", \"picgo.picBed.aliyun.accessKeyId\": \"\", \"picgo.picBed.aliyun.accessKeySecret\": \"\", \"picgo.picBed.aliyun.area\": \"\", \"picgo.picBed.aliyun.bucket\": \"\", \"picgo.picBed.aliyun.path\": \"\", \"picgo.picBed.current\": \"aliun\", \"picgo.dataPath\": \"\", // Vim \"vim.easymotion\": true, \"vim.incsearch\": true, \"vim.useSystemClipboard\": true, \"vim.useCtrlKeys\": true, \"vim.hlsearch\": true, \"vim.leader\": \"\u003cspace\u003e\", \"vim.smartRelativeLine\": true, // 相对行号 \"vim.handleKeys\": { // false 则由 vscode 进行处理 \"\u003cC-a\u003e\": false, // 全选 \"\u003cC-p\u003e\": false, // 打开配置 \"\u003cC-w\u003e\": false, // 关闭当前窗口 \"\u003cC-q\u003e\": false, // 选择侧边栏 \"\u003cC-f\u003e\": false, // 查找 \"\u003cC-b\u003e\": false, // 保存 }, \"vim.insertModeKeyBindings\": [ { \"before\": [\"j\", \"j\"], \"after\": [\"\u003cEsc\u003e\"] } ], \"vim.normalModeKeyBindingsNonRecursive\": [ { \"before\": [\"\u003cleader\u003e\", \"d\"], \"after\": [\"d\", \"d\"] }, { \"before\": [\"\u003cC-n\u003e\"], \"commands\": [\":nohl\"] }, { \"before\": [\"K\"], \"commands\": [\"lineBreakInsert\"], \"silent\": true } ], // To improve performance \"extensions.experimental.affinity\": { \"vscodevim.vim\": 1 }, // ----------------------------- 语言配置 ----------------------------- // Cpp \"C_Cpp.clang_format_style\": \"{ BasedOnStyle: Chromium, IndentWidth: 3, ColumnLimit: 0}\", \"C_Cpp.formatting\": \"vcFormat\", // Java \"[java]\": { \"editor.defaultFormatter\": \"redhat.java\" }, \"redhat.telemetry.enabled\": true, // Python \"python.condaPath\": \"\\\"E:\\\\source\\\\miniconda3\\\\envs\\\\re\\\\python.exe\\\"\", \"[python]\": { \"editor.defaultFormatter\": \"ms-python.black-formatter\" }, // JavaScript \"[javascript]\": { \"editor.defaultFormatter\": \"vscode.typescript-language-features\" }, // markdown \"[markdown]\": { \"editor.wordWrap\": \"on\", \"editor.defaultFormatter\": \"yzhang.markdown-all-in-one\" }, } ","date":"2024.12.3","objectID":"/blog/posts/config/vscode/:1:3","tags":["config"],"title":"VsCode 配置和使用","uri":"/blog/posts/config/vscode/"},{"categories":["config"],"content":"插件相关\r","date":"2024.12.3","objectID":"/blog/posts/config/vscode/:2:0","tags":["config"],"title":"VsCode 配置和使用","uri":"/blog/posts/config/vscode/"},{"categories":["config"],"content":"PicGo\r上传配置\r因为使用 vscode 来写 Markdown 语法，所以想着仿照之前 Typora + PicGo 配置的剪切板粘贴上传图片功能。同时因为之前在 PicGo 应用中配置过，所以这次就直接获取参数然后输入即可。 配置信息\r这里的配置信息直接从软件中抄过来，或者随便看看阿里云图床的配置文章就可以知道大致该怎么填写了。之后这里 Aliyun: Path 选择之前自己创建的目录，可以通过 OSS -\u003e Bucket -\u003e 文件列表 来查看。另外选择 Pic Bed: Current 为 aliyun，这样上面的配置才可以生效。同时还设置了快捷键，对于使用截图软件，把图片放在剪切板上的情况，使用快捷键 ctrl + alt + u，这样和普通粘贴分割开了，更加方便。 文件名设置\rCustom Output Format 就是文件在 vscode 中的展现形式，这里针对 Fixit 主题中 Image 的要求 进行了修改。这样后面的 title 可以直接生成，然后我们需要修改的就是前面的 alt 了。 layouts/partials/plugin/image.html 文件在之前进行修改了，所以图片下面的图标显示的就只有 alt 了，只有鼠标放在图片上面和放大图片显示的才是 title。 本地格式\rCustom Upload Format就是上传文件的格式，随便设置即可，因为不会有太多的格式要求，这里就是默认的配置。 上传格式\r保存路径\r这里有一个 Data Path，vscode 插件会把每次的提交形成 json 数据添加到这个文件中，所以可以设置存储路径，之后查看图片可以迅速找到相关信息。这里我也将路径设置为博客的仓库，使其作为附属信息上传到 github 上。 路径配置\r","date":"2024.12.3","objectID":"/blog/posts/config/vscode/:2:1","tags":["config"],"title":"VsCode 配置和使用","uri":"/blog/posts/config/vscode/"},{"categories":["config"],"content":"java 环境配置\r下载\r主要使用 wsl 来配置 java 环境来做实验，所以对于 java 的下载直接使用包安装工具来进行。 配置环境主要需要两个插件 Language Support for Java(TM) by Red Hat 和 Extension Pack for Java，然后它们会连续下载多个插件，一般这两个下载完了就配置了基础环境。 Language Support for Java(TM) by Red Hat\rExtension Pack for Java\r项目创建\r之后我们就可以在 Ctrl + P 打开的配置栏中创建 java 项目了。 项目添加\rjar 包添加\r如果我们想要添加外部下载的 jar 包，我们可以使用 Maven 来进行配置（之前插件也一起下载了）。但是我这里没有相关环境，所以采用手动导入的方式。因此可以在一个 java 项目中打开 java 文件，可以看见下面的 JAVA PROJECTS 标识，它可以进行相关配置。 我们可以直接点击 Referenced Libraries 右侧的 + 直接在文件系统中找自己下载的 jar 包。 添加 jar 包\r或者也可以直接如下图所示在配置界面进行添加。 添加 jar 包\r","date":"2024.12.3","objectID":"/blog/posts/config/vscode/:2:2","tags":["config"],"title":"VsCode 配置和使用","uri":"/blog/posts/config/vscode/"},{"categories":["config"],"content":"jupyter\r配置环境\r我是用的就是 miniconda 环境。下载并配置环境之后，在 vscode 中下载插件 python 和 jupyter。之后在 settings 中输入 python path ，然后输入 miniconda -\u003e envs -\u003e python.exe 的文件位置，这样之后就会直接使用默认的 python 解释器（我猜的，还没有经过验证）。 配置 python 路径\r介绍\r对我来说，Jupyter Notebook 在编程时具有语法高亮、缩进、tab补全的功能。并且可以在代码块下方展示运行结果。对代码编写说明文档或语句时，支持 Markdown 语法，支持使用 LaTeX 编写数学性说明。我主要想的就是可以通过划分不同的代码块来加强代码的理解，同时还可以使用 markdown 语法来进行讲解，这个比 python 需要最后再运行强很多，我需要的就是这种划分代码段分别运行，最后还能关联在一起的效果。这里之后根据 官方指南 进行 jupyter 的讲解。 运行细胞\r在这里，每一个代码块被称为一个 cell。要运行代码，可以在命令和编辑模式下使用键盘快捷键。要运行当前单元格，使用 Ctrl+Enter。要运行当前单元格并前进到下一个单元格，使用 Shift+Enter。之后也可以使用图形界面的选项来进行相关操作。 上文提到了命令模式和编辑模式，所以这里需要知道运行细胞存在三种个状态：未选中、命令模式和编辑模式。代码单元格和编辑器边框左侧的垂直条显示单元格的当前状态。 当没有可见的栏时，该单元格未被选中。选择单元格后，它可以处于命令模式或编辑模式 在命令模式下，单元格左侧将出现一个实心垂直条，该单元可以进行操作并接受键盘命令 在编辑模式下，单元格编辑器周围有一个实心垂直条由边框连接起来，单元格的内容可以修改 在键盘上，按 Enter 键可进入编辑模式，按 Esc 键可进入命令模式。 注意 有的主题不能呈现上述的颜色条变化，所以为了兼顾主题和 jupyter，我选择 Github Theme 中的 Complete Dark 主题，起码比较而言，它可以兼顾两者。后面发现这个主题是在别的插件共同渲染下才满足我的想法的，把别的插件禁用之后就不太好看了，所以又找了一个插件 Learn with Sumit Theme，选择 Blue Velvet 主题。 快捷键\r命令模式\r快捷键 备注 Enter 转入编辑模式 Shift/Alt+Enter 运行当前选定的单元格并在紧邻下方插入一个新单元格（焦点移至新单元格） Ctrl+Alt+Enter 运行当前选定的单元格 y 单元转入代码状态 m 单元转入markdown状态 up 选中上方单元 k 选中上方单元 down 选中下方单元 j 选中下方单元 a 在上方插入新单元 b 在下方插入新单元 dd 删除选中的单元 l 转换行号 Shift+Space 向上滚动 Space 向下滚动 编辑模式\r这里实际上就是 vscode 的内置快捷键，但是由于我下载了 IDEA 快捷键的插件和别的一些插件，所以目前 vscode 的快捷键很混乱，下main记录一下当前可以使用的快捷键操作。 快捷键 备注 Esc 转入命令模式 Shift/Alt+Enter 运行当前选定的单元格并在紧邻下方插入一个新单元格（焦点移至新单元格） Ctrl+Alt+Enter 运行当前选定的单元格 Ctrl + X 剪切/剪切行（空选定） Ctrl + C 复制/复制行（空选定） Delete / Backspace 删除光标右边、左边的字 Alt + ↑ / ↓ 向上/向下移动行 Ctrl + D 向下复制行（来自 IDEA 的快捷键） Ctrl + Y 删除行（来自 IDEA 的快捷键） Ctrl + Shift + \\ 跳到匹配的括号 Ctrl + ] / [ 缩进/突出行 Ctrl + ← / → 光标到字首/字尾 Ctrl + / 切换行注释 Shift + Alt + A 切换块注释 ","date":"2024.12.3","objectID":"/blog/posts/config/vscode/:2:3","tags":["config"],"title":"VsCode 配置和使用","uri":"/blog/posts/config/vscode/"},{"categories":["config"],"content":"Vim\r为了方便在所有的编辑器中都可以使用同一套快捷键，所以采用了 Vim 作为自己的编辑方式，这里记录的就是自己的配置。对于 vscode 而言就是下载相应的插件，这里就是在插件市场搜索 Vim 插件进行下载即可，如下面图片所示。 Vim 插件\r关于 Vim 的具体配置和使用在另外一篇文章中，方便之后直接抄配置。 ","date":"2024.12.3","objectID":"/blog/posts/config/vscode/:2:4","tags":["config"],"title":"VsCode 配置和使用","uri":"/blog/posts/config/vscode/"},{"categories":["config"],"content":"为了更好的使用电脑，这里记录一下关于 Windows 的相关配置。 ","date":"2024.12.3","objectID":"/blog/posts/config/windows/:0:0","tags":["config"],"title":"Windows 便捷使用","uri":"/blog/posts/config/windows/"},{"categories":["config"],"content":"电脑相关\r","date":"2024.12.3","objectID":"/blog/posts/config/windows/:1:0","tags":["config"],"title":"Windows 便捷使用","uri":"/blog/posts/config/windows/"},{"categories":["config"],"content":"屏幕鼠标切换\r目前使用的是双屏，虽然想的是手不离开键盘，但是屏幕切换的时候总要调整当前鼠标的位置，不然焦点就在另一个屏幕上了。所以发现了项目 MouseSwitcher，这里我设置通过快捷键 \u003cAlt-left/right\u003e 进行鼠标的切换。虽然实验下来发现不能解决聚焦的问题，但是能切换鼠标就是进步，聚焦可以通过别的程序快捷键来实现。这里贴一下 介绍，可以看这个来进行掌握。 后期发现可以在 控制面板 -\u003e 轻松使用设置中心 -\u003e 使鼠标更易于使用 中进行配置，选择这个 通过将鼠标悬停在窗口上来激活窗口 就可以了。这样当我们在不同屏幕切换鼠标的时候，我们就可以直接激活聚焦于当前窗口了，快捷方便。 鼠标设置\r后面发现其实使用 \u003cShift-Tab\u003e 切换标签页也能实现聚焦的切换，然后配合上面的设置，就可以进行达成一样的效果。所以后期如果熟练使用标签页的话，可以把这个工具舍弃了。再后来发现这个 是鼠标更易于使用 的作用不大，当我进行界面的切换的时候，虽然鼠标不在我切换后的界面中，但是控制权已经切换了。像一些快捷键，比如 PgUp/PgDn 也可以在转换后的界面进行使用，所以这个可以不进行设置。设置后的坏处就是一些暂时的窗口，如 utools，状态栏 就会一闪而过，很烦人。 ","date":"2024.12.3","objectID":"/blog/posts/config/windows/:2:0","tags":["config"],"title":"Windows 便捷使用","uri":"/blog/posts/config/windows/"},{"categories":["config"],"content":"Edge\r","date":"2024.12.3","objectID":"/blog/posts/config/windows/:3:0","tags":["config"],"title":"Windows 便捷使用","uri":"/blog/posts/config/windows/"},{"categories":["config"],"content":"标签页切换\r面对众多标签页，可以使用快捷键进行切换。其中从左到右是 \u003cShift-Tab\u003e，从右到左是 \u003cCtrl-Shift-Tab\u003e。而对于我的垂直页面，从上到下就是 \u003cShift-Tab\u003e，从下到上就是 \u003cCtrl-Shift-Tab\u003e。同时可以使用 \u003cCtrl-count\u003e 的方式来切换标签页，\u003cCtrl-9\u003e 就是最后一个，而其它的则是按顺序特定切换。 ","date":"2024.12.3","objectID":"/blog/posts/config/windows/:3:1","tags":["config"],"title":"Windows 便捷使用","uri":"/blog/posts/config/windows/"},{"categories":["config"],"content":"主页展示\r使用 \u003cCtrl-T\u003e 可以直接打开新的主页，同时这里配置了 青柠起始页 插件，使得主页直接聚焦于上方搜索栏中，这样就可以直接键盘输入内容进行搜索了。并且 \u003cCtrl-N\u003e 为打开新的 edge 浏览器，可以实现新开。 ","date":"2024.12.3","objectID":"/blog/posts/config/windows/:3:2","tags":["config"],"title":"Windows 便捷使用","uri":"/blog/posts/config/windows/"},{"categories":["config"],"content":"其余操作\r可以使用 PhUp 和 PgDn(空格) 替代鼠标的滚轮，实现上下页滑动的效果 \u003cCtrl-l\u003e 和 \u003cAlt-d\u003e 都是选择地址栏中的 URL 以进行编辑 \u003cCtrl-r\u003e 和 F5 作用一样，重新加载当前页，不过有缓存内容，加上 Shift 可以忽略缓存内容 \u003cCtrl-w\u003e 关闭当前标签页，加上 Shift 就是关闭当前窗口了 \u003cAlt-left/right\u003e 是返回/前进，但是我们在上面屏幕的切换中占用了这个快捷键，所以不能使用了 ","date":"2024.12.3","objectID":"/blog/posts/config/windows/:3:3","tags":["config"],"title":"Windows 便捷使用","uri":"/blog/posts/config/windows/"},{"categories":["config"],"content":"Vscode\r具体就是看 VsCode 配置和使用那篇文章，里面包含快捷键的使用。 ","date":"2024.12.3","objectID":"/blog/posts/config/windows/:4:0","tags":["config"],"title":"Windows 便捷使用","uri":"/blog/posts/config/windows/"},{"categories":["android"],"content":"记录一下 Unidbg 的使用。 具体的介绍就不讲了，使用就是直接从 官网 拉取代码，然后在 IDEA 中进行配置即可。基础操作可以看 博客。 ","date":"2024.11.30","objectID":"/blog/posts/android/unidbg/:0:0","tags":["android"],"title":"Unidbg 记录","uri":"/blog/posts/android/unidbg/"},{"categories":["android"],"content":"基础板子\r下面展示的就是基础使用的板子，每次对于 so 文件的分析都需要先设置这个板子，然后再在板子上进行自己需要的配置。 package com.ctf.wmctf; import com.github.unidbg.AndroidEmulator; import com.github.unidbg.Module; import com.github.unidbg.linux.android.AndroidEmulatorBuilder; import com.github.unidbg.linux.android.AndroidResolver; import com.github.unidbg.linux.android.dvm.DalvikModule; import com.github.unidbg.linux.android.dvm.VM; import com.github.unidbg.memory.Memory; import java.io.File; public class easyAndroid { private final AndroidEmulator emulator; private final VM vm; private final Module module; private final boolean logging; easyAndroid(boolean logging) { this.logging = logging; emulator = AndroidEmulatorBuilder.for64Bit() .setProcessName(\"com.s0rry.easyandroid\") .build(); // 创建模拟器实例，进程名建议依照实际进程名填写，可以规避针对进程名的校验 final Memory memory = emulator.getMemory(); // 模拟器的内存操作接口 memory.setLibraryResolver(new AndroidResolver(23)); // 设置系统类库解析 // 创建 Android 虚拟机并传入 APK（可不传，感觉无所谓），Unidbg 可以替我们做部分签名校验的工作 vm = emulator.createDalvikVM(new File(\"unidbg-android\\\\unidbg-android\\\\src\\\\test\\\\resources\\\\ctf\\\\wmctf\\\\easyAndroid.apk\")); vm.setVerbose(logging); // 设置是否打印Jni调用细节 // 加载目标 so 文件到 unicorn 虚拟内存，加载成功以后会默认调用 init_array 等函数 DalvikModule dm = vm.loadLibrary(new File(\"unidbg-android\\\\src\\\\test\\\\resources\\\\ctf\\\\wmctf\\\\libeasyandroid.so\"), true); dm.callJNI_OnLoad(emulator); // 手动执行JNI_OnLoad函数 module = dm.getModule(); // 获取本 so 模块的句柄 } public static void main(String[] args) { easyAndroid easyandroid = new easyAndroid(true); } } ","date":"2024.11.30","objectID":"/blog/posts/android/unidbg/:1:0","tags":["android"],"title":"Unidbg 记录","uri":"/blog/posts/android/unidbg/"},{"categories":["android"],"content":"Hook\rhook 在 Unidbg 感觉是最为重要的东西。有了它的存在，我们才能在调试中获取更多的信息，及其推荐一篇 文章，这个写的极好，感觉把我自己能想到的需要 hook 的场景方法都讲了一遍。 ","date":"2024.11.30","objectID":"/blog/posts/android/unidbg/:2:0","tags":["android"],"title":"Unidbg 记录","uri":"/blog/posts/android/unidbg/"},{"categories":["android"],"content":"hook 时机过晚\r我们正常的 hook 都是位于 so 加载之后，执行 JNI_OnLoad 之前，而我们的加载顺序为 init -\u003e init_array -\u003e JNI_OnLoad，所以如果 .init 段 和 .init_array 段存在代码逻辑，我们想要去 hook，那么使用之前的方法就太晚了，所以我们需要将 hook 的时机提前到 init 执行前。由此可以看上面文章的 讲述，存在三种应对方案，我这里只是为了快速回忆而做的笔记： 提前加载 libc：这个就需要我们想要 hook 的是目标函数中的 libc 函数。比如 .init 段存在 strcmp，我们想看看比对的数据，这个时候就可以首先 hook libc 来得到相关数据。但是如果目标函数不是这个，而是某个地址的寄存器信息等，那么就没有办法了。 固定地址下断点：我们都是通过 vm.loadLibrary 来加载第一个用户的 so 文件，基地址固定为 0x40000000。因此可以通过偏移和基地址计算出固定地址，之后通过 Console Debugger 来 Hook。 使用 Unidbg 提供的模块监听器：这是我认为最为合适的方案，就是添加一个监听器。之后要么用第三方 hook 框架，要么使用原生 unicornHook，这两个分情况使用，但是这个方案我觉得最为合适。具体的使用可以看看自己写的 WMCTF2024 easyAndroid 的反混淆 -\u003e notion。 ","date":"2024.11.30","objectID":"/blog/posts/android/unidbg/:2:1","tags":["android"],"title":"Unidbg 记录","uri":"/blog/posts/android/unidbg/"},{"categories":["android"],"content":"注意事项\r在 unidbg 的项目中不要进行 IDA 的分析，因为产生的分析文件是只读的，Maven 不能自动复制它们，这就会产生 build 的错误 ","date":"2024.11.30","objectID":"/blog/posts/android/unidbg/:3:0","tags":["android"],"title":"Unidbg 记录","uri":"/blog/posts/android/unidbg/"},{"categories":["compile"],"content":"这里记录 llvm、ir 相关知识。 ","date":"2024.11.28","objectID":"/blog/posts/compile/llvm-ir-control/:0:0","tags":["compile"],"title":"04 LLVM IR Control","uri":"/blog/posts/compile/llvm-ir-control/"},{"categories":["compile"],"content":"LLVM\r","date":"2024.11.28","objectID":"/blog/posts/compile/llvm-ir-control/:1:0","tags":["compile"],"title":"04 LLVM IR Control","uri":"/blog/posts/compile/llvm-ir-control/"},{"categories":["compile"],"content":"介绍\r注意 The LLVM Project is a collection of modular and reusable compiler and tool-chain technologies. Despite its name, LLVM has little to do with traditional virtual machines. The name “LLVM” itself is not an acronym; it is the full name of the project. LLVM IR\r","date":"2024.11.28","objectID":"/blog/posts/compile/llvm-ir-control/:1:1","tags":["compile"],"title":"04 LLVM IR Control","uri":"/blog/posts/compile/llvm-ir-control/"},{"categories":["compile"],"content":"IR\rIR\rIR 生成\r使用 clang -S -emit-llvm -fno-discard-value-names xxx.c -o xxx.ll -O1 -g0 将 c 代码转化为 IR 的形式。 -S：生成汇编代码（IR）作为输出，而不是直接生成二进制文件或可执行文件 -emit-llvm：生成 LLVM 中间表示（IR），而不是目标平台的汇编代码。-S 都是和它一起进行使用 -fno-discard-value-names：在生成的 IR 中不丢弃值的名称（如变量名和函数名），保留符号信息，便于调式或分析 -o xxx.ll：指定生成人类可读的 .ll 形式文件，而不是 bitcode 形式的 .bc 形式文件 -O1：优化级别设置，-O1 表示中等优化级别 -g0：控制调试信息的级别，它表示不包含调试信息 之后会形成下面形式的代码。它是强类型，同时为三地址码和静态单赋值形式。 .ll文件\r这里静态单赋值就是每个寄存器只被定义一次，也就是它只在左侧出现一遍，右侧没有限制。这样在软件上就可以实现无限制个数的寄存器了，同时对于变量定义的寻找也更为简便。下面就是 SSA 的转换，要使控制流满足 SSA，那么需要使用 $\\phi$ 函数来根据控制流决定选择 y1 还是 y2。 SSA转换\r不同优化等级的控制流\r都是对于下面的代码通过上述命令生成不同优化等级的 IR 代码。 int factorial(int val){ if(val == 0){ return 1; } return val * factorial(val - 1); } 下面就是没有优化的 CFG 图，可以看出它有一个开辟空间的操作，同时左右分支都把结果传到了 %2 这个寄存器之中，然后返回基本块姐可以直接使用这个进行返回。 无优化 CFG\r而下面就是 o1 优化的 CFG 图，它使用了 $\\phi$ 函数来对分支进行处理，若是从 %3 基本块跳转来的，那么返回值使用 %6；若是从 %1 基本块跳转来的，返回值使用 1。 o1 优化 CFG\r然后对于下面的代码，不同优化等级的 IR 代码列举如下。 int factorial(int val){ int temp = 1; for (int i = 2; i \u003c= val; i ++){ temp *= i; } return temp; } 这里没有优化的话，就会出现开辟空间的操作（下面的 alloca），然后因此涉及 load 和 store 操作。 无优化版本\r而若是采用了 o1 优化，那么上述开辟空间和加载等就没有了，整个代码都很简洁，多出了 $\\phi$ 函数的操作。 o1 优化版本\r","date":"2024.11.28","objectID":"/blog/posts/compile/llvm-ir-control/:1:2","tags":["compile"],"title":"04 LLVM IR Control","uri":"/blog/posts/compile/llvm-ir-control/"},{"categories":["compile"],"content":"中间代码生成\r","date":"2024.11.28","objectID":"/blog/posts/compile/llvm-ir-control/:2:0","tags":["compile"],"title":"04 LLVM IR Control","uri":"/blog/posts/compile/llvm-ir-control/"},{"categories":["compile"],"content":"表达式翻译\r非布尔表达式翻译\r这里如图所示，E.code 表示翻译后的中间代码，E.addr 表示原本的变量和生成的中间变量。下面的流程就是通过构建语法树，然后自底向上匹配规则，逐一生成中间变量，同时也就生成了中间代码。 表达式翻译\r数组引用翻译\r下面就是数组引用的中间代码翻译流程，我们首先就是需要对声明进行解析，这样我们才能得到数组的宽度。之后才会对于 i，j 进行解析，运用公式得到最后的地址。 数组引用翻译\r对于数组声明就采用之前的翻译制导方案，通过继承属性和综合属性来传递相关信息，最后形成数组类型。 语法制导翻译\r翻译规则\r翻译语法树\r这里主要关注 getelementptr，它对于数组元素进行解包获取内部信息。对于这里的参数 [2 x [3 x i32]] 是 base type，[2 x [3 x i32]]* %2 是数组的首地址，同时返回一个指向元素的指针，所以后面需要进行 load 操作。 翻译后中间代码\r","date":"2024.11.28","objectID":"/blog/posts/compile/llvm-ir-control/:2:1","tags":["compile"],"title":"04 LLVM IR Control","uri":"/blog/posts/compile/llvm-ir-control/"},{"categories":["compile"],"content":"控制流翻译（easy）\r这里就是对于下面的产生式进行翻译，转化为中间代码的形式。同时使用的是 Visitor 方式，因为我们这里需要及时输出生成的中间代码，避免频繁的字符串拼接操作，而使用 Listener 模式会有上面的问题，直接进行 Attributed Grammar 方式则会很麻烦。 同时这里为简单模式的翻译，它只需要使用综合属性，各个非终结符的分工、合作明确。所以产生的代码也相对冗杂。 产生式翻译\r翻译规则\rif 条件语句\r下面就是对于 if 条件语句的翻译规则。总结起来就是我们首先需要标记两个标签 true 和 false，然后看 if 中的 cond 来分配这两个标签到 if 中的语句块和 if 结尾，这样我们就实现了对于 if 语句的翻译。 最终生成的 IR 也是如下面的类型： # 源代码 if (true) { a = b; b = c; } # 翻译代码 br true b.true1 b.false2 b.true1: a = b c = c b.false2: if else 语句\r而对于 if else 形式的，需要在上面的基础上添加一个 end 的标签，这样比如在 true 的语句执行完之后，直接到 if 之后的语句，而不是 else 语句。 最终生成的 IR 也是如下面的类型： # 源代码 if (true) { a = b; if (true) { c = d; } else { e = g; } } else { d = f; } # 翻译代码 br true b.true1 b.false2 b.true1: a = b br true b.true4 b.false5 b.true4: c = d br end.6 b.false5: e = g b.end6: br b.end3 b.false2: d = f b.end3: while 循环语句\r对于 while 循环，我们需要设置一个 begin 的标签，这样我们才可以在满足循环条件的前提下从头开始执行代码。 # 源代码 while (true) { a = b; if (true) { c = d; } else { e = f; } } a = c; # 翻译代码 begin1: br true b.true2 b.false3 b.true2: a = b br true b.true4 b.false5 b.true4: c = d br b.end6 b.false5: e = f b.end6: br begin1 b.false3: a = c break 语句\r这里 break 语句的翻译难点就在于它只会跳出离他最近的一层循环，所以我们需要追踪它会跳到哪一个标签处。面对这种情况，我们采用栈结构来存储相关标签。如下图所示，我们在内外层 while 处分别压入相关标签，面对 break 时就跳到栈顶的标签处，这样就实现了只跳出离它最近一层的功能。 break 栈展示\r# 源代码 while (true) { while (false) { if (true) { break; # 这里的 break 只会跳出 while } a = b; } a = c; break; a = d; } a = e; # 翻译代码 begin1: br true b.true2 b.false3 b.true2: begin4: br false b.true5 b.false6 b.true5: br true b.true7 b.false8 b.true7: br b.false6 b.false8: a = b br begin4 b.false6: a = c br b.flase3 a = d br begin1 b.false3: a = e 短路求值\r短路求值和 if else 语句很像，例如 ||，前面的正确后，要标记正确并跳转到最后，前面失败，那么就看后面的正误来跳转。 # 源代码 while (true || false) { a = b; } a = c; # 翻译代码 begin1: br true or.true2 or.false3 or.false3: t1 = false br or.end4 or.true2: t1 = true or.end4: br t1 b.true5 b.false6 b.true5: a = b br begin1 b.false6: a = c ","date":"2024.11.28","objectID":"/blog/posts/compile/llvm-ir-control/:2:2","tags":["compile"],"title":"04 LLVM IR Control","uri":"/blog/posts/compile/llvm-ir-control/"},{"categories":["compile"],"content":"控制流翻译（hard）\r这里为困难模式，和简单模式相比，它直接用布尔表达式改变控制流，无需计算最终逻辑值，即将不同非终结符的作用混杂在一起。因此它需要从父节点获取更具体的跳转目标，缩短跳转路径。为此父节点要为子节点准备跳转指令的目标标签，子节点就通过继承属性确定跳转目标。 if 语句\r如下图所示，在困难模式下 $B.false = S_{1}.next = S.next$，这样通过将不同非终结符设置的标签根据位置组合成一个的做法简化了生成的代码。 相关操作\rif 语句\rif else 语句\r相关操作\rif else 语句\rwhile 循环语句\r相关操作\rwhile 循环语句\r布尔表达式\r布尔表达式\r具体例子\r","date":"2024.11.28","objectID":"/blog/posts/compile/llvm-ir-control/:2:3","tags":["compile"],"title":"04 LLVM IR Control","uri":"/blog/posts/compile/llvm-ir-control/"},{"categories":["compile"],"content":"backpatch\r技术介绍\r根据以下的规则，综合属性 $B.truelist$ 保存需要跳转到 $B.true$ 标签的指令，综合属性 $B.falselist$ 保存需要跳转到 $B.false$ 标签的指令，综合属性 $S/L.nextlist$ 保存需要跳转到 $S/L.next$ 标签的指令。 之后就是为左部非终结符 $B$ 计算综合属性 $B.truelist$ 与 $B.falselist$，为左部非终结符 $S/L$ 计算综合属性 $S/L.nextlist$，并考虑每个综合属性，为已能确定目标地址的跳转指令进行回填。下面只有 (3) 与 (7) 的 gen 生成了新的代码，控制流语句主要目的就是 控制 流。 相关规则\r由上面的做法，需要为回填操作添加多的属性来获取信息，由此就有了 M 属性和 N 属性。 M 属性\rM 属性就是获取下一个 code 的第一条语句的位置，这样我们就可以得到相关的位置信息了，之后我们就可以根据这个信息来进行回填。 M 属性示例\rN 属性\rN 属性就是添加一条 goto 语句，一开始就是为了解决 if else 的跳转到最后的情况。 N 相关规则\r","date":"2024.11.28","objectID":"/blog/posts/compile/llvm-ir-control/:2:4","tags":["compile"],"title":"04 LLVM IR Control","uri":"/blog/posts/compile/llvm-ir-control/"},{"categories":["compile"],"content":"目标代码生成\r","date":"2024.11.28","objectID":"/blog/posts/compile/llvm-ir-control/:3:0","tags":["compile"],"title":"04 LLVM IR Control","uri":"/blog/posts/compile/llvm-ir-control/"},{"categories":["reverse"],"content":"记录一下在逆向过程中常使用的算法，记录一下 python 脚本。 ","date":"2024.11.27","objectID":"/blog/posts/reverse/algorithm/:0:0","tags":["reverse","algorithm"],"title":"逆向中的算法","uri":"/blog/posts/reverse/algorithm/"},{"categories":["reverse"],"content":"base64\rfrom Crypto.Util.number import * # Base64 字符表 table = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\" def encode(data): n = bytes_to_long(data) # 转换为整数 pad = (3 - (len(data) % 3)) % 3 # 计算需要填充的字节数 n \u003c\u003c= 8 * pad # 左移填充 indices = [] # 存储 Base64 索引 while n: # 逐步取模 indices.append(n % 64) n //= 64 indices = indices[::-1] # 反转顺序 result = \"\".join([table[i] for i in indices]) # 索引映射到字符表 result += \"=\" * pad # 添加填充字符 print(result) encode(b\"flag{wecome_to_try_base64}\") 直接使用库来进行加解密 import base64 # Python3 中字符都是 unicode 编码，而 b64encode函数的参数为 byte 类型，所以必须先转码 enc = base64.b64encode(\"AlwaysBeta\".encode(\"utf-8\")) dec = base64.b64decode(enc) # 换表解密 new_table = \"ABCDEFQRSTUVWXYPGHIJKLMNOZabcdefghijklmnopqrstuvwxyz0123456789+/\" old_table = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\" cipher = \"zMXHz3TIgnxLxJhFAdtZn2fFk3lYCrtPC2l9\".swapcase() # swapcase 是大小写转化，大写转小写等 print(base64.b64decode(cipher.translate(str.maketrans(new_table, old_table)))) ","date":"2024.11.27","objectID":"/blog/posts/reverse/algorithm/:1:0","tags":["reverse","algorithm"],"title":"逆向中的算法","uri":"/blog/posts/reverse/algorithm/"},{"categories":["reverse"],"content":"RC4\rdef RC4(input, key): sbox = list(range(256)) j = 0 for i in range(256): j = (j + sbox[i] + key[i % len(key)]) % 256 sbox[i], sbox[j] = sbox[j], sbox[i] i, j = 0, 0 output = [] keystream = [] for k in range(len(input)): i = (i + 1) % 256 j = (j + sbox[i]) % 256 sbox[i], sbox[j] = sbox[j], sbox[i] keystream.append(sbox[(sbox[i] + sbox[j]) % 256]) output.append(input[k] ^ keystream[k]) return bytes(output), keystream key = b\"fun@eZ\" out = RC4(b\"Hello, world!\", key)[0] print(out) inp = RC4(out, key)[0] print(inp) ","date":"2024.11.27","objectID":"/blog/posts/reverse/algorithm/:2:0","tags":["reverse","algorithm"],"title":"逆向中的算法","uri":"/blog/posts/reverse/algorithm/"},{"categories":["reverse"],"content":"Tea 系列\r","date":"2024.11.27","objectID":"/blog/posts/reverse/algorithm/:3:0","tags":["reverse","algorithm"],"title":"逆向中的算法","uri":"/blog/posts/reverse/algorithm/"},{"categories":["reverse"],"content":"tea\rfrom ctypes import * def encrypt(rounds, v, k): v0, v1 = c_uint32(v[0]), c_uint32(v[1]) delta = 0x9E3779B9 sum = c_uint32(0) k0, k1, k2, k3 = c_uint32(k[0]), c_uint32(k[1]), c_uint32(k[2]), c_uint32(k[3]) for i in range(rounds): sum.value += delta v0.value += ((v1.value \u003c\u003c 4) + k0.value) ^ (v1.value + sum.value) ^ ((v1.value \u003e\u003e 5) + k1.value) v1.value += ((v0.value \u003c\u003c 4) + k2.value) ^ (v0.value + sum.value) ^ ((v0.value \u003e\u003e 5) + k3.value) return [v0.value, v1.value] def decrypt(rounds, v, k): v0, v1 = c_uint32(v[0]), c_uint32(v[1]) delta = 0x9E3779B9 sum = c_uint32(delta * rounds) k0, k1, k2, k3 = c_uint32(k[0]), c_uint32(k[1]), c_uint32(k[2]), c_uint32(k[3]) for i in range(rounds): v1.value -= (((v0.value \u003c\u003c 4) + k2.value) ^ (v0.value + sum.value) ^ ((v0.value \u003e\u003e 5) + k3.value)) v0.value -= (((v1.value \u003c\u003c 4) + k0.value) ^ (v1.value + sum.value) ^ ((v1.value \u003e\u003e 5) + k1.value)) sum.value -= delta return [v0.value, v1.value] v = [1, 2] k = [1, 2, 3, 4] enc = encrypt(32, v, k) print(\"%#x %#x\" % (enc[0], enc[1])) dec = decrypt(32, enc, k) print(\"%#x %#x\" % (dec[0], dec[1])) ","date":"2024.11.27","objectID":"/blog/posts/reverse/algorithm/:3:1","tags":["reverse","algorithm"],"title":"逆向中的算法","uri":"/blog/posts/reverse/algorithm/"},{"categories":["reverse"],"content":"xtea\rfrom ctypes import * def encrypt(rounds, v, k): v0, v1 = c_uint32(v[0]), c_uint32(v[1]) delta = 0x9E3779B9 sum = c_uint32(0) for i in range(rounds): v0.value += (((v1.value \u003c\u003c 4) ^ (v1.value \u003e\u003e 5)) + v1.value) ^ (sum.value + k[sum.value \u0026 3]) sum.value += delta v1.value += (((v0.value \u003c\u003c 4) ^ (v0.value \u003e\u003e 5)) + v0.value) ^ (sum.value + k[(sum.value \u003e\u003e 11) \u0026 3]) return [v0.value, v1.value] def decrypt(rounds, v, k): v0, v1 = c_uint32(v[0]), c_uint32(v[1]) delta = 0x9E3779B9 sum = c_uint32(delta * rounds) for i in range(rounds): v1.value -= (((v0.value \u003c\u003c 4) ^ (v0.value \u003e\u003e 5)) + v0.value) ^ (sum.value + k[(sum.value \u003e\u003e 11) \u0026 3]) sum.value -= delta v0.value -= (((v1.value \u003c\u003c 4) ^ (v1.value \u003e\u003e 5)) + v1.value) ^ (sum.value + k[sum.value \u0026 3]) return [v0.value, v1.value] v = [1, 2] k = [1, 2, 3, 4] enc = encrypt(32, v, k) print(\"%#x %#x\" % (enc[0], enc[1])) dec = decrypt(32, enc, k) print(\"%#x %#x\" % (dec[0], dec[1])) ","date":"2024.11.27","objectID":"/blog/posts/reverse/algorithm/:3:2","tags":["reverse","algorithm"],"title":"逆向中的算法","uri":"/blog/posts/reverse/algorithm/"},{"categories":["reverse"],"content":"xxtea\rfrom ctypes import * def MX(z, y, total, key, p, e): temp1 = (z.value \u003e\u003e 5 ^ y.value \u003c\u003c 2) + (y.value \u003e\u003e 3 ^ z.value \u003c\u003c 4) temp2 = (total.value ^ y.value) + (key[(p \u0026 3) ^ e.value] ^ z.value) return c_uint32(temp1 ^ temp2) def encrypt(n, v, key): delta = 0x9E3779B9 rounds = 6 + 52 // n total = c_uint32(0) z = c_uint32(v[n - 1]) e = c_uint32(0) while rounds \u003e 0: total.value += delta e.value = (total.value \u003e\u003e 2) \u0026 3 for p in range(n - 1): y = c_uint32(v[p + 1]) v[p] = c_uint32(v[p] + MX(z, y, total, key, p, e).value).value z.value = v[p] y = c_uint32(v[0]) v[n - 1] = c_uint32(v[n - 1] + MX(z, y, total, key, n - 1, e).value).value z.value = v[n - 1] rounds -= 1 return v def decrypt(n, v, key): delta = 0x9E3779B9 rounds = 6 + 52 // n total = c_uint32(rounds * delta) y = c_uint32(v[0]) e = c_uint32(0) while rounds \u003e 0: e.value = (total.value \u003e\u003e 2) \u0026 3 for p in range(n - 1, 0, -1): z = c_uint32(v[p - 1]) v[p] = c_uint32((v[p] - MX(z, y, total, key, p, e).value)).value y.value = v[p] z = c_uint32(v[n - 1]) v[0] = c_uint32(v[0] - MX(z, y, total, key, 0, e).value).value y.value = v[0] total.value -= delta rounds -= 1 return v v = [0x1, 0x2] k = [0x1, 0x2, 0x3, 0x4] n = 2 # 这里2为轮数，也就是 v 的个数 enc = encrypt(n, v, k) print(\"%#x %#x\" % (enc[0], enc[1])) dec = decrypt(n, enc, k) print(\"%#x %#x\" % (dec[0], dec[1])) ","date":"2024.11.27","objectID":"/blog/posts/reverse/algorithm/:3:3","tags":["reverse","algorithm"],"title":"逆向中的算法","uri":"/blog/posts/reverse/algorithm/"},{"categories":["reverse"],"content":"SM 系列\r","date":"2024.11.27","objectID":"/blog/posts/reverse/algorithm/:4:0","tags":["reverse","algorithm"],"title":"逆向中的算法","uri":"/blog/posts/reverse/algorithm/"},{"categories":["reverse"],"content":"SM4\rSM4 算法的原理参考这篇 文章。这里需要注意一下， SM4 是对称密码，它一次性加密 32 x 4 bit 的数据，也就是对 16 字节的数据进行加密。 # S盒，用于字节替换 S_BOX = [ 0xd6, 0x90, 0xe9, 0xfe, 0xcc, 0xe1, 0x3d, 0xb7, 0x16, 0xb6, 0x14, 0xc2, 0x28, 0xfb, 0x2c, 0x05, 0x2b, 0x67, 0x9a, 0x76, 0x2a, 0xbe, 0x04, 0xc3, 0xaa, 0x44, 0x13, 0x26, 0x49, 0x86, 0x06, 0x99, 0x9c, 0x42, 0x50, 0xf4, 0x91, 0xef, 0x98, 0x7a, 0x33, 0x54, 0x0b, 0x43, 0xed, 0xcf, 0xac, 0x62, 0xe4, 0xb3, 0x1c, 0xa9, 0xc9, 0x08, 0xe8, 0x95, 0x80, 0xdf, 0x94, 0xfa, 0x75, 0x8f, 0x3f, 0xa6, 0x47, 0x07, 0xa7, 0xfc, 0xf3, 0x73, 0x17, 0xba, 0x83, 0x59, 0x3c, 0x19, 0xe6, 0x85, 0x4f, 0xa8, 0x68, 0x6b, 0x81, 0xb2, 0x71, 0x64, 0xda, 0x8b, 0xf8, 0xeb, 0x0f, 0x4b, 0x70, 0x56, 0x9d, 0x35, 0x1e, 0x24, 0x0e, 0x5e, 0x63, 0x58, 0xd1, 0xa2, 0x25, 0x22, 0x7c, 0x3b, 0x01, 0x21, 0x78, 0x87, 0xd4, 0x00, 0x46, 0x57, 0x9f, 0xd3, 0x27, 0x52, 0x4c, 0x36, 0x02, 0xe7, 0xa0, 0xc4, 0xc8, 0x9e, 0xea, 0xbf, 0x8a, 0xd2, 0x40, 0xc7, 0x38, 0xb5, 0xa3, 0xf7, 0xf2, 0xce, 0xf9, 0x61, 0x15, 0xa1, 0xe0, 0xae, 0x5d, 0xa4, 0x9b, 0x34, 0x1a, 0x55, 0xad, 0x93, 0x32, 0x30, 0xf5, 0x8c, 0xb1, 0xe3, 0x1d, 0xf6, 0xe2, 0x2e, 0x82, 0x66, 0xca, 0x60, 0xc0, 0x29, 0x23, 0xab, 0x0d, 0x53, 0x4e, 0x6f, 0xd5, 0xdb, 0x37, 0x45, 0xde, 0xfd, 0x8e, 0x2f, 0x03, 0xff, 0x6a, 0x72, 0x6d, 0x6c, 0x5b, 0x51, 0x8d, 0x1b, 0xaf, 0x92, 0xbb, 0xdd, 0xbc, 0x7f, 0x11, 0xd9, 0x5c, 0x41, 0x1f, 0x10, 0x5a, 0xd8, 0x0a, 0xc1, 0x31, 0x88, 0xa5, 0xcd, 0x7b, 0xbd, 0x2d, 0x74, 0xd0, 0x12, 0xb8, 0xe5, 0xb4, 0xb0, 0x89, 0x69, 0x97, 0x4a, 0x0c, 0x96, 0x77, 0x7e, 0x65, 0xb9, 0xf1, 0x09, 0xc5, 0x6e, 0xc6, 0x84, 0x18, 0xf0, 0x7d, 0xec, 0x3a, 0xdc, 0x4d, 0x20, 0x79, 0xee, 0x5f, 0x3e, 0xd7, 0xcb, 0x39, 0x48 ] # 轮常数，用于密钥扩展 FK = [0xa3b1bac6, 0x56aa3350, 0x677d9197, 0xb27022dc] # 固定参数，用于轮密钥生成 CK = [ 0x00070e15, 0x1c232a31, 0x383f464d, 0x545b6269, 0x70777e85, 0x8c939aa1, 0xa8afb6bd, 0xc4cbd2d9, 0xe0e7eef5, 0xfc030a11, 0x181f262d, 0x343b4249, 0x50575e65, 0x6c737a81, 0x888f969d, 0xa4abb2b9, 0xc0c7ced5, 0xdce3eaf1, 0xf8ff060d, 0x141b2229, 0x30373e45, 0x4c535a61, 0x686f767d, 0x848b9299, 0xa0a7aeb5, 0xbcc3cad1, 0xd8dfe6ed, 0xf4fb0209, 0x10171e25, 0x2c333a41, 0x484f565d, 0x646b7279 ] def byte_sub(byte): \"\"\"S盒字节替换\"\"\" return S_BOX[byte] def l_transformation(b): \"\"\"线性变换\"\"\" return b ^ (b \u003c\u003c 2) ^ (b \u003c\u003c 10) ^ (b \u003c\u003c 18) ^ (b \u003c\u003c 24) def t_function(x): \"\"\"T函数，包括S盒替换和线性变换\"\"\" b = byte_sub((x \u003e\u003e 24) \u0026 0xFF) \u003c\u003c 24 | \\ byte_sub((x \u003e\u003e 16) \u0026 0xFF) \u003c\u003c 16 | \\ byte_sub((x \u003e\u003e 8) \u0026 0xFF) \u003c\u003c 8 | \\ byte_sub(x \u0026 0xFF) return l_transformation(b) def key_expansion(key): \"\"\"密钥扩展生成32轮密钥\"\"\" K = [key[i] ^ FK[i] for i in range(4)] rk = [] for i in range(32): temp = K[i] ^ t_function(K[i + 1] ^ K[i + 2] ^ K[i + 3] ^ CK[i]) rk.append(temp) K.append(temp) return rk def sm4_encrypt_block(plaintext, key): \"\"\"SM4单块加密\"\"\" rk = key_expansion(key) X = plaintext.copy() for i in range(32): temp = X[i] ^ t_function(X[i + 1] ^ X[i + 2] ^ X[i + 3] ^ rk[i]) X.append(temp) return X[35:31:-1] # 反序输出 def sm4_decrypt_block(ciphertext, key): \"\"\"SM4单块解密\"\"\" rk = key_expansion(key)[::-1] # 轮密钥反序 X = ciphertext.copy() for i in range(32): temp = X[i] ^ t_function(X[i + 1] ^ X[i + 2] ^ X[i + 3] ^ rk[i]) X.append(temp) return X[35:31:-1] # 反序输出 # 示例使用 plaintext = [0x01234567, 0x89abcdef, 0xfedcba98, 0x76543210] key = [0x01234567, 0x89abcdef, 0xfedcba98, 0x76543210] ciphertext = sm4_encrypt_block(plaintext, key) print(f\"encrypt: {ciphertext}\") decrypted_text = sm4_decrypt_block(ciphertext, key) print(f\"decrypt: {decrypted_text}\") ","date":"2024.11.27","objectID":"/blog/posts/reverse/algorithm/:4:1","tags":["reverse","algorithm"],"title":"逆向中的算法","uri":"/blog/posts/reverse/algorithm/"},{"categories":["config"],"content":"记录一下每次刷机和刷机之后需要的配置 ","date":"2024.11.24","objectID":"/blog/posts/config/android-phone/:0:0","tags":["config","android"],"title":"安卓手机配置","uri":"/blog/posts/config/android-phone/"},{"categories":["config"],"content":"刷机\r这里记录一下刷机的流程，都是按照我的 pixel3 手机来的。 ","date":"2024.11.24","objectID":"/blog/posts/config/android-phone/:1:0","tags":["config","android"],"title":"安卓手机配置","uri":"/blog/posts/config/android-phone/"},{"categories":["config"],"content":"解锁 OEM 和 BL\r首先是通过在设置 \u003e 系统 \u003e 关于手机多次点击版本号，知道提示进入开发者模式。之后在手机的设置 \u003e 系统 \u003e 开发者选项中找到 OEM 解锁，将其打开。 最后就是使用以下命令解锁 BL 锁，解锁之后，手机会重置。 adb devices # 先检查设备是否存在，存在则执行以下指令 adb reboot bootloader # 重启进入fastboot mode fastboot flashing unlock # 解锁 ","date":"2024.11.24","objectID":"/blog/posts/config/android-phone/:1:1","tags":["config","android"],"title":"安卓手机配置","uri":"/blog/posts/config/android-phone/"},{"categories":["config"],"content":"驱动安装\rwindows 需要进行这一步，不然可能识别不了手机。从 官网 下载驱动安装，之后进入设备管理器 \u003e 其它设备下查看相应的设备，右键更新驱动程序，选择浏览我的电脑以查找驱动程序，将下载驱动解压缩路径选中，点击下一步，安装即可 ","date":"2024.11.24","objectID":"/blog/posts/config/android-phone/:1:2","tags":["config","android"],"title":"安卓手机配置","uri":"/blog/posts/config/android-phone/"},{"categories":["config"],"content":"刷入官方镜像\r我使用的手机是 pixel3，所以直接从 google 镜像官网找 blueline 型号的镜像下载。解压压缩包之后进入目录，运行下面的命令，等待萨湖如官方镜像完成。 adb reboot bootloader # 先进入bootloader模式 ./flash-all.sh ","date":"2024.11.24","objectID":"/blog/posts/config/android-phone/:1:3","tags":["config","android"],"title":"安卓手机配置","uri":"/blog/posts/config/android-phone/"},{"categories":["config"],"content":"使用 Magisk 进行 Root\r先从 官方 下载面具，之后使用 adb install xx.apk 来进行安装。之后从上面的官方镜像的 image-blueline-xxxxxx.zip 中得到 boot.img 文件，通过 adb push boot.img /sdcard/ 上传到手机之后，使用 Magisk，点击安装 \u003e 选择并修补一个文件对该文件进行修补，之后会得到 magisk_patched-xxx_xxx.img 文件。在把这个文件通过 adb pull /sdcard/Download/magisk_patched-xxx_xxx.img 下载到电脑，再次刷入即可 root。 adb reboot bootloader # 先进入fastboot mode fastboot flash boot magisk_patched-xxx_xxx.img 只需要在 adb shell 中执行 su 命令，Magisk 弹出确认，点击允许即可获取 root。 ","date":"2024.11.24","objectID":"/blog/posts/config/android-phone/:1:4","tags":["config","android"],"title":"安卓手机配置","uri":"/blog/posts/config/android-phone/"},{"categories":["config"],"content":"软件下载\r","date":"2024.11.24","objectID":"/blog/posts/config/android-phone/:2:0","tags":["config","android"],"title":"安卓手机配置","uri":"/blog/posts/config/android-phone/"},{"categories":["config"],"content":"Magisk\r上面说了下载链接，这里就不展开述说了。面具的作用有很多，我觉得最主要的就是获取 root 权限，由此可以配合使用很多操作。 同时可以下载 zygisk，可以有更多修改能力，也可以下载 LSPosed，启动 Xposed Hook。 ","date":"2024.11.24","objectID":"/blog/posts/config/android-phone/:2:1","tags":["config","android"],"title":"安卓手机配置","uri":"/blog/posts/config/android-phone/"},{"categories":["config"],"content":"CaptiveMgr\r去除 WiFi 受限无法连接叹号的问题，自己软件的下在链接忘了，贴一个别人存的地址，应该是一样的。相关博客。 ","date":"2024.11.24","objectID":"/blog/posts/config/android-phone/:2:2","tags":["config","android"],"title":"安卓手机配置","uri":"/blog/posts/config/android-phone/"},{"categories":["config"],"content":"Termux\rTermux是一个适用于 Android 的终端模拟器，其环境类似于 Linux 环境。 无需Root或设置即可使用。 Termux 会自动进行最小安装 - 使用 APT 包管理器即可获得其他软件包。可以从 官方网址 进行下载。 Linux 中很多命令 Android 中都没有，下载又很麻烦，所以使用这个终端可以方便运行各项命令，因为可以直接使用包管理来进行软件下载。 ","date":"2024.11.24","objectID":"/blog/posts/config/android-phone/:2:3","tags":["config","android"],"title":"安卓手机配置","uri":"/blog/posts/config/android-phone/"},{"categories":["config"],"content":"使用\r","date":"2024.11.24","objectID":"/blog/posts/config/android-phone/:3:0","tags":["config","android"],"title":"安卓手机配置","uri":"/blog/posts/config/android-phone/"},{"categories":["config"],"content":"投屏\r因为存在需求要把安卓的屏幕投射到电脑上，所以下载了 晨钟酱 的 投屏控制器，虽然启动的慢，但是它可以进行黑屏启动，这样就不需要被手机的息屏给打断了。不过后续发现，过一段时间，这个也会和 QtScrcpy 一样黑屏不显示，所以还是推荐使用 QtScrcpy，毕竟它启动的快一些。 投屏控制器\r","date":"2024.11.24","objectID":"/blog/posts/config/android-phone/:3:1","tags":["config","android"],"title":"安卓手机配置","uri":"/blog/posts/config/android-phone/"},{"categories":["android"],"content":"借鉴 《程序员的自我修养》，ELF视频 和 文字记录 做的笔记。主要想要探究一下 ELF 的链接和装载。 ","date":"2024.11.24","objectID":"/blog/posts/android/elf/:0:0","tags":["reverse","android"],"title":"ELF 解析","uri":"/blog/posts/android/elf/"},{"categories":["android"],"content":"基础配置\r分析文件\r这里以 ARM-v8a 架构中的 ls 举例讲解 ELF 文件格式。 使用 /system/bin/ls 文件来进行分析，首先将其复制一份放在 /data/local/tmp 目录中，之后 pull 到本地使用 010 Editor 进行修改，然后再 push 上去运行验证。下面为了方便采用 bat 脚本执行。 adb push ls /data/local/tmp/ls adb shell \"chmod 777 /data/local/tmp/ls\" adb shell \"cd /data/local/tmp \u0026\u0026 ./ls\" 要提及一下 Toybox，它是一个小型、高效的命令工具集，为嵌入式系统和 Android 提供了常用命令的实现。同时它也是单一的可执行文件，集成了多个常用命令，如 ls、cp、mv、mkdir 等。我们使用的 ls 就是 Toybox 的软链接，它通过识别程序调用时的名称（即 argv[0]）决定执行的具体命令。所以我尝试把 ls 更改名称为 modify-ls，一开始能用，然后后面就报错 toybox: Unknown command modify-ls 了，所以还是使用原来的名称。 ","date":"2024.11.24","objectID":"/blog/posts/android/elf/:1:0","tags":["reverse","android"],"title":"ELF 解析","uri":"/blog/posts/android/elf/"},{"categories":["android"],"content":"elf_header\relf_header\r上图就是 elf_header 的结构，它包含数个结构信息。下面主要展示真正起作用的部分和相关知识点，其余没有用处的部分一笔带过，这里有没有用处的评价标准是相对于在 Linux 中正常运行而言的，不影响运行就是没有作用的。 ","date":"2024.11.24","objectID":"/blog/posts/android/elf/:2:0","tags":["reverse","android"],"title":"ELF 解析","uri":"/blog/posts/android/elf/"},{"categories":["android"],"content":"结构分析\re_ident\relf_header 下面有一个子结构体 e_ident，它里面其实只有第一个 file_identification 是有用的，并且是固定的，为 .ELF。 file_identification\r其余的属性，例如 ei_class_2_e，它是用来描述 elf 的位数的，32位和64位。但是实际上 linker 加载该 elf 文件的时候，根本不会在意这个值。如果把这个结构中除了 file_identification 全部更改为 EE（FF为 -1，可能存在作用），程序依然可以正常运行。 e_type\r这个的值虽然是一个枚举，但是是实际上无论是 exe 还是 so，它们的值都必须是 ET_DYN (3) ，如果是一个 exe 文件，将这个值改成 ET_EXEC (2)，它反而不能运行。 e_machine\r这个字段说明 CPU 平台，比如 x86，arm32，arm64 等。如果这个地方给错了，不能正常运行。 e_entry_START_ADDRESS\r这个是 elf 加载到内存时执行的初始地址，也就是程序加载后，加载器将控制权转移到的第一条指令地址，通常是 _start 函数的地址，之后这个函数会调用 libc 的启动函数 __libc_start_main，而 __libc_start_main 就会调用用户定义的 main 函数。这里它是一个虚拟地址，表示的相对偏移，真实的虚拟地址 = 加载的虚拟地址的基址 + 这个相对偏移。 PROGRAM_HEADER_OFFSET\r指 e_phoff_PROGRAM_HEADER_OFFSET_IN_FILE，它是 program_header_table 段的偏移。elf_header 后面就是 program_header_table，斯普哦它一般也是 elf_header 的大小。除非有人故意在这两者之间插入一些无用的数据。 SECTION_HEADER_OFFSET\r指 e_shoff_SECTION_HEADER_OFFSET_IN_FILE，它表示 section header table 段的偏移，但是没有什么用处，实际上跟 section 有关的都没啥用，因为 elf 加载的时候根本就不使用 section 相关的东西。上面是 Android 8 之前的版本，而 Android 8 及以后还是会读取 section header的，但不是所有的 section 都会读取。但是我使用 Android 9 进行实验，SDK 为 28，我发现按照之前的修改方式进行，程序还是可以运行。这里是 Android14 的 linker 代码，可以看到它存在 ReadSectionHeaders()、ReadDynamicSection() 函数来读取相关信息。 修改 section header table\r但是 IDA 是使用 section 来进行解析的。我们将 section header 都覆盖为 EE。之后可以正常运行，但是 IDA 解析一开始会报错，之后发现解析不了段信息。实际上加载一个 so 文件的时候，IDA 的 segment view 里面就是解析的 section header。如果我们破坏了甚至是弄一个假的 .text/.data section，那么 IDA 就没有办法正常解析了。 IDA 解析\r","date":"2024.11.24","objectID":"/blog/posts/android/elf/:2:1","tags":["reverse","android"],"title":"ELF 解析","uri":"/blog/posts/android/elf/"},{"categories":["android"],"content":"其余信息\re_ehsize_ELF_HEADER_SIZE：没有用处，加载的时候根本不检查，它是根据位数的默认大小，所以改动 elf_header 的大小会有不可预料的后果 e_phentsize_PROGRAM_HEADER_ENTRY_SIZE_IN_FILE：program_header_table 是一个数组，这个表示数组中每个元素的大小 e_phnum_NUMBER_OF_PROGRAM_HEADER_ENTRIES：这个表示上面数组中元素的个数 ","date":"2024.11.24","objectID":"/blog/posts/android/elf/:2:2","tags":["reverse","android"],"title":"ELF 解析","uri":"/blog/posts/android/elf/"},{"categories":["android"],"content":"总结\r所以最后可以把 elf_header 修改成下面的形式，它也是可以正常运行的。下面保留的相关结构就是 file_identification、e_type、e_machine、e_entry_START_ADDRESS、e_phoff_PROGRAM_HEADER_OFFSET_IN_FILE、e_phentsize_PROGRAM_HEADER_ENTRY_SIZE_IN_FILE、e_phnum_NUMBER_OF_PROGRAM_HEADER_ENTRIES。 修改后ELF\r","date":"2024.11.24","objectID":"/blog/posts/android/elf/:2:3","tags":["reverse","android"],"title":"ELF 解析","uri":"/blog/posts/android/elf/"},{"categories":["android"],"content":"program_header_table\r这是一个数组，拥有数个元素结构体 program_table_element，元素个数和元素大小都在 elf_header 中展示出来了。这里是对单个元素结构进行分析，它每一个元素结构描述的都是内存和文件的对应关系。 program_header_table\r","date":"2024.11.24","objectID":"/blog/posts/android/elf/:3:0","tags":["reverse","android"],"title":"ELF 解析","uri":"/blog/posts/android/elf/"},{"categories":["android"],"content":"结构分析\rp_type\r段类型，为 1 表示可加载段，然后根据读写权限可以分成代码段和数据段。这两个可加载段最为重要，其它段就是服务这两个段而存在的。 p_flags\r段属性。表示段为可读，可写，可执行等。对于加载段很重要，区分了代码段和数据段，其它段就基本没什么意义。 FROM_FILE_BEGIN\r指 p_offset_FROM_FILE_BEGIN，它表示段在文件中的偏移，可以直接在文件中根据偏移进行查看。 VIRTUAL_ADDRESS\r指 p_vaddr_VIRTUAL_ADDRESS，它表示段的虚拟地址。它表示的是一个相对偏移，因为段被加载到的虚拟地址是不确定的，所以真实的虚拟地址 = 加载的虚拟地址的基址 + 这个相对偏移 PHYSICAL_ADDRESS\r指 p_paddr_PHYSICAL_ADDRESS，它表示段的物理地址。因为都运行在用户态，没有物理地址的概念，所以所有段的这个都没有用。段只有虚拟地址才有意义。 SEGMENT_FILE_LENGTH\r指 p_filesz_SEGMENT_FILE_LENGTH，表示段在文件中的长度。 SEGMENT_RAM_LENGTH\r指 p_memsz_SEGMENT_RAM_LENGTH，表示段在内存中的长度。 p_align\r段的对齐方式。ELF 中的对齐都是内存对齐，不存在文件对齐，它都是密集排列的。它没有什么作用，加载器中写定了内存对齐都是 4K。 ","date":"2024.11.24","objectID":"/blog/posts/android/elf/:3:1","tags":["reverse","android"],"title":"ELF 解析","uri":"/blog/posts/android/elf/"},{"categories":["android"],"content":"文件内存映射\rprogram_header_table 中的每个元素，描述的其实是将段加载到内存中时，elf文件中的段映射到了内存中。 p_offset_FROM_FILE_BEGIN 与 p_filesz_SEGMENT_FILE_LENGTH 表示了文件中的段。 p_vaddr_VIRTUAL_ADDRESS 与 p_memsz_SEGMENT_RAM_LENGTH 表示了内存中的段。 这两者构成一个映射关系，linker 在加载 elf 的时候采用的是 mmap。p_filesz_SEGMENT_FILE_LENGTH 与 p_memsz_SEGMENT_RAM_LENGTH 的大小不一定一样，因为为了节省 elf 文件大小，有些值为 0 的段，比如 .bss 就不占文件空间。但是加载到内存后，还是要分配空间的。 ","date":"2024.11.24","objectID":"/blog/posts/android/elf/:3:2","tags":["reverse","android"],"title":"ELF 解析","uri":"/blog/posts/android/elf/"},{"categories":["android"],"content":"Program Header\r这个段里只有两块数据有用，其余都可以忽略。 Program Header\r文件偏移与虚拟地址，它们都是 0x40，而且这个值与 elf_header 必须是相等的。这个值某种意义上就是 elf_header 的大小。那为什么还需要在这里再储存一次呢？ 可以简单理解，linker 里面有一些指针指向的是 elf_header，而有些指针指向的是 program_header，互相转换的时候，会使用指针偏移来计算，偏移大小就是这里的 0x40，所以就在这里也记录了值，偏于指针计算。 p_data 就是整个 program_header_table 的内容。 p_type 不知道有没有用，没试过改这里，假定有用吧。 ","date":"2024.11.24","objectID":"/blog/posts/android/elf/:3:3","tags":["reverse","android"],"title":"ELF 解析","uri":"/blog/posts/android/elf/"},{"categories":["android"],"content":"Interpreter Path\r这个段就是描述解析器（linker）路径的，它与上面的 Program Header 段必须要出现在可加载段前面。 Interpreter Path\r这个段需要关注的数据就是 p_data，可以发现它是一个字符串 /system/bin/linker64，那么 LENGTH 就都是指的容纳这个字符串的长度，在这里也就是最多 0x15 长度的字符串。ELF 的可加载段就是由这个路径文件来加载的，包括重定位操作等等。 我们可以根据这个路径获取系统的 linker，之后在入口点写入死循环，然后更改 ls 文件的解析器路径为我们修改的 linker 的路径。这里因为字符串的长度不能超过 0x15，同时需要以 00 字节结尾，所以更改 linker64 为 li64。 文件修改\r之后运行 ls 文件，发现卡死了，所以查看其 maps 信息。这里发现相比之前的死循环，这里少了很多东西，像是 libc 相关的东西也没有了。这是因为现在进程卡在了非常早的时机，连这个 ELF 文件自身的段的重定位都没有做，只是光将 ELF 文件中的东西放到内存里面了。 但是可以发现下图前面两行表示 ls 文件被加载了，这又是为什么？我们已经在 linker 的入口函数加了一个死循环，linker 根本不会执行加载逻辑才对。其实这是因为这两个可执行段不是由 ls 中指定的 linker 加载的，而是其他进程中的 linker 加载的，目前先简单的理解为是 init 进程干的吧（感觉需要更为细致的判断，这里的理解是猜测）。剩下的其他 ELF 依赖文件，比如 libc 等则将由路径中指定的 linker 来加载。 修改后 maps 信息\r我们可以使用 IDA 附加进程来更为细致的查看。在 Modules 中可以发现只有两个 so 被加载。 IDA Modules 信息\r之后还原 linker 的入口地址指令，让其继续执行，并且在 ls 的入口地址加上断点。按 F8 步过 linker_init 函数，发现此时 linker 加载了很多的 so。同时这里的 linker 存在一个特点，他没有 imports（导入表）。这是因为它不依赖其他 so，它也不能依赖其他 so。它运行的时候，其他 so 都没有加载。比如 open 函数，它就不能使用 libc 中的 open 函数，它必须自己实现 open 函数。所以它必须实现所有自己需要的函数。 IDA Modules 信息\r","date":"2024.11.24","objectID":"/blog/posts/android/elf/:3:4","tags":["reverse","android"],"title":"ELF 解析","uri":"/blog/posts/android/elf/"},{"categories":["android"],"content":"Android9 可加载段\r重要说明\r这里加载段和上面的内容都是对于我 Android9 的设备来看，与其它 Android 的版本存在差异。 这里最为重要的就是下面的两个可加载段了，根据其权限可以判断出，具有 R_X 属性的段为代码段，具有 RW_ 属性的段为数据段。 可加载段\r代码段\r之前根据 elf_header 的 e_entry_START_ADDRESS 值，我们可以知道代码的入口在虚拟地址 0xD760。对于程序在内存运行，段的加载就是根据它的虚拟地址和内存长度来进行排列的。这里代码段的虚拟地址为 0x0，可以判断它是排在第一个的。同时代码入口和段的真实虚拟地址都是 加载的虚拟地址的基址 + 相对偏移 得来的，它们的加载虚拟地址基址都是一样的，同时段的大小为 0x6041C，比 0xD760 大，所以代码的入口在代码段中。 代码段信息\r同时代码段的文件大小和内存大小相同，且段在文件中的偏移也是 0，所以可以判断它们为 1对1 映射。所以该入口地址在文件中也是在代码段中的（0x0 + 0xD760），我们可以直接查找 0xD760 来找到入口的代码。 修改前首地址\r之后修改为 00 00 00 14 ，它在 ARM64 中表示 b #0，是死循环代码，这样可以帮助我们查看内存中的段分布。 修改后首地址\r之后启动文件，通过 ps -a 查看进程信息，之后通过 proc/self/maps 查看内存中段信息。 内存段信息\r根据上图可以看到段在内存中的分布，这里第一个可读可执行的就是代码段，而后面三个一起就是数据段。这里代码段的起始位置 0x60a053e000 就是加载的虚拟地址的基址，因为代码段的相对偏移为 0。之后我们得到代码段的大小为 0x6041C，二者相加等于 0x60A059E41C，然后是 4K 对齐，结束地址为 0x60A059F000。同时我们用户的起始代码地址为 0x60a053e000 + 0xD760 = 0x60A054B760。在下图 IDA 的分析中，可以看到起始地址代码刚好为我们更改的结果。 IDA 中起始地址\r在 IDA 的分析中，这里的代码在名为 .text 的 segment 中，它是 IDA 根据 section 的信息分析的，在 section 中，.text的起始位置为 0xD740，刚好和这里的图片相对应。由此也可以验证 IDA 的分析是根据 section 来进行的，但是运行是根据 program 来进行的。 IDA 代码段信息\r数据段\r而后面三个一起为数据段，我们查看数据段信息，可以知道它的虚拟地址为 0x7CFC0，而基址为 0x60a053e000，二者相加为 0x60A05BAFC0。因为内存页都是 4K 对齐的，所以不能从这个地址开始给其赋予可读可写的权限，必须是给整个内存页赋予权限，所以这里就从 0x60a05ba000 开始赋予权限，但是真实的地址还是 0x60A05BAFC0。 同时数据段的大小为 0x89EE，0x60a05ba000 + 0x89EE = 0x60A05C29EE，根据 4kb 对齐就是 0x60A05C3000。这个地方应该和三部分的末尾相对应，但是我这里差了 1kb 的空间，每次都是这样，怀疑我这个系统往这个里面添加了什么数据，但是目前不知道。所以这里就当作它是刚好映射到结尾吧。 数据段信息\r这里从 IDA 的 Segments View 中可以看出，这个地址对应的 segment 就是 .preinit_array，也就是说这个数据段从这里往后进行放置。我们可以根据该段的文件偏移找到这个地方的代码，可以发现二者是相对应的。 IDA 数据段信息\r但是问题还是在这里，为什么这个数据段被分成了三部分？这是因为数据段中的一部分权限会被修改，它是被下面的 GNU Read-only After Relocation 段给修改的。这个段描述的就是，在重定位之后，将虚拟位置 0x7CFC0 且大小为 0x3040 字节的区域的权限改为只读。这里 0x7CFC0 就是数据段的起始位置，之后计算数据段的第一部分大小， 0x60a05be000 - 0x60a05ba000 = 0x4000，它刚好是 0x3040 4kb 对齐的结果，这说明第一部分就是这个段修改权限所产生的。 GNU Read-only After Relocation\r而剩余的两部分也都是数据段的内容，它们都是可读可写的，第一个后面存在文件路径，而后面那个没有。这是因为这个数据段的文件大小小于映射后的 RAM 大小，所以 RAM 超出文件的部分就没有文件映射了，也就没有文件路径了。 ","date":"2024.11.24","objectID":"/blog/posts/android/elf/:3:5","tags":["reverse","android"],"title":"ELF 解析","uri":"/blog/posts/android/elf/"},{"categories":["compile"],"content":"这里记录符号表、语义属性相关知识。 ","date":"2024.11.21","objectID":"/blog/posts/compile/semantics-symboltable-ag/:0:0","tags":["compile"],"title":"03 Semantics SymbolTable Ag","uri":"/blog/posts/compile/semantics-symboltable-ag/"},{"categories":["compile"],"content":"Symbol Table\r符号主要指变量名、函数名、类型名、标签名。在语义分析中需要进行符号检查，即检查程序是否会出现符号乱用的情况，例如前一句将符号 symbol 定义为变量，之后又把它当作函数来进行函数调用了。而符号表就是用于保存各种符号相关信息的数据结构，它不仅在前端语法分析之后会发挥作用，还会在后端的生成中间代码的过程进行使用。下面就是一个符号表的示例。 符号表\r但是对于符号检查而言，最为困难的就是作用域了。“领域特定语言”（DSL），例如简单的键值对，它们通常只有单作用域（全局作用域），而对于我们真正要分析的“通用程序设计语言”（GPL），它通常就需要嵌套作用域了，这时就需要不同的哈希表来进行符号存储了，由此每个符号表代表了一个作用域，不同的作用域需要通过树结构来维护。 多作用域符号表\r下面就是每个作用域需要提供的接口： 作用域接口\r符号表相关的类层次结构设计如下，它是 Lab3 的一部分，表示了符号表的设计。 符号表类层次结构\r","date":"2024.11.21","objectID":"/blog/posts/compile/semantics-symboltable-ag/:1:0","tags":["compile"],"title":"03 Semantics SymbolTable Ag","uri":"/blog/posts/compile/semantics-symboltable-ag/"},{"categories":["compile"],"content":"语义分析\r这里讲述的就是属性文法，它指的就是为上下文无关文法赋予语义，分析的就是如何基于上下文无关文法做上下文相关分析。 因此得出在语法分析过程中实现属性文法，就是通过在推导过程中嵌入语义动作，例如 $B \\rightarrow X \\{ \\textcolor{red}{a} \\} Y$。语义动作嵌入的位置决定了何时执行该动作，基本思想就是一个动作在它左边的所有文法符号都处理过之后立刻执行。在 antlr4 中，使用参数的形式来表示继承属性，使用返回值来表示综合属性。 ","date":"2024.11.21","objectID":"/blog/posts/compile/semantics-symboltable-ag/:2:0","tags":["compile"],"title":"03 Semantics SymbolTable Ag","uri":"/blog/posts/compile/semantics-symboltable-ag/"},{"categories":["compile"],"content":"语法制导定义\rSDD（语法制导定义，Syntax-Directed Definition）是一个上下文无关文法的属性及规则的结合。 每个文法符号都可以关联多个属性 每个产生式都可以关联一组规则 SDD 唯一确定了语法分析树上每个非终结符节点的属性值，但它没有规定以什么方式、什么顺序计算这些属性值。顺序是根据不同的语法分析器的情况而言的，比如 antlr4 是深度优先遍历，那么它呃顺序也就是这个了。 antlr 遍历\rS 属性定义\r综合属性和 S 属性\r属性依赖关系\r由上图可以看出 S 属性的含义。本质而言，他就是父节点的信息需要依赖于子节点的传递，只有子节点首先处理完得到了结果，才能传递给父节点进行使用。 S 属性性质\rL 属性定义\r集成属性\r由上图，$T^{’}$ 有一个综合属性 syn 与一个继承属性 inh。这里继承属性 $T^{’}.inh$ 用于在表达式中从左向右传递中间计算结果。 L 属性\r上述的定义就是排除依赖右兄弟节点的情况，因为这种情况对于深度优先建立的语法分析树而言是不可能实现的。 例子\r属性文法计算后缀表达式\r后缀表示\r后缀表达 S 属性\r这里是子节点向父节点传递信息，所以使用综合属性。 数组类型文法举例\r综合信息的流向是从下向上传递信息，继承信息流向是从左向右、从上到下传递信息。所以为了把二者进行结合，那么就先通过继承属性从左向右、从上到下传递信息，再通过综合属性从下向上传递信息。 数组类型\r信息传递\r代码实现\r上图绿色为继承属性，把 int 从左往右，从上往下传递；红色为综合属性，把类型信息从下往上传递。 属性文法计算有符号二进制数\r有符号二进制数\rL 属性定义\r语法树传递信息\r","date":"2024.11.21","objectID":"/blog/posts/compile/semantics-symboltable-ag/:2:1","tags":["compile"],"title":"03 Semantics SymbolTable Ag","uri":"/blog/posts/compile/semantics-symboltable-ag/"},{"categories":["compile"],"content":"语法制导的翻译方案\rSDT\rSDD 只是一个理念，我们之后的例子都是进行 SDT 的操作，也就是真实实现。 ","date":"2024.11.21","objectID":"/blog/posts/compile/semantics-symboltable-ag/:2:2","tags":["compile"],"title":"03 Semantics SymbolTable Ag","uri":"/blog/posts/compile/semantics-symboltable-ag/"},{"categories":["config"],"content":"记录一下 IDA 的使用。 ","date":"2024.11.20","objectID":"/blog/posts/config/ida/:0:0","tags":["config"],"title":"IDA 配置和使用","uri":"/blog/posts/config/ida/"},{"categories":["config"],"content":"下载\r这里贴一个 IDA9.0rc1 的 来源，虽然不是从这里获取的资源，但是想来应该都差不多。之后就是使用评论区给出的 keygen2.py 文件进行 patch 和生成 License，也就是 idapro.hexlic 文件，这样就可以正常使用了。 ","date":"2024.11.20","objectID":"/blog/posts/config/ida/:1:0","tags":["config"],"title":"IDA 配置和使用","uri":"/blog/posts/config/ida/"},{"categories":["config"],"content":"配置\r","date":"2024.11.20","objectID":"/blog/posts/config/ida/:2:0","tags":["config"],"title":"IDA 配置和使用","uri":"/blog/posts/config/ida/"},{"categories":["config"],"content":"修改 python 版本\r想让 IDA 使用自己的 python，那么就需要进行相关配置，这里直接使用 idapyswitch.exe 进行切换即可，下面的命令就会更改 IDA 使用的 python 版本。我这里使用的是 miniconda 的虚拟环境，但是都是一样的，只要找到 python3.dll 就行。 .\\idapyswitch.exe --force-path E:\\xxxxx\\miniconda3\\envs\\re\\python3.dll ","date":"2024.11.20","objectID":"/blog/posts/config/ida/:2:1","tags":["config"],"title":"IDA 配置和使用","uri":"/blog/posts/config/ida/"},{"categories":["config"],"content":"插件\rIDA 有很多好用的插件，但是按照之前 beta 的版本，有一些会产生冲突，所以这里记录目前可以使用或者被修改过的插件。 ","date":"2024.11.20","objectID":"/blog/posts/config/ida/:3:0","tags":["config"],"title":"IDA 配置和使用","uri":"/blog/posts/config/ida/"},{"categories":["config"],"content":"keypatch\r从 修改仓库 获取的，直接把 keypatch.py 放在 plugins 目录中就可以使用了。具体使用可以看链接的 README 文件 ","date":"2024.11.20","objectID":"/blog/posts/config/ida/:3:1","tags":["config"],"title":"IDA 配置和使用","uri":"/blog/posts/config/ida/"},{"categories":["config"],"content":"lazyida\r从 官方仓库 获取的，作者修改了错误的地方，配置方式和上面一样，具体使用也是看 README。 ","date":"2024.11.20","objectID":"/blog/posts/config/ida/:3:2","tags":["config"],"title":"IDA 配置和使用","uri":"/blog/posts/config/ida/"},{"categories":["config"],"content":"D810\r从 官方仓库 获取的，反正最开始的时候没有报错，当作可以正常使用 🐶。之后使用时选择适当的规则，然后点击 start，就可以按 F5 自动反编译，解决 OLLVM 混淆。如果已经存在 F5 缓存，可以将一段代码 nop 掉，之后撤销操作，再 F5 反编译即可 D810使用\r","date":"2024.11.20","objectID":"/blog/posts/config/ida/:3:3","tags":["config"],"title":"IDA 配置和使用","uri":"/blog/posts/config/ida/"},{"categories":["config"],"content":"findcrypt-yara\r从 官方仓库 获取之后，把 findcrypt3.py 和 findcrypt3.rules 放在 plugins 目录中。之后因为 yara-python 版本的问题，需要进行 修正，直接将下面的代码覆盖原来的即可。同时也可以给 rules 添加国密SM4算法的识别规则。 def yarasearch(self, memory, offsets, rules): print(\"\u003e\u003e\u003e start yara search\") values = list() matches = rules.match(data=memory) for match in matches: for stringR in match.strings: name = match.rule for string in stringR.instances: if name.endswith(\"_API\"): try: name = name + \"_\" + idc.GetString(self.toVirtualAddress(string.offset, offsets)) except: pass value = [ self.toVirtualAddress(string.offset, offsets), match.namespace, name + \"_\" + hex(self.toVirtualAddress(string.offset, offsets)).lstrip(\"0x\").rstrip(\"L\").upper(), stringR.identifier, repr(string.matched_data) ] idaapi.set_name(value[0], name + \"_\" + hex(self.toVirtualAddress(string.offset, offsets)).lstrip(\"0x\").rstrip(\"L\").upper() , 0) values.append(value) print(\"\u003c\u003c\u003c end yara search\") return values ","date":"2024.11.20","objectID":"/blog/posts/config/ida/:3:4","tags":["config"],"title":"IDA 配置和使用","uri":"/blog/posts/config/ida/"},{"categories":["config"],"content":"ipyida\r可以在 IDA 中配置 ipython 来执行 python 代码。直接从 官方仓库 执行命令下载即可。然后使用 Shift + . 就可以打开进行使用了，注意 exit 的使用，它会直接退出 IDA，然后分析文件会遗留，但是是分散的格式。 ","date":"2024.11.20","objectID":"/blog/posts/config/ida/:3:5","tags":["config"],"title":"IDA 配置和使用","uri":"/blog/posts/config/ida/"},{"categories":["config"],"content":"bindiff\r直接去 官网 下载 bindiff，我这里下载的是 bindiff 8。然后在它的 Plugins 目录下存在 IDA 的插件，我这里是找到 IDA-9.0-rc1 的 适配 版本，直接放到 IDA 的 plugins 目录下，然后点击 File 就可以看到 bindiff 并使用了。 ","date":"2024.11.20","objectID":"/blog/posts/config/ida/:3:6","tags":["config"],"title":"IDA 配置和使用","uri":"/blog/posts/config/ida/"},{"categories":["config"],"content":"使用\r可以查看别人的 博客1、博客2、博客3 来学习具体的使用方法和小技巧。 ","date":"2024.11.20","objectID":"/blog/posts/config/ida/:4:0","tags":["config"],"title":"IDA 配置和使用","uri":"/blog/posts/config/ida/"},{"categories":["config"],"content":"日常使用\r这里补一下 IDA 官方快捷方式，这个更为全面。 主要操作\rctrl + e：找到main函数 Shift + f12：可以打开 string 窗口，一键找出所有的字符串，右击 setup，对窗口的属性进行设置。同时附加时使用可以显示 strings Shift + f7：可以查看 Segments 窗口。查看不同的段 空格：在 Text View 和 Group View 中来回切换 f5/Tab：一键反汇编，Tab 可以在汇编界面与伪代码界面来回切换 Ctrl + X：交叉引用 Alt + T：在汇编界面中搜索汇编语言 Shift + E：提取数据 Ctrl + Shift + W：保存快照，它会生成一个新的 ida 数据库文件，本质为另存为 插件操作\rCtrl + Alt + K：(Keypatch快捷键) 进行patch Shift + .：打开 ipyida 插件 类型更改\rD 转换为原始数据 C 转换为汇编代码 P 重新生成函数 a 将数据转换为字符串，主要可以应对小端序存储 N 更改变量的名称 Y 更改变量的类型，比如把 _int64 更正为 BYTE*（或者 char *） U undefine，取消定义函数、代码、数据的定义，转化为原始字节的形式 V 简化函数的格式，有时候函数没有 return 时可以使用，查看更方便 M 枚举所有相同的数据 ; 在反汇编后的界面中写下注释 / 在反编译后伪代码的界面中写下注释 \\ 在反编译后伪代码的界面中隐藏/显示变量和函数的类型描述 有时候变量特别多的时候隐藏掉类型描述看起来会轻松很多 右键点击 Hide casts 也可以隐藏类似 *(DWORD) 的类型描述 动态调试\r快捷键\rF2 增加断点 F7 单步步入，遇到函数，将进入函数代码内部 F8 单步步过，执行下一条指令，不进入函数代码内部 F4 运行到光标处（断点处） F9 继续运行 Ctrl + F2 终止一个正在运行的调试进程，重新开始调试 Ctrl + F7 自动步入，在所有的函数调用中一条一条地执行命令，断点或异常时，自动停止 Ctrl + F8 自动步过，一条一条的执行命令，程序到达断点，或者发生异常时，自动步过过程都会停止 附加\r应对一些强壳，可以先启动 .exe 程序，之后使用IDA的附加功能 (Debugger-\u003eattach)，附加进程，可以越过壳。之后可以使用 Shift + f12 和 Shift + f7 定位关键字符位置和段属性，将该程序的 Code 段使用 IDAPYTHON 转化为反汇编形式进行动调。 from ida_ua import * cur_addr = 0x401000 #起始地址 end_addr = 0x410000 #终止地址 def make_insn(start,end): adr = start out_ins = insn_t() while True: if(adr \u003e= end): break create_insn(adr) size = decode_insn(out_ins,adr) adr += size print(\"end!\") make_insn(cur_addr,end_addr) print(\"Done\") 可能 Code 段很大，编译很慢，可以结合手动按c反汇编结合查看 ELF 文件\r以下就是注意事项，具体流程直接上网查找即可。 Linux开启远程连接服务，在虚拟机中打开 IDA 在 Linux 中的调试工具 首先需要将文件提权，否则不能运行，也就不能调试了 IDA连接虚拟机，开始动调 Linux进行附加时，需要先打开 linux_server 服务，然后另起端口打开运行的程序，之后就可以附加了。这里需要先使用 sudo vim /etc/sysctl.d/10-ptrace.conf 更改最后一行 kernel.yama.ptrace_scope = 0，重启系统后，普通用户就可以使用 attach ID 连接程序调试了。 注意 wsl 的 Hosrname 可以设置为 127.0.0.1，有时候设置成 wsl 的 ip 不太起效果 为了方便 IDA 中的 application，可以使用 realpath ./file 直接获取文件的路径 ","date":"2024.11.20","objectID":"/blog/posts/config/ida/:4:1","tags":["config"],"title":"IDA 配置和使用","uri":"/blog/posts/config/ida/"},{"categories":["config"],"content":"idapython\ridapython 可以对相关数据进行操作，学习可以参照下面的文章。 IDA Python 使用总结 ","date":"2024.11.20","objectID":"/blog/posts/config/ida/:4:2","tags":["config"],"title":"IDA 配置和使用","uri":"/blog/posts/config/ida/"},{"categories":["config"],"content":"相关技巧\r从IDA中获取数据时，如果在分析程序中发现数据的排列为 qword 等，建议不要以小端序转化，而是直接从 IDA 的 IDA View-RIP 界面复制，不用 Ctrl + e 提取数据。除此之外， qword 的转化可能会出现一些多余数据，记得识别 这里补充一点：若是在 IDA View-RIP 界面中的数据不是正规数据（这里指十六进制），则采用 Ctrl + e 进行十六进制提取，再分别以4个字节为一组，然后倒转称为小端序的顺序进行解密操作 总而言之 可以首先使用 Ctrl + e 进行提取，然后手动进行倒转转换 也可以使用 D 进行打乱，再分别使用 D 聚合成每4个字节为一组的形式 也可以使用插件 lazyida，在数据上 右键 -\u003e convert -\u003e Covern to …… DWORD list（这里注意看前面的标识进行相依字节长度的转化）就可以在下方output框看见正确的小端序数组 Patch 函数的时候，可以直接使用汇编。然后另存为文件即可跳过函数 mov eax,0x1 retn x ;这里的 x 需要根据函数末尾的返回来抄写，防止栈不平衡 遇到 (_BYTE *)\u0026qword_4058 之类的，若是知道这是表示的数组，那么可以再汇编界面按 D 变成数据，之后 F5 重新生成伪代码，则可以看到数组变成 byte_4058[] 之类的数组形式 若是函数格式中有 (_BYTE) 等干扰分析时，可以： 使用 Y 更改变量的类型，比如把 _int64 更正为 BYTE*或者 (char *) 使用 V 简化函数的格式，有时候函数没有 return 时可以使用，这样看更方便 IDA在识别花指令时，很可能在一个连续的函数中显示红色的 sub_3D9 endp ; sp-analysis failed 类似的信息，这个时候可以使用 Edit -\u003e Function -\u003e Delete function 删除函数定义，然后在正确的位置 retn 使用 Edit -\u003e Function -\u003e Set function end 设置函数结尾。之后 F5 反编译可以看到正常的函数 有时候 U + P 不能生成函数，可以先删除函数定义，选中函数块之后按 P 定义函数 ","date":"2024.11.20","objectID":"/blog/posts/config/ida/:4:3","tags":["config"],"title":"IDA 配置和使用","uri":"/blog/posts/config/ida/"},{"categories":["compile"],"content":"这里记录上下文无关文法、LL、LR 算法相关知识。 ","date":"2024.11.14","objectID":"/blog/posts/compile/parser-cfg-ll-lr/:0:0","tags":["compile"],"title":"02 Parser Cfg LL LR","uri":"/blog/posts/compile/parser-cfg-ll-lr/"},{"categories":["compile"],"content":"CFG\r","date":"2024.11.14","objectID":"/blog/posts/compile/parser-cfg-ll-lr/:1:0","tags":["compile"],"title":"02 Parser Cfg LL LR","uri":"/blog/posts/compile/parser-cfg-ll-lr/"},{"categories":["compile"],"content":"语法\rDefinition(Context-Free Grammar, 上下文无关文法)，上下文无关文法 G 是一个四元组 G = (T, N, S, P)： T 是 终结符号（Terminal）集合，对应于词法分析器产生的词法单元 N 是 非终结符号（Non-terminal）集合 S 是 开始（Start）符号（$S \\in N $且唯一） P 是 产生式（Production）集合 $$ A \\in N \\rightarrow \\alpha \\in (T \\cup N)^* $$ 头部/左部（Head）A：单个非终结符 体部/右部（Body）$\\alpha$：终结符与非终结符构成的串，也可以是空串$\\epsilon$ ","date":"2024.11.14","objectID":"/blog/posts/compile/parser-cfg-ll-lr/:1:1","tags":["compile"],"title":"02 Parser Cfg LL LR","uri":"/blog/posts/compile/parser-cfg-ll-lr/"},{"categories":["compile"],"content":"语义\r上下文无关文法 G 定义了一个语言 L(G)。语言是串的集合，从文法得到串的过程就是推导（Derivation）。推导就是将某个产生式的左边替换成它的右边，每一步推导需要选择替换哪个非终结符号，以及使用哪个产生式。对于下面的推导式而言，E 就是非终结符，id 就是终结符，目的就是从左边推到为右边，得到只包含终结符的式子。 $$ E \\rightarrow E + E \\mid E * E \\mid (E) \\mid -E \\mid \\text{id} $$ 对于推导也存在区分，如果一直选择最左边的非终结符进行推导，就称为 Leftmost Derivation，如下所示： $$ E \\implies -E \\implies -(E) \\implies -(E + E) \\implies \\pmb{-(\\text{id} + E)} \\implies -(\\text{id} + \\text{id}) $$ 如果一直选择最右边的非终结符进行推导，就称为 Rightmost Derivation，如下所示 $$ E \\implies -E \\implies -(E) \\implies -(E + E) \\implies \\pmb{-(E + \\text{id})} \\implies -(\\text{id} + \\text{id}) $$ 由上述推导规则可以得到相关简单表示： $$ \\begin{align} E \u0026\\implies -E \\text{ : 经过一步推导得出} \\\\ E \u0026\\xRightarrow{\\text{+}} -(\\text{id} + E) \\text{ : 经过一步或多步推导得出} \\\\ E \u0026\\xRightarrow{\\text{*}} -(\\text{id} + E) \\text{ : 经过零步或多步推导得出} \\end{align} $$ 在推导的过程中，除了最左边的 program 和最后边的 文法写的程序，中间产物都被称为句型（Sentential Form），即 如果 $S \\xRightarrow{*} \\alpha$，且 $\\alpha \\in (T \\cup N)^*$，则称 $\\alpha$ 是文法 G 的一个句型。而最右边的结果被称为句子（Sentence），即 如果 $S \\xRightarrow{*} w$，且 $w \\in T^*$，则称 $w$ 是文法 G 的一个句子。 那么此时就可以定义文法 G 生成的语言 L(G) 了，即 文法 G 的语言 L(G) 是它能推导出的所有句子构成的集合 $L(G) = { w \\mid S \\xRightarrow{*} w }$。 ","date":"2024.11.14","objectID":"/blog/posts/compile/parser-cfg-ll-lr/:1:2","tags":["compile"],"title":"02 Parser Cfg LL LR","uri":"/blog/posts/compile/parser-cfg-ll-lr/"},{"categories":["compile"],"content":"LL(1) 语法分析算法\r自顶向下的、递归下降的、基于预测分析表的、适用于 LL(1) 文法的 LL(1) 语法分析器。 ","date":"2024.11.14","objectID":"/blog/posts/compile/parser-cfg-ll-lr/:2:0","tags":["compile"],"title":"02 Parser Cfg LL LR","uri":"/blog/posts/compile/parser-cfg-ll-lr/"},{"categories":["compile"],"content":"自顶向下\r这里指语法分析树从上往下进行构建，根节点是文法的起始符号 S，叶节点是词法单元流 w$，仅包含终结符号与特殊的文件结束符 $(EOF)，中间节点表示对某个非终结符应用某个产生式进行推导。那么这里的问题就是选择哪个非终结符，以及选择哪个产生式。这里对于 LL(1) 而言，第一个 L 就是表示从左向右读入词法单元；第二个 L 表示在推导的每一步，LL(1) 总是选择最左边的非终结符进行展开。即构建最左推导；1 表示只需向前看一个输入符号便可确定使用哪条产生式。 ","date":"2024.11.14","objectID":"/blog/posts/compile/parser-cfg-ll-lr/:2:1","tags":["compile"],"title":"02 Parser Cfg LL LR","uri":"/blog/posts/compile/parser-cfg-ll-lr/"},{"categories":["compile"],"content":"递归下降\r这里指实现方式，就是为每个非终结符写一个递归函数，内部按需调用其它非终结符对应的递归函数，下降一层。 递归下降的典型实现框架\r递归下降过程\r","date":"2024.11.14","objectID":"/blog/posts/compile/parser-cfg-ll-lr/:2:2","tags":["compile"],"title":"02 Parser Cfg LL LR","uri":"/blog/posts/compile/parser-cfg-ll-lr/"},{"categories":["compile"],"content":"基于预测分析表\r设计预测分析表就是源于上述递归下降过程的一个问题，在上图展开非终结符 S 的过程中，为什么前两次玄策了 $S \\implies (S + F)$，而第三次选择了 $S \\implies F$？这里就是因为它们面对的当前词法单元不同。由此根据不同的词法单元，形成了一张预测分析表，之后就可以使用预测分析表来确定产生式。 预测分析表\r这里指明了每个非终结符在面对不同的词法单元或文件结束符时，该选择哪个产生式（按编号进行索引）或者报错（空单元格）。下面就是递归下降、基于预测分析表的实现方法，这里根据预测分析表，从左往右逐个字符进行匹配。 实现方法\r那么如何得到这个预测分析表呢，就需要先知道两个概念。 FIRST 集合\r$FIRST(\\alpha)$ 是可从 $\\alpha$ 推导得到的句型的首终结符号的集合。即对于任意的（产生式的右部）$\\alpha \\in (N \\cup T)^*$： $$ \\text{FIRST}(\\alpha) = \\{ t \\in T \\cup \\{ \\epsilon \\} \\mid \\alpha \\overset{*}{\\Rightarrow} \\textcolor{red}{t} \\beta \\lor \\alpha \\overset{*}{\\Rightarrow} \\epsilon \\} $$ 因此对于这个集合而言，考虑非终结符 $A$ 的所有产生式 $A \\rightarrow \\alpha_1, \\quad A \\rightarrow \\alpha_2, \\quad \\ldots, \\quad A \\rightarrow \\alpha_m$，如果它们对应的 $FIRST(\\alpha)$ 集合互不相交，则只需查看当前输入词法单元，即可确定选择哪个产生式（或报错）。 符号 X 的 FIRST 集合计算\r符号串 $X\\beta$ 的 FIRST 集合计算\r具体可以看下面的例子： 后面跟的为终结符 ... A-\u003eaB|ε A-\u003ec ... First(A) = {a，ε，c} 后面跟的为非终结符 # 情况一 ... A-\u003eBa B-\u003eb ... First(A) = {b} # 情况二 ... A-\u003eBc B-\u003eb|ε ... First(A) = {b, c} # 情况三 ... A-\u003eBC B-\u003eb|ε C-\u003ec|ε ... First(A) = {b, c, ε} FOLLOW 集合\r$FOLLOW(A)$ 是可能在某些句型中紧跟在 $A$ 右边的终结符的集合。即对于任意的（产生式的左部）非终结符$A \\in N$： $$ \\text{FOLLOW}(A) = \\{ t \\in T \\cup \\{ \\text{\\$} \\} \\mid \\exists s.\\ S \\overset{*}{\\Rightarrow} s \\triangleq \\beta A \\textcolor{red}{t} \\gamma \\} $$ 这里的 $\\$$ 就是文法开始符，只在第一个字符的 $FOLLOW$ 集合中进行添加。考虑产生式 $A \\rightarrow \\alpha$，如果从 $\\alpha$ 可能推导出空串（$\\textcolor{red}{\\alpha \\overset{*}{\\Rightarrow} \\epsilon}$），则只有当当前词法单元 $t \\in FOLLOW(A)$，才可以选择该产生式。 符号 X 的 FOLLOW 集合计算\r符号串 $X\\beta$ 的 FOLLOW 集合计算\r构建预测分析表\r根据上述对于 FIRST集合 和 FOLLOW集合 的描述，可以计算给定文法 G 的预测分析表：对应每条产生式 $A \\rightarrow \\alpha$ 与终结符 $\\textcolor{blue}{t}$，如果 $$ t \\in \\text{FIRST}(\\alpha) \\\\ \\alpha^* \\Rightarrow \\epsilon \\land t \\in \\text{FOLLOW}(A) $$ 则在表格 $[\\textcolor{red}{A}, \\textcolor{blue}{t}]$ 中填入 $A \\rightarrow \\alpha$（编号）。 综合例子\r对于下面的例子，可以得到它们的 FIRST 和 FOLLOW 集合。 $$ X \\rightarrow Y \\\\ X \\rightarrow \\alpha \\\\ Y \\rightarrow \\epsilon \\\\ Y \\rightarrow c \\\\ Z \\rightarrow d \\\\ Z \\rightarrow XYZ $$ FIRST集合 FOLLOW集合 $FIRST(X) = \\{a, c, \\epsilon \\}$ $FIRST(Y) = \\{c, \\epsilon \\}$ $FOLLOW(X) = \\{a, c, d, \\$ \\}$ $FITST(Z) = \\{a, c, d \\}$ $FOLLOW(Y) = \\{a, c, d, \\$ \\}$ $FITST(XYZ) = \\{a, c, d \\}$ $FOLLOW(Z) = \\empty$ $FITST(YZ) = \\{a, c, d \\}$ 关于 FIRST 和 FOLLOW 的更多讲解和例子可以看 这篇文章。之后根据上面信息，可以构建相应的预测分析表。也可以看 这个视频 来学习两个集合的构造方法。 预测分析表\r","date":"2024.11.14","objectID":"/blog/posts/compile/parser-cfg-ll-lr/:2:3","tags":["compile"],"title":"02 Parser Cfg LL LR","uri":"/blog/posts/compile/parser-cfg-ll-lr/"},{"categories":["compile"],"content":"适用于 LL(1) 文法\r这里主要说明它的局限性。对于上面预测分析表的构建，需要定义 LL(1) 文法，即如果文法 G 的预测分析表是无冲突的，则 G 是 LL(1)文法。无冲突就是每个单元格里只有一个产生式（编号）即仅根据当前 token 即可递推 production。那么根据这个无冲突的预测分析表，对于当前选择的非终结符，仅根据输入中当前的词法单元（LL(1)）即可确定需要使用哪条产生式。这里根据 LL(1) 文法的定义就可以看到其局限性，它需要的就是预测分析表是无冲突的，其他情况就不适用这个文法了。 ","date":"2024.11.14","objectID":"/blog/posts/compile/parser-cfg-ll-lr/:2:4","tags":["compile"],"title":"02 Parser Cfg LL LR","uri":"/blog/posts/compile/parser-cfg-ll-lr/"},{"categories":["compile"],"content":"Adaptive LL(*) 语法分析算法\r看 视频 和 论文 理解吧。 记录一下别人的 博客1，博客2。第二篇是视频的笔记，虽然它只是截图保存🐶。 ","date":"2024.11.14","objectID":"/blog/posts/compile/parser-cfg-ll-lr/:3:0","tags":["compile"],"title":"02 Parser Cfg LL LR","uri":"/blog/posts/compile/parser-cfg-ll-lr/"},{"categories":["compile"],"content":"LR(0) 语法分析算法\r相关文法\r对于 LL(k) 而言，它的缺陷就是在仅看到右部的前 k 个词法单元时就必须预测要使用哪条产生式。那么相对应的，LR(k) 的优势就是看到某个产生式的整个右部对应的词法单元之后再做决定。 那么 LR 语法分析器的特点就是自底向上、不断归约、基于句柄识别自动机，适用于 LR 文法。 ","date":"2024.11.14","objectID":"/blog/posts/compile/parser-cfg-ll-lr/:4:0","tags":["compile"],"title":"02 Parser Cfg LL LR","uri":"/blog/posts/compile/parser-cfg-ll-lr/"},{"categories":["compile"],"content":"语法分析树\r这里的语法分析树是自底向上进行构建的，根节点是文法的起始符号 S；每个中间非终结符节点表示使用它的某条产生式进行归约；叶节点是词法单元流 $w$$，它仅包含终结符号与特殊的文本结束符 $$$。 ","date":"2024.11.14","objectID":"/blog/posts/compile/parser-cfg-ll-lr/:4:1","tags":["compile"],"title":"02 Parser Cfg LL LR","uri":"/blog/posts/compile/parser-cfg-ll-lr/"},{"categories":["compile"],"content":"推导与归约\r下图就是推导与归约的示例，我们首先进行自顶向下的最右推导，之后直接沿着反方向进行归约。因此这里的 LR 语法分析器中的 L 就是从左向右扫描输入；R 就是构建反向最右推导。 推导与归约1\r推导与归约2\r由上图可以看出，这里的最右推导就是指对于一个产生式，最先解析右侧的的非终结符，使其转化为终结符，然后再解析左侧的非终结符。因此反方向的归约从左到右进行识别，如果识别到产生式完整的右部，那么就转化为产生式的左部，这样持续进行，最终把终结符推导为最后的 E。 这里之所以采用最右推导是为了效率，若是最左推导，那么归约就是从右往左了。这样构造的话，非终结符必须全部输入完成才会进行解析，因为这里是从后往前。而采用最右推导，那么归约就是从前往后了，这样在读入输入的时候就可以进行归约构建，增强效率。 归约过程\r归约如下图所示。上边缘就是需要进行归约的部分，剩余的输入就是还没有读入的部分，这里就是一遍读入一遍进行归约。 归约概念\r下面就是归约的过程，通过栈结构将左侧的子树进行压栈，如果栈的栈顶部分满足产生式，可以推导出左半部分，那么就移除右半部分，将左半部分压栈。否则从左往右读入输入进行压栈。 归约过程\r那么现在的问题就是对于这个栈结构，什么时候进行归约，又是按哪条产生式进行归约？例如上面：为什么第二个 F 以 T * F 整体被归约为 T？那么回答就是，这与栈的当前状态 “T * F” 相关。 LR 分析表\r分析表解析\r这里需要注意，对于归约和 GOTO 语句而言，若是进行了归约，那么就存在着出栈的操作，此时对于 GOTO 语句的状态就不是当前的状态，而是上一次的状态，因为出栈引起了状态的变化。 移入与归约\rLR 分析表的构建\r这里展示了 LR(0) 句柄识别有穷状态自动机，我们之后要做的就是解释这个状态集怎么转化为分析表的。 句柄识别有穷状态自动机\r状态\r首先我们需要知道状态是什么，对于这个分析表，状态是语法分析树的上边缘，存储在栈中。由此我们可以的到下面的结论。 状态定义\r项集和项集族\r点的含义\r由上图，· 表示我们当前看的位置。一开始，栈为空，期望输入是 E 可以展开得到的一个句子并以 $ 结束。输入以 E 开始，意味着它可能以 E 的任何一个右部开始。 句柄\r根据下图我们可以知道句柄的概念，之后就需要知道句柄可能在哪里出现，有之前的操作我们可以看出，句柄在栈顶位置，所以这里就需要我们设计出一种满足“句柄总是出现在栈顶”性质的 LR 语法分析器。 句柄概念\r增广文法\r这里我们需要进行的就是添加一个文法帮助我们进行转换。 增广文法\r状态机转换\r由一开始的自动机就可以看出，这里通过 · 点的移动来进行状态的转换，根据产生式的内容进行转移。 $$ J = \\text{GOTO}(I, X) = \\text{CLOSURE}(\\{[A \\to \\alpha X \\cdot \\beta ] \\mid [A \\to \\alpha \\cdot X \\beta] \\}) $$ 接受状态： $$ F = \\{ I \\in C \\mid \\exists [A \\to \\alpha \\cdot] \\in I \\} $$ 此时，产生式 $ A \\ to \\alpha$ 的完整右部出现在栈顶 状态转移\r构建分析表\r分析表构造规则\r分析表构造规则\r分析表构造规则\r分析表构造规则\r","date":"2024.11.14","objectID":"/blog/posts/compile/parser-cfg-ll-lr/:4:2","tags":["compile"],"title":"02 Parser Cfg LL LR","uri":"/blog/posts/compile/parser-cfg-ll-lr/"},{"categories":["compile"],"content":"文法定义\r文法定义\r这里的无冲突，就是指每一个空位上只有一个规则，而上图可见是冲突的，它不是 LR(0) 文法。这里的 0 指的就是归约时无需向前看，· 移动到什么地方就进行归约判断。 自动机与栈\r","date":"2024.11.14","objectID":"/blog/posts/compile/parser-cfg-ll-lr/:4:3","tags":["compile"],"title":"02 Parser Cfg LL LR","uri":"/blog/posts/compile/parser-cfg-ll-lr/"},{"categories":["compile"],"content":"其余\r余下的 LSR(1)、LR(1)、LALR(1) 算法都是面对移入归约冲突、归越归约冲突的不同改进。具体可以看课程。 ","date":"2024.11.14","objectID":"/blog/posts/compile/parser-cfg-ll-lr/:4:4","tags":["compile"],"title":"02 Parser Cfg LL LR","uri":"/blog/posts/compile/parser-cfg-ll-lr/"},{"categories":["AI"],"content":"这里记录在学习人工智能，主要是大语言模型时的相关知识点，用以梳理逻辑。 ","date":"2024.11.12","objectID":"/blog/posts/ai/ai_knowledge/:0:0","tags":["AI"],"title":"AI Knowledge","uri":"/blog/posts/ai/ai_knowledge/"},{"categories":["AI"],"content":"人工智能相关\r","date":"2024.11.12","objectID":"/blog/posts/ai/ai_knowledge/:1:0","tags":["AI"],"title":"AI Knowledge","uri":"/blog/posts/ai/ai_knowledge/"},{"categories":["AI"],"content":"基础概念\r简单而言，人工智能是一个很宽泛的概念，只是一个广义上的称呼，这里主要还是区分机器学习、神经网络、深度学习的相关概念。 机器学习：这是人工智能的基础，通过提供大量样本数据来建立模型，以识别和预测新的事物。例如，通过输入一张图片并经过一系列的运算后，模型可以判断这张图片属于哪一类。机器学习的核心目的是从数据中发现模式和规律，并通过构建一个模型来处理数据，实现预测和决策。通过训练数学模型，机器学习能够根据已知的数据和结果的映射关系，在遇到新数据时准确预测输出结果。 神经网络：神经网络模仿动物大脑的结构和功能，本质上是用数学公式来构建模型。它是机器学习的一个分支。机器学习本身依赖数学公式进行计算，而神经网络的特别之处在于拥有大量“神经元”，即隐藏层。可以简单地理解为，机器学习是基于数学公式建模的总体概念，而神经网络和其他方法（如k-means聚类）则是不同的具体实现形式，主要差异体现在算法，即数学公式的不同。如下图，机器学习的每个类别对应一种算法。具体细节可参考这个视频，虽然标题涉及机器学习，但讲解的重点主要是神经网络的原理和概念。同时也可以观看这个视频，讲解更为细致全面。 深度学习：这是人工神经网络的一个特例，其复杂性远超一般神经网络。深度学习通过增加神经网络的隐藏层数量来实现更高难度的任务。与一般神经网络相比，深度学习网络结构更为深层和复杂。其他类型的神经网络是在不同方面进行了改进，因此产生了不同的类别，但它们都包含隐藏层这一特征。 关系图\r更为详细的解释可以看这篇文章，讲述了更为细节的内容。 ","date":"2024.11.12","objectID":"/blog/posts/ai/ai_knowledge/:1:1","tags":["AI"],"title":"AI Knowledge","uri":"/blog/posts/ai/ai_knowledge/"},{"categories":["AI"],"content":"大语言模型\r","date":"2024.11.12","objectID":"/blog/posts/ai/ai_knowledge/:2:0","tags":["AI"],"title":"AI Knowledge","uri":"/blog/posts/ai/ai_knowledge/"},{"categories":["AI"],"content":"自然语言处理阶段\r第一阶段：统计模型 + 数据（特征工程） 决策树、SVM、HMM、CRF、TF-IDF、BOW 第二阶段：神经网路 + 数据 Linear、CNN、RNN、GRU、LSTM、Transformer、Word2vec、Glove 第三阶段：神经网络 + 预训练模型 + （少量）数据 GPT、BERT、ROBERTa、ALBERT、BART、T5 第四阶段：神经网络 + 更大的预训练模型 + Prompt ChatGPT、Bloom、LLaMA、Alpaca、Vicuna、MOSS、文心一言、通义千问、星火 ","date":"2024.11.12","objectID":"/blog/posts/ai/ai_knowledge/:2:1","tags":["AI"],"title":"AI Knowledge","uri":"/blog/posts/ai/ai_knowledge/"},{"categories":["AI"],"content":"Transformer\r使用 GPT 进行举例，它的全称为 Generative Pre-trained Transformer。显而易见，Transformer 就是它技术的关键所在，这里可以通过这篇文章详细了解它们之间的联系，以及 Transformer 的核心 Self-Attention。然后这个视频更为细致的讲解其底层知识。 ","date":"2024.11.12","objectID":"/blog/posts/ai/ai_knowledge/:2:2","tags":["AI"],"title":"AI Knowledge","uri":"/blog/posts/ai/ai_knowledge/"},{"categories":["android"],"content":"RiskGuard app 的总览，介绍该项目的相关信息。 ","date":"2024.11.9","objectID":"/blog/posts/android/riskguard/:0:0","tags":["android"],"title":"RiskGuard","uri":"/blog/posts/android/riskguard/"},{"categories":["android"],"content":"UI 设计\r跟随 Project Manager 进行 UI 设计，采用 kotilin 进行了实现，延续该项目的配色风格，在此基础上添加自己的模块。这里补充一下状态栏的设计，在 res/values/themes.xml 中删除 style 标签下所有内容，转化为下面的内容，那么状态栏就会透明的，图标和文字为深色，这样 UI 会更适配。 配置状态栏\r","date":"2024.11.9","objectID":"/blog/posts/android/riskguard/:1:0","tags":["android"],"title":"RiskGuard","uri":"/blog/posts/android/riskguard/"},{"categories":["android"],"content":"展开二级列表\r一开始是想的实现珍惜大佬 hunter 的 UI 设计，所以一直寻找可折叠的 textview 项目，但是发现都不尽如人意。直到找到了 ExpandableRecyclerView 这个项目，所以就使用这个进行二级列表的设置。也因此改为 kotilin 作为 UI 界面的语言，这样便于直接进行代码移植。之后把之前的设备信息获取代码移植后，配置了列表的折叠和展开后，也添加了一个全部展开/折叠功能。 ","date":"2024.11.9","objectID":"/blog/posts/android/riskguard/:2:0","tags":["android"],"title":"RiskGuard","uri":"/blog/posts/android/riskguard/"},{"categories":["programming"],"content":"学习 C++ 的使用，主要区分 C++ 和 C 的区别。 ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:0:0","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"C++ 历史速览\r这里通过求和的案例来记录 C、C++98、C++11、C++17 的变化。 古代 C 语言，采用 malloc 和 free 来管理内存，同时数据需要自己进行定义，并通过 for 循环实现求和操作。同时打印采用 printf，需要声明打印数据的类型才可以正确打印。 #include \u003cstdlib.h\u003e #include \u003cstdio.h\u003e int main() { size_t nv = 4; int *v = (int *)malloc(nv * sizeof(int)); v[0] = 4; v[1] = 3; v[2] = 2; v[3] = 1; int sum = 0; for (size_t i = 0; i \u003c nv; i++) { sum += v[i]; } printf(\"%d\\n\", sum); free(v); return 0; } 近代 C++98 引入 STL 容器库，这样就不需要自己进行内存的释放了，离开了当前作用域会自己进行销毁。这里的创建和销毁实质上就是 STL 容器的构造函数和析构函数。同时引入了重载的 cout 函数，因此不需要指定变量类型，可以直接进行打印， #include \u003cvector\u003e #include \u003ciostream\u003e int main() { std::vector\u003cint\u003e v(4); v[0] = 4; v[1] = 3; v[2] = 2; v[3] = 1; int sum = 0; for (size_t i = 0; i \u003c v.size(); i++) { sum += v[i]; } std::cout \u003c\u003c sum \u003c\u003c std::endl; return 0; } 近现代 C++11 引入了 {} 初始化表达式和 range-based for-loop 机制。这样可以通过花括号来进行赋值，并且支持通过迭代器来进行遍历，应用函数。 #include \u003cvector\u003e #include \u003ciostream\u003e #include \u003calgorithm\u003e int sum = 0; void func(int vi) { sum += vi; } int main() { std::vector\u003cint\u003e v = {4, 3, 2, 1}; // 在 \u003calgorithm\u003e 中进行实现 std::for_each(v.begin(), v.end(), func); std::cout \u003c\u003c sum \u003c\u003c std::endl; return 0; } 同时近现代 C++11 还引入了 lambda 表达式。如下可以看出不需要定义全局变量 sum 了，可以对局部变量进行相关操作。 #include \u003cvector\u003e #include \u003ciostream\u003e #include \u003calgorithm\u003e int main() { std::vector\u003cint\u003e v = {4, 3, 2, 1}; int sum = 0; std::for_each(v.begin(), v.end(), [\u0026] (int vi) { sum += vi; }); std::cout \u003c\u003c sum \u003c\u003c std::endl; return 0; } 现代 C++14 支持 lambda 用 auto 自动推断类型。 #include \u003cvector\u003e #include \u003ciostream\u003e #include \u003calgorithm\u003e int main() { std::vector\u003cint\u003e v = {4, 3, 2, 1}; int sum = 0; std::for_each(v.begin(), v.end(), [\u0026] (auto vi) { sum += vi; }); std::cout \u003c\u003c sum \u003c\u003c std::endl; return 0; } 当代 C++17 拥有 CTAD（compile-time argument deduction），可以进行编译期参数推断，但是需要在 CMAKE 中添加 set(CMAKE_CXX_STANDARD 17)。同时还引入常用数值算法。 #include \u003cvector\u003e #include \u003ciostream\u003e #include \u003calgorithm\u003e int main() { std::vector v = {4, 3, 2, 1}; int sum = 0; std::for_each(v.begin(), v.end(), [\u0026] (auto vi) { sum += vi; }); std::cout \u003c\u003c sum \u003c\u003c std::endl; return 0; } #include \u003cvector\u003e #include \u003ciostream\u003e #include \u003cnumeric\u003e int main() { std::vector v = { 4, 3, 2, 1 }; // 下面三者效果等同 //int sum = std::reduce(v.begin(), v.end(), 0, [](int x, int y) { // return x + y; //}); // 下面两种就是引入常用数值算法，主要实现在 \u003cnumeric\u003e 头文件中 //int sum = std::reduce(v.begin(), v.end()); int sum = std::reduce(v.begin(), v.end(), 0, std::plus{}); std::cout \u003c\u003c sum \u003c\u003c std::endl; return 0; } ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:1:0","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"头文件\rC++ 包含标准 C 语言头文件，对于原本 C 的头文件，C++ 有两种方式进行引用，一种是原有的方式（后面跟 .h），一种就是去掉 .h，在库前面添加一个 c 标识这是原本 C 的头文件。对于自己写的头文件还是原方式进行引用，即 \"\"。 #include \u003ciostream\u003e // 基本输入输出 #include \u003ccstdio\u003e // 在原来 C 语言的库前面加一个 c，去掉 .h #include \u003cstdio.h\u003e // 采用原有方式进行引用 #include \"myFile.h\" // 自己的文件，采用原有方式引用 ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:2:0","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"命名空间\r","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:3:0","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"基础知识\r命名空间增加了标识符的使用率，减少因为命名产生的冲突。对于命名空间而言，其中的变量和函数等都是属于自己这个空间的，需要通过标识空间名来指明数据的归属，这样可以使得不同命名空间可以存在同样名称的数据，它们之间不会产生冲突。 声明命名空间：namespace 空间名{}，命名空间的声明不能写在函数中 访问数据：空间名::空间中的成员名 省略前缀的方式：using namespace 空间名，表示从这个地方开始，后面都可以省略前缀。 #include \u003ciostream\u003e using namespace std; // 标准命名空间 namespace A{ int num = 1; void print(){ printf(\"A\\n\"); } } namespace B{ int num = 2; void print(){ printf(\"A\\n\"); } } namespace C{ namespace D{ int cd_num = 3; } } int g_num = 1001; int main(){ // 使用省略前缀的方式，可以直接使用其中的函数 cout \u003c\u003c \"命名空间\" \u003c\u003c endl; std::cout\u003c\u003c \"命名空间\" \u003c\u003c endl; // 不同命名空间访问数据 A::num = 2; B::print(); // 省略前缀方式访问数据 using namespace A; num = 3; // A 命名空间数据 using namespace B; B::num = 4; // B 命名空间数据，省略前缀需要注意二义性的问题，所以还需要标识命名空间 // 嵌套命名空间 C::D::cd_num = 5; using namespace C::D; cd_num = 5; // :: 为作用域分辨符，同时可以用于指明全局变量 int g_num = 11; printf(\"num %d\\n\", g_num); // 变量访问采用就近原则，这里就是访问上面的局部变量，返回 11 printf(\"num %d\\n\", ::g_num); // 使用作用域分辨符，指明访问全局变量，返回 1001 return 0; } ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:3:1","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"内联命名空间\r对于嵌套命名空间，访问需要使用多层空间名，但是可以采用内联命名空间的方式直接进行访问。 #include \u003ciostream\u003e using namespace std; namespace Version{ inline namespace v2017{ void showVersion(){ cout \u003c\u003c \"v2017\" \u003c\u003c endl; } } namespace v2020{ void showVersion(){ cout \u003c\u003c \"v2020\" \u003c\u003c endl; } } } int main(){ // 上面采用内联命名空间，即添加了 inline，设置了默认的情况，使得下面的两个语句效果等价。 // Version::v2017::showVersion(); Version::showVersion(); } ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:3:2","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"动态内存分配\rC 的动态内存采用函数 malloc calloc realloc free，具体可以从 堆基础 获取详细介绍。而 C++ 使用 new delete 操作符进行动态内存分配。但是当前这种分配方式还是容易产生一系列问题，所以引入了 RAII（Resource Acquisition Is Initialization）的思想，它认为资源获取就是初始化，反之，资源释放就是销毁，具体可以看下面 RAII 与智能指针 的部分，那么之后面对这样的情况，就应该使用智能指针，不再使用 new delete。 #include \u003ciostream\u003e using namespace std; void showArr(int* arr, int len){ for (int i = 0; i \u003c len; i ++){ cout \u003c\u003c arr[i] \u003c\u003c \" \"; } cout \u003c\u003c endl; } int main(){ // C 内存分配 int *p = (int*)calloc(5, sizeof(int)); showArr(p, 5); // calloc 会初始化为 0，所以可以直接打印 free(p); p = nullptr; // free 只是清除空间，还需要制空指针，不然就是一个野指针 // C++ 内存分配 int* page = new int; // 申请一个 int *page = 19; // 这里简单 new 不会初始化为 0，需要自行设置值 // int* page = new int(19); // 可以使用 c++ 的括号赋值直接进行赋值 cout \u003c\u003c *page \u003c\u003c endl; delete page; page = nullptr; page = new int[29]; // 申请一个数组，这里也不会初始化，所以可以采用下面的括号赋值，后面自动初始化为 0；或者使用 for 循环进行赋值 // page = new int[29]{1, 2, 3, 7, 12, 12}; showArr(page, 29); delete[] page; page = nullptr; return 0; } ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:4:0","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"Lambda 表达式\rC++ 中 lambda 表达式语法为 [capture](parameters) -\u003e return_type { body }，相关描述如下： capture：变量捕获，定义了 lambda 如何捕获上下文中的变量 parameters：函数参数列表 return_type（可选）：定义返回类型（通常省略，由编译器推导） body：函数体 ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:5:0","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"Capture Clause\r这里定义的就是 lambda 是否可以捕获上下文的相关变量，直接写变量就是 按值捕获，加上 \u0026 就是按 引用捕获。 [] 表示不捕获上下文变量 [N, \u0026M] 表示 N 为按值捕获，不能修改原变量的值，M 为引用捕获，可以修改外围变量的值 [\u0026] 表示按照引用捕获，捕获所有封闭范围的变量，也就是所有在 lambda 外部作用域中定义的变量都通过引用传递给 lambda [=] 同上含义，但是表示所有的变量都按值捕获 [\u0026, =N] 表示 N 为按值捕获，其他变量都是按引用捕获 [this] 在某个 class 中使用匿名函数，可以通过这个方式捕获当前实例的指针 [*this] C++17，之后还可以按值捕获该实例 [N, \u0026M, K=5] C++14 之后，可以在捕获语句中定义新的变量并初始化，这样的变量 K 无需出现在匿名函数外围环境中 ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:5:1","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"Parameters\rC++ 14 之后，参数列表支持 auto 类型，例如 [](auto a, auto b) {return a + b;}，这个让匿名函数变的更通用更泛型。 ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:5:2","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"枚举类型\rC 和 C++ 都提供了枚举类型，两者有一定的区别。这里主要就是 C++ 的枚举类型，不涉及 C 的。 ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:6:0","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"enum 枚举类型\rC++ 中的 enum 就是枚举类型的标识符，它只允许赋值枚举值；同时枚举元素会暴露在外部作用域，两个不同枚举类型若是含有相同枚举元素，会产生冲突；不同的枚举可以直接进行比较 // 定义 enum WEEK {MON, TUE, WED, THI, FIR, SAT, SUN}; enum SHAPE {CIRCLE, RECT, POINT, LINE}; // 只允许赋值枚举值，前面的 enum 不进行添加就可以使用 WEEK today = 3; // 错误 error C2440：“初始化”：无法从“int”转换为“main::WEEK” today = CIRCLE; // 错误 error C2440：“=”：无法从“main::SHAPE”转换为“main::WEEK” // 枚举元素暴露在外部作用域 enumc OTHER {RECT}; // 错误 error C2365：“RECT”：重定义；以前的定义是“枚举数” Int RECT = 12; // 错误同上，但是可以通过枚举名访问指定的枚举属性 OTHER::RECT; // 正确 // 不同类型的枚举也可以直接比较 if (CIRCLE == MON){ cout \u003c\u003c \"yes\" \u003c\u003c endl; } ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:6:1","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"enum class 强枚举类型\r这里强枚举类型不会将枚举元素暴露在外部作用域，必须通过枚举名去访问；同时不相关的两个枚举类型不能直接比较，编译报错 // 定义 enum class WEEK {MON, TUE, WED, THI, FIR, SAT, SUN}; enum class SHAPE {CIRCLE, RECT, POINT, LINE}; // 不暴露在外部作用域 cout \u003c\u003c SHAPCE::RECT \u003c\u003c endl; // 输出 1 // 不相关的两个枚举类型不能直接比较 if (SHAPCE::RECT == WEEK::MON){ // error c2676：二进制“==\"：“main::SHAPE\"不定义该运算符或到预定义运算符可接收的类型的转换 cout \u003c\u003c \"yes\" \u003c\u003c endl; } ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:6:2","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"内联函数\r函数调用时，需要跳转到函数的地址去执行，执行完成后返回到被调用函数，比较费时，因此，C++中提供了一种操作方式，允许编译时直接把函数替换到调用处，即内联函数，它没有普通函数调用时的额外开销（压栈，跳转，返回）。在函数前面加上 inline 申明为内联函数。 内联函数声明时 inline 关键字必须和函数定义结合在一起，否则编译器会直接忽略内联请求。 C++ 编译器不一定准许函数的内联请求（只是对编译器的请求，因此编译器可以拒绝） 现代C++编译器能够进行编译优化，因此一些函数即使没有 inline 声明，也可能被编译器内联编译 #include \u003ciostream\u003e using namespace std; // 宏定义，会在编译的时候（预处理）进行替换，节省空间和时间，效率高，不会类型检查 #define MAX(a, b) a \u003e b ? a : b; // 内联函数，用来替换宏定义。inline 是关键字 /* 1. 不能存在任何形式的循环语句，不能存在过多的条件判断语句 2. 函数体不能过于庞大，不能对函数进行取址操作 3. 编译器对于内联函数的限制并不是绝对的，内联函数相对于普通函数的优势只是省去了函数调用时压栈，跳转和返回的开销。因此，当函数体的执行开销远大于压栈，跳转和返回所用的开销时，那么内联将无意义。 */ inline int mmax(int a, int b){ return a \u003e b ? a : b; } ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:7:0","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"强制类型转化\rC 风格的强制类型转换很简单，据使用 Type b = (Type)a 形式进行转换。但是 C 风格的类型转换有不少缺点：万物皆可转，不容易区分，不容易查找代码。因此 C++ 提供了四种类型转换操作符来应对不同场合。 类型转换操作符 作用 static_cast 静态类型转换，编译器做类型检查，基本类型能转换，指针不能 reinterpret_cast 重新解释类型 const_cast 去const属性 dynamic_cast 动态类型转换，运行时检查类型安全（转换失败返回nullptr）如子类和父类之间的多态类型转换 #include \u003ciostream\u003e using namespace std; class Animal { public: virtual void cry() = 0; virtual ~Animal(){} }; class Dog:public Animal { public: void cry() override{ cout \u003c\u003c \"狗吠\" \u003c\u003c endl; } void seeHome(){ cout \u003c\u003c \"看家\" \u003c\u003c endl; } }; class Cat:public Animal { public: void cry() override{ cout \u003c\u003c \"猫叫\" \u003c\u003c endl; } void catchMouse(){ cout \u003c\u003c \"抓老鼠\" \u003c\u003c endl; } }; void obj(Animal* base){ base-\u003ecry(); // Dog：看家 // ((Dog*)base)-\u003eseeHome(); // 这种行为，不会根据传入参数的实际对象进行相应函数调用，而是只要转换就进行调用，也就是这样没有安全检查 Dog* dog = dynamic_cast\u003cDog*\u003e(base); // 这里运行时进行判断，如果转换成功返回子类所在地址，转换失败返回空指针 if (dog) { dog-\u003eseeHome(); } // Cat：抓老鼠 //((Cat*)base)-\u003ecatchMouse(); Cat* cat = dynamic_cast\u003cCat*\u003e(base); if (cat) { cat-\u003ecatchMouse(); } } int main(){ // 1. static_cast 类似 C 风格的强制转换，进行无条件转换，静态类型转换。 /* 基本数据类型转换，enum，struct，int，char，float 等。static_cast 不能进行无关类型（如非基类和子类）指针之间的转换。 可以用于 void* 和其他指针类型之间的转换（但是不能用于非 void 指针之间的转换） 不能用于两个不相关类型的转换，如 int 和 int* 之间的转换，虽然二者都是四个字节，但他们一个表示数据，一个表示地址，类型不相关，无法进行转换。 */ int age = 10; // double d = age; // 隐式类型转换 // double d = (double)age; // C 风格转换 double d = static_cast\u003cdouble\u003e(age); int* p = \u0026age; // double* pd = (double*)p; // 正确 // double* pd = static_cast\u003cdouble\u003e(p); // error C2440：“static cast”：无法从“int *”转换为“double *” void* pv = static_cast\u003cvoid*\u003e(p); // 正确 double* pdd = static_cast\u003cdouble*\u003e(pv); // 正确 // 2. reinterpret_cast 专门用来转换指针 double *pd = reinterpret_cast\u003cdouble*\u003e(p); // 正确 // 3. const_cast 去掉 const 属性 const int week = 7; // week = 5; // 错误，不能直接修改常量 int\u0026 rint = const_cast\u003cint\u0026\u003e(week); rint = 5; // 正确，可以去掉 const 属性，但是原先的值没有进行修改 // 4. dynamic_cast 把父类指针转为子类指针（判断父类指针指向的是哪个子类对象） Animal* pdog = new Dog; Animal* pcat = new Cat; obj(pdog); obj(pcat); } ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:8:0","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"RAII 与智能指针\r","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:9:0","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"unique_ptr\r在没有智能指针的 C++ 中，我们只能手动去 new 和 delete 指针。这非常容易出错，一旦忘记释放指针，就会导致内存泄露等情况。因此 C++11 引入了 unique_ptr 容器，它的结构函数中会调用 delete p，因此不会造成忘记释放指针的情况，同时这里释放之后，就会把指针设置为 nullptr，防范了空悬指针的情况。 #include \u003ccstdio\u003e #include \u003cmemory\u003e struct C { C() { printf(\"分配内存!\\n\"); } ~C() { printf(\"释放内存!\\n\"); } }; int main() { std::unique_ptr\u003cC\u003e p = std::make_unique\u003cC\u003e(); if (1 + 1 == 2) { printf(\"出了点小状况……\\n\"); return 1; // 自动释放 p } return 0; // 自动释放 p } 同时 unique_ptr 删除了拷贝构造函数，所有直接进行调用会出错，也就是 func(std::unique_ptr\u003cC\u003e p) -\u003e func(p) 会因为触发一次拷贝而报错。因此需要分以下两种情况进行调用。 #include \u003ccstdio\u003e #include \u003cmemory\u003e struct C { C() { printf(\"分配内存!\\n\"); } ~C() { printf(\"释放内存!\\n\"); } void do_something() { printf(\"成员函数!\\n\"); } }; void func1(C *p) { p-\u003edo_something(); } std::vector\u003cstd::unique_ptr\u003cC\u003e\u003e objlist; void func2(std::unique_ptr\u003cC\u003e p) { objlist.push_back(std::move(p)); // 进一步移动到 objlist } int main() { std::unique_ptr\u003cC\u003e p = std::make_unique\u003cC\u003e(); // func1() 不需要资源的占有权，即不需要生命周期，那么就可以使用原始指针 func1(p.get()); // func2() 需要生命周期，所以可以使用移动构造函数 printf(\"移交前：%p\\n\", p.get()); // 不为 null func2(std::move(p)); // 通过移动构造函数，转移指针控制权 printf(\"移交后：%p\\n\", p.get()); // null，因为移动会清除原对象，如果需要保留，那么就提前使用 C *raw_p = p.get(); 进行获取 return 0; } ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:9:1","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"shared_ptr\rshared_ptr 是更为智能的指针，它牺牲效率换来自由度，允许拷贝，它解决重复释放的方式是通过引用计数： 当一个 shared_ptr 初始化时，将计数器设为1 当一个 shared_ptr 被拷贝时，计数器加1 当一个 shared_ptr 被解构时，计数器减1。减到0时，则自动销毁他指向的对象 这样就可以保证，只要还有存在哪怕一个指针指向该对象，就不会被解构。同时我们可以使用 p.use_count() 来获取当前指针的引用计数。 #include \u003ccstdio\u003e #include \u003cmemory\u003e #include \u003cvector\u003e struct C { int m_number; C() { printf(\"分配内存!\\n\"); m_number = 42; } ~C() { printf(\"释放内存!\\n\"); m_number = -2333333; } void do_something() { printf(\"我的数字是 %d!\\n\", m_number); } }; std::vector\u003cstd::shared_ptr\u003cC\u003e\u003e objlist; void func(std::shared_ptr\u003cC\u003e p) { objlist.push_back(std::move(p)); // 这里用移动可以更高效，但不必须 } int main() { std::shared_ptr\u003cC\u003e p = std::make_shared\u003cC\u003e(); // 引用计数初始化为 1 func(p); // shared_ptr 允许拷贝！和当前指针共享所有权，引用计数加 1 func(p); // 多次也没问题，多个 shared_ptr 会共享所有权，引用计数加 1 p-\u003edo_something(); // 正常执行，p 指向的地址本来就没有改变 objlist.clear(); // 刚刚 p 移交给 func 的生命周期结束了！引用计数减 2 p-\u003edo_something(); // 正常执行，因为引用计数还剩 1，不会被释放 return 0; // 到这里最后一个引用 p 也被释放，p 指向的对象才终于释放 } ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:9:2","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["programming"],"content":"指针相关\r下面列举了使用指针类型取相关变量的数据，这里假设 eax = 0x401234 它存储了一个地址，而这个地址存储数据 0x12345678，那么对于下面的指针会获取不同的数据。 *(char*)eax; *(int*)eax; 对于 *(char*)eax;，首先 (char*)eax 将寄存器 eax 的值转换为 char* 类型，意味着 eax 被看作是指向一个字符的指针。而 *(char*)eax 是对该地址处的数据进行解引用操作，因为已经把 eax 强制转换为 char*，所以此时解引用会得到这个地址处的一个字符。因此它只会取得该地址处的 最低字节，即 0x78（注意小端序存储，0x78 0x56 0x34 0x12 顺序的数据会转化为 0x12345678） 对于 *(int*)eax; 也是同样的理解，最终会获取数据 0x12345678。 ","date":"2024.11.6","objectID":"/blog/posts/programming/cpp/:10:0","tags":["programming"],"title":"C 拾遗和 C++ 学习","uri":"/blog/posts/programming/cpp/"},{"categories":["config"],"content":"关于 Android Studio 的使用和配置。 ","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:0:0","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"Gradle下载配置\r直接更换国内腾讯 镜像源，打开 gradle - wrapper -gradle-weapper.properties 进行更改。然后点击 Sync Now 进行同步。参考-\u003e Android导入项目时Gradle下载速度慢_导入gradle项目特别慢 #Sun Feb 25 20:22:32 GMT+08:00 2024 distributionBase=GRADLE_USER_HOME distributionPath=wrapper/dists distributionUrl=https\\://mirrors.cloud.tencent.com/gradle/gradle-8.2-bin.zip # 这里就对应替换为腾讯的镜像地址 zipStoreBase=GRADLE_USER_HOME zipStorePath=wrapper/dists 但是这样也是很慢，还是得等，有时候还会突然跑到源地址去下载，搞不明白。(后续补充：有时候改了国内源，然后停止加载zip文件，之后重试一下就快很多了) ","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:1:0","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"新版 AS 添加依赖\r这里是在 settings.gradle.kts 文件中添加 maven 仓库，然后在 app 的 build.gradle.kts 文件中添加依赖。查看这里Android Studio | 2022.3.1版本解决创建项目下载gradle缓慢问题 // settings.gradle.kts 文件中 dependencyResolutionManagement { repositoriesMode.set(RepositoriesMode.FAIL_ON_PROJECT_REPOS) repositories { google() mavenCentral() maven { url = uri(\"https://jitpack.io\") }// as改版后的新添加方式 } } // build.gradle.kts 文件中 dependencies { implementation(\"com.github.Hitomis:CircleMenu:v1.1.0\") // as改版后的新添加方式 } ","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:2:0","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"SD卡读写权限\r对于低版本 sdk，只需要以下权限即可(在AndroidManifest.xml中修改)，在manifest标签中添加 \u003cuses-permission android:name=\"android.permission.READ_EXTERNAL_STORAGE\"\u003e\u003c/uses-permission\u003e \u003cuses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"\u003e\u003c/uses-permission\u003e 而对于高版本的 sdk，这里是34，则需要添加东西。新加入的在application标签中添加 \u003cuses-permission android:name=\"android.permission.READ_EXTERNAL_STORAGE\"\u003e\u003c/uses-permission\u003e \u003cuses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"\u003e\u003c/uses-permission\u003e \u003cuses-permission android:name=\"android.permission.MANAGE_EXTERNAL_STORAGE\"\u003e\u003c/uses-permission\u003e android:requestLegacyExternalStorage=\"true\" ","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:3:0","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"dataBinding使用\r刚开始创建的 Launch Activity 不用理会，但是对于新增加的Activity，想要使用 dataBinding 功能，就需要先在 build.gradle.kts文件中添加下面代码： android { ...... buildFeatures { dataBinding = true // 确保这里启用了数据绑定 } } 然后在相关 xml 文件中，对准 androidx.constraintlayout.widget.ConstraintLayout按下alt + enter 转化为 databinding 的模式。这样后续的 activity 才可以使用并编译apk成功。 ","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:4:0","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"图片导入\r资源主要就是存放在 res 目录下，如下表所示各个目录的作用。 目录 作用 drawable 存放所有的图片及图标配置（xml文件展示） layout 布局，创建一个Activity一般会同步创建一个布局。 mipmap 存放各种分辨率的图标，平常用的就是 xxhdpi values 一些固定的配置，例如值，主题等 这里 drawable 存放的东西大多就是 图标的配置，在 As 中可以利用 File → New → Android Resource File 来生成一个配置文件，例如背景之类的可以重复使用。 同时使用File → New → Image Asset 可以创建 app 应用的图标，提供 svg 图片即可自动创建。使用File → New → Vector Asset 则是创建图片，将一个 svg 格式的图片转化为 xml 文件形式，然后供 ImageView 等控件使用。如果只有 png 格式的图片，首先需要转化为 svg 格式，这里推荐网站，其余网站转化的 svg 可能存在问题，As 不一定可以使用。 ","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:5:0","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"So文件生成\r","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:6:0","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"添加新文件\r对于头文件 .h，首先在 cpp 目录下创建相应文件，然后在 CmakeLists.txt 文件中添加下面的代码 include_directories( basic.h ) 对于文件 .cpp，在 cpp 目录下创建，然后在 CmakeLists.txt 文件中的 add_libraty 添加新创建的 .cpp文件。这样多个 .cpp 文件就会编译为一个 so 库。 add_library(${CMAKE_PROJECT_NAME} SHARED # List C/C++ source files with relative paths to this CMakeLists.txt. native-lib.cpp checkfrida.cpp) 添加文件\r","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:6:1","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"编译多个so文件\r这里就是再添加一个 add_library 文件，这样就可以编译为多个了。 add_library( # so 文件的名字 checkroot # 共享库 SHARED # List C/C++ source files with relative paths to this CMakeLists.txt. checkroot.cpp) 编译多个文件\r","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:6:2","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"引入第三方库\r使用第三方库的函数，这里就是利用 target_link_libraries 进行引用，注意对于每一个so文件，都需要进行引用操作。这里上面的是默认的，要是能在 checkroot.cpp 文件中使用 log，那么就需要自己手动进行引用了。 target_link_libraries( checkroot # List libraries link to the target library android log ) 引入库\r","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:6:3","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"使用汇编配置\r起因就是需要使用 SVC 这条汇编语句完成 frida 的检测，但是查阅资料不知道怎么进行，最后终于尝试成功了，在此记录流程。 代码设置\r这里是复制 参考资料 的代码，然后做了删减达到了下面的效果。 bionic_asm.h ，它包含了汇编语言需要的一些基础配置，例如 \u003casm/unistd.h\u003e 中的系统调用号，MAX_ERRNO 是最大错误号的定义。最为关键的就是对于 ENTRY 和 END 的定义，没有这个定义，下面的汇编语言也就是会识别错误。同时这里和下面的 syscall.S 文件一样，只定义了 aarch64 ， x86_64 两个架构，所以在build.gradle.kts 的 abiFilters 选项也只能存在两个架构，不然会因为其他架构的汇编不存在而报错。 #pragma once #include \u003casm/unistd.h\u003e /* For system call numbers. */ #define MAX_ERRNO 4095 /* For recognizing system call error returns. */ #define __bionic_asm_custom_entry(f) #define __bionic_asm_custom_end(f) #define __bionic_asm_function_type @function #define __bionic_asm_custom_note_gnu_section() #if defined(__aarch64__) #define __bionic_asm_align 16 #elif defined(__x86_64__) #define __bionic_asm_align 16 #endif #define ENTRY_NO_DWARF(f) \\ .text; \\ .globl f; \\ .balign __bionic_asm_align; \\ .type f, __bionic_asm_function_type; \\ f: \\ __bionic_asm_custom_entry(f); \\ #define ENTRY(f) \\ ENTRY_NO_DWARF(f) \\ .cfi_startproc \\ #define END_NO_DWARF(f) \\ .size f, .-f; \\ __bionic_asm_custom_end(f) \\ #define END(f) \\ .cfi_endproc; \\ END_NO_DWARF(f) \\ /* Like ENTRY, but with hidden visibility. */ #define ENTRY_PRIVATE(f) \\ ENTRY(f); \\ .hidden f \\ /* Like ENTRY_NO_DWARF, but with hidden visibility. */ #define ENTRY_PRIVATE_NO_DWARF(f) \\ ENTRY_NO_DWARF(f); \\ .hidden f \\ #define __BIONIC_WEAK_ASM_FOR_NATIVE_BRIDGE(f) \\ .weak f; \\ #define ALIAS_SYMBOL(alias, original) \\ .globl alias; \\ .equ alias, original #define NOTE_GNU_PROPERTY() \\ __bionic_asm_custom_note_gnu_section() 这里就是 syscal.S 文件，它这里实现了对于 openat 函数的底层汇编，但是因为这里找不到 __set_errno_internal 这个函数所在的文件，没有办法链接，所以直接注释掉。因为这个就是对于错误进行处理的函数，就是运行失败之后进入错误处理再返回，告知错误的类型。我这里不需要进行维护，所以错误的类型可有可无，直接注释即可。 #include \"bionic_asm.h\" #if defined(__aarch64__) ENTRY(my_openat) mov x8, __NR_openat svc #0 cmn x0, #(MAX_ERRNO + 1) cneg x0, x0, hi // b.hi __set_errno_internal ret END(my_openat) #elif defined(__x86_64__) ENTRY(my_openat) movl $__NR_openat, %eax syscall cmpq $-MAX_ERRNO, %rax jb my_openat_return negl %eax movl %eax, %edi // call __set_errno_internal my_openat_return: ret END(my_openat) #endif 然后最后在需要使用外部汇编文件的代码中加入下面这一条代码。这样才可以进行函数的调用。 extern \"C\" int my_openat(int dirfd, const char *const __pass_object_size pathname, int flags, mode_t modes); AS 配置\rCMakeLists.txt\r添加 enable_language(ASM)，允许使用汇编语言，不加这个就会报错。同时下面链接库添加新增的上面的两个文件，保证可以整合到一个 so 文件中。 enable_language(ASM) add_library( # so 文件的名字 checkfrida # 共享库 SHARED # List C/C++ source files with relative paths to this CMakeLists.txt. checkfrida.cpp asm/bionic_asm.h asm/syscall.S ) build.gradle.kts\r在 android-—defaultConfig 的路径下添加 ndk，这里的 abiFilters 和官网给的样例不一致，具体看 参考资料，需要使用特定格式进行架构的添加。这里查看下面 使用不同结构的代码。同时这里因为只写了两种架构的 SVC 汇编，所以固定了架构，因此最后生成的 so 就只有这两种架构的文件了。如果不定义这个配置，那么默认就是会存在四种架构的 so 文件。 ","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:6:4","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"使用不同架构\rAs 进行了改版，所以之前的方式不能使用了。这里是在 app 的 build.gradle.kts 文件中添加。下面展示了两种方式。 android { ... defaultConfig { ... ndk { // 第一种方式 abiFilters.addAll(arrayOf(\"arm64-v8a\", \"x86_64\")) // 第二种方式 //abiFilters.add(\"arm64-v8a\") //abiFilters.add(\"x86_64\") //abiFilters.add(\"armeabi-v7a\") //abiFilters.add(\"x86\") } } } ","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:6:5","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"相关操作\r","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:7:0","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"存储空间管理\r移动 .gradle 到指定位置 将 .gradle文件 从C盘移动到D盘，这里同时还需要相应修改 idea.plugins.path 和 idea.log.path 移动 .android 到指定位置 解决方案 ","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:7:1","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"生成apk\rbuild -\u003e Generate Signed Bundle or APK：选择APK，然后创建key（注意需要路径完整），之后再选择 release 即可 生成的 app-debug.apk 在路径 /app/build/outputs/apk/debug/ 下面，或者在 /app/build/intermediates/apk/debug/ 目录下。这里不知道是什么的变化引起的生成存放目录的变化。 build → Generate Signed Bundle or APK 可以生成签名后的 apk（release版本），它存放在和 debug类似的目录下。这里我的 vivo 手机不能使用 debug 版本，只能下载 release 版本。 ","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:7:2","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"报错汇总\r遇到问题 Cannot use connection to Gradle distribution 'https://mirrors.cloud.tencent.com/gradle/gradle-8.2-bin.zip' as it has been stopped. 这里直接关闭项目，再重新打开即可。 Error running 'app': Default Activity not found 这里修改 configuration，更改 Launch Options -\u003e Launch:nothing android studio怎么运行没有activity的service、broadcastReceiver、cotentProvicer等 日志不能在 As 中显示 检查算法助手有没有 hook 对应程序，如果存在，那么它好像开机自启，自动将日志捕获了。关闭应用 hook 就可以显示日志了 ","date":"2024.11.5","objectID":"/blog/posts/config/android-studio/:8:0","tags":["config","android"],"title":"Android Studio 配置","uri":"/blog/posts/config/android-studio/"},{"categories":["config"],"content":"自己对于 FixIt 主题的一些配置 ","date":"2024.11.1","objectID":"/blog/posts/config/fixit/:0:0","tags":["config"],"title":"Fixit 配置","uri":"/blog/posts/config/fixit/"},{"categories":["config"],"content":"大致流程\r这里我使用的就是Git 子模块的安装方式，更详细的流程参照这篇快速上手。在必要的配置后，博客就可以进行本地浏览了。之后我使用 Github Action 的方式，将本地博客的所有文件上传到一个私密仓库，之后创建 Github Action 将通过 hugo --gc --minify 命令构建的 public 目录上传到另一个公开仓库 blog 中，这样就可以设置静态网站进行博客访问了，具体可以参考Huogo 主题配置。这样博客基本上就好了，可以开始额外的操作了。 ","date":"2024.11.1","objectID":"/blog/posts/config/fixit/:1:0","tags":["config"],"title":"Fixit 配置","uri":"/blog/posts/config/fixit/"},{"categories":["config"],"content":"图片自适应\r我使用的图片都是存储在图床中的，使用的就是 Markdown 的经典语法。但是它在 FixIt 的渲染中会左对齐，有时还会在图片后面跟随文字，感官很不好，所以思考怎么进行改进。在参考FixIt主题使用lightgallery自适应显示图片之后，自己进行了调整，达到了现在的效果。 这里主要参照上述参考文章的两种尝试，采用 lightgallery 来呈现图片。我参照 FixIt配置篇 将 lightgallery 设置为 \"force\" 但是如果没有 alt 和 title 属性，我的图片不会按照画廊形式呈现，所以我只能另辟蹊径。因为我使用 PicGo 进行图床配置，刚好它支持修改本地输出图片链接的格式，因此我修改PicGo的配置，把 Custom Output Format 配置为![${uploadedName}](${url} \"${uploadedName}\")。这样它会自己填充 alt 和 title，我只需要按照自身需求修改 alt 属性即可。 另外点击图片之后会显示 alt 和 title 两个信息，所以打开 F12 进行观察，发现第一行来自标签 \u003ch2\u003e，也是 alt 属性，第二行来自标签 \u003cp\u003e，是 title 属性。这部分都是在 themes/layouts/partials/plugin/image.html 中，所以直接修改主题文件，把这里的 \u003cp\u003e 删除，然后 \u003ch2\u003e 移动到中间，这样渲染的时候只会在下面出现 alt 属性了，放大图片和鼠标移动到图片上才会看到 title 属性。 修改配置文件\r","date":"2024.11.1","objectID":"/blog/posts/config/fixit/:2:0","tags":["config"],"title":"Fixit 配置","uri":"/blog/posts/config/fixit/"},{"categories":["config"],"content":"GitHub 提交记录贪吃蛇动画\r参照GitHub 提交记录贪食蛇动画进行配置，不过我将 Github Action 写在了同名仓库中了，这样就少创建了一个仓库，其他都是按照参考文章所述的进行配置即可。插一嘴，本来想把 github-metrics.svg 单独放在一个分支中，但是不熟悉自动化部署脚本，配置了一会儿发现总是部署失败，所以直接使用GitHub 个人主页美化教程来进行配置了。 ","date":"2024.11.1","objectID":"/blog/posts/config/fixit/:3:0","tags":["config"],"title":"Fixit 配置","uri":"/blog/posts/config/fixit/"},{"categories":["config"],"content":"配置文章修改时间\rFixit 的 frontmatter 有一个属性 lastmod，它就是文章最后修改时间，一开始的配置不知道有没有效果，但是还是参照 loveit主题配置 之后，形成了现在的配置。 当前配置是在 atchetypes/default.md 文件中，设置 lastmod: {{ .Date }}。config/_default/hugo.toml 中设置 enableGitInfo = true， 同时在 config/_default/params.toml 中，找到 [gitInfo] 选项，设置 repo 为自己 public 目录发布的仓库地址。这样就会在文章左下角和一开始的信息标识处显示更新时间。 这里的文章修改时间很玄学，要么新创建的文章上面信息展示的地方没有修改时间，要么地下的修改时间没有提交的 hash 值，要么修改的文章信息没有变化。最近一次成功修改还是通过 git 将 content 和 public 中的所有文件都删除，然后再提交之后才可以进行时间修改的，但是这个时候所有文章的修改时间都一样了。所以目前初步认为起关键作用的还是 content 目录下面文件的 git 提交时间，之后修改了文件，就不直接使用 add 进行添加了，而是把文件先删除，之后再提交，这样不知道会不会正确显示修改时间。 验证发现 git add . 也能发挥作用，但是修改后直接进行 hugo --gc --minify 提交发现不会出现修改信息，之后本地使用 hugo server -D 反而出现了修改时间。之后尝试首先本地部署，再进行生成，发现该效果会呈现出最新的修改时间。（这里的办法没用，目前得到的信息就是本次的修改不会改变本次的修改时间，只有下一次的 push 才会真正显示出上一次的修改，所以目前只能这样了，只要修改时间不像之前一样突然没有了就行）。 ","date":"2024.11.1","objectID":"/blog/posts/config/fixit/:4:0","tags":["config"],"title":"Fixit 配置","uri":"/blog/posts/config/fixit/"},{"categories":["config"],"content":"网站图标\r对于网站图标实在是束手无策，一开始采用官网配置，但是给的利用网站不能生成符合要求的一系列文件，同时把生成文件都放在/static目录下，最后会把这些文件生成在/public根目录下，很不喜，所以放弃了这种方法。后来尝试和 author 的 avatar 属性配置一样，把 svg 图片放在images目录下，但是展示不出来。最后看到别人文章，发现直接使用图床的图片可以生成，所以我现在也是利用这样的方式展示网站图标。 ","date":"2024.11.1","objectID":"/blog/posts/config/fixit/:5:0","tags":["config"],"title":"Fixit 配置","uri":"/blog/posts/config/fixit/"},{"categories":["config"],"content":"阅读原始文档\r可以看到左下角存在 阅读原始文档 的选项，点击就直接可以看到 markdown 的原文本了，当然这个是我们不想要的（虽然网页直接下载 pdf 也是一样的）。因此我们可以设置配置中的 outputs.toml 文件，在 page 中去除 markdown，这样点击就不会看到我们的源码了。 配置修改\r","date":"2024.11.1","objectID":"/blog/posts/config/fixit/:6:0","tags":["config"],"title":"Fixit 配置","uri":"/blog/posts/config/fixit/"},{"categories":["config"],"content":"Git 提交\r受上面 配置文章修改时间 的影响，记录下这里的 git 操作，以便之后忘记了还可以看这里进行会议。 # 本地预览渲染效果 hugo server -D # 最小化生成渲染文件 hugo --gc --minify # 将修改和新增的文件信息都添加到暂存区 git add . # 将删除文件信息添加到暂存区 git rm xxx # 根据暂存区信息提交代码到本地仓库 git commit -m \"xxx\" # 推送本地仓库代码到远程仓库 git push -u origin main 经过上面的操作，就会把代码传到 github 上了，之后使用 GitHub Action 把 public 传到公开仓库中，之后就可以根据静态页面进行查看了。这里还可以使用 Vscode 的提交板块，它应该和 jetbrains 的产品一样，内置了删除的操作，所以我们只需要写 commit 信息然后一直点击按钮就可以进行提交了，不需要区分添加修改和删除操作（这里我没有试过，只是猜测，但是 jetbrains 和 As 的 commit 是这样设计的）。 ","date":"2024.11.1","objectID":"/blog/posts/config/fixit/:7:0","tags":["config"],"title":"Fixit 配置","uri":"/blog/posts/config/fixit/"},{"categories":["config"],"content":"数学公式\r本来这个只是一个小问题，但是后来发现只要公式渲染出现问题，大概就是这个原因，所以记录一下 fixit 的数学公式渲染。主要的解决方案就是在 fixit 数学公式 的“关于转义字符相关的注意事项”，这里就是 Hugo 渲染的时候，数学公式中的有些字符和 HTML 产生冲突，所以需要转义处理。这里已经罗列了很多需要转移的字符，但是除此之外还有一些，这里通过 Latex 语法来显示额外也需要进行转义的字符： { -\u003e \\\\{ } -\u003e \\\\} $ -\u003e \\\\$ ","date":"2024.11.1","objectID":"/blog/posts/config/fixit/:8:0","tags":["config"],"title":"Fixit 配置","uri":"/blog/posts/config/fixit/"},{"categories":["config"],"content":"报错归纳\r","date":"2024.11.1","objectID":"/blog/posts/config/fixit/:9:0","tags":["config"],"title":"Fixit 配置","uri":"/blog/posts/config/fixit/"},{"categories":["config"],"content":"github action 报错\r在使用 GitHub Action 时，一直采用的 hugo-version: latest，本来没什么问题，但是 24 年年初出错了，显示了下面的内容。 github action 出错\r然后到处纠错，尝试根据报错把上面 阅读原始文档 图片中 home 一栏的 search 给删除，但是会向前报错 offline。于是怀疑时因为 Fixit 更新导致的问题，于是将这个更改为本地版本号，发现成功自动化部署。于是之后就把 version 固定为下面临近一次成功部署的版本号，之后尝试成功，可以使用。 成功部署版本号\r","date":"2024.11.1","objectID":"/blog/posts/config/fixit/:9:1","tags":["config"],"title":"Fixit 配置","uri":"/blog/posts/config/fixit/"},{"categories":["config"],"content":"一些关于 git 的操作，更多可以看看 深入Git。 git流程\r","date":"2024.11.1","objectID":"/blog/posts/config/git/:0:0","tags":["config"],"title":"Some about Git","uri":"/blog/posts/config/git/"},{"categories":["config"],"content":"相关命令\r# 本地仓库初始化 git init # 将文件提交到暂存区 git add \u003cfile_name\u003e git add . # 添加全部修改和新增文件 # 将文件提交到本地仓库 git commit -m \"commit information\" \u003cfile_name\u003e git commit -m \"commit inforamtion\" # 提交全部到本地仓库 # 添加远程仓库，这里是因为存在多个 git 用户所进行的配置，正常为 git@github:czTangt/blog.git git remote add origin git@github_czTangt:czTangt/blog.git # 提交到远程仓库 git push -u origin \u003cbranch_name\u003e git push -f # 强制提交 # 拉取远程仓库 git pull \u003cremote_name\u003e \u003cremote_branch_name\u003e # 这里是从远程仓库 remote_name 拉取指定分支 remote_branch_name 的更新并合并到当前分支。 # remote_name 通常是远程仓库的别名，例如默认的 origin，表示克隆时配置的远程仓库。 # git pull 是 git fetch 和 git merge 的组合操作，首先获取远程更新，然后尝试合并到当前分支。 # 分支操作 git branch -a # 查看所有分支 git branch \u003clocal_branch_name\u003e # 创建本地分支 git checkout \u003clocal_branch_name\u003e # 切换本地分支 git checkout -b \u003clocal_branch_name\u003e # 创建本地分支并切换 git branch -d \u003clocal_branch_name\u003e # 删除本地分支 git checkout -b \u003clocal_branch_name\u003e origin/\u003cremote_branch_name\u003e # 创建本地分支并与远程分支连接 git push origin --delete \u003cremote_branch_name\u003e # 删除远程分支 # 分支冲突 # 在 git 当中，通常合并分支可以自动完成。如果遇到分支冲突，需要手动选择要保留的分支，然后再次进行 git commit。 # 恢复文件为之前提交的状态 git checkout commit_version -- \u003cfile_name\u003e # 首先从 git log 获取某次提交的 commit_version，然后执行命令可以手动恢复这次提交的特定文件 ","date":"2024.11.1","objectID":"/blog/posts/config/git/:1:0","tags":["config"],"title":"Some about Git","uri":"/blog/posts/config/git/"},{"categories":["config"],"content":"文件删除\r对于 Git 而言，git add . 会将所有修改、新增的文件信息提交到暂存区，之后使用 git commit -m \"xxx\" 会将暂存区的文件信息提交到本地仓库。但是这种方法是对于新增和修改的文件而言，对于删除的文件信息不会进行更新。所以可以采用两种方案： 使用 git rm xxx 一个一个手动删除文件（rm 命令可以使用 git rm -r xxx 递归目录进行删除），之后这些文件删除信息就提交到暂存区了，后续就可以继续使用 git commit -m \"xxx\" 来将代码提交到本地仓库。 使用 git commit -am 命令，该命令会在提交到本地仓库时，先更新修改和删除的文件信息到暂存区（注意它不会提交新增加的文件信息）。所以加了 -a 在 commit 的时候，可以帮助省一步 git add，但是也只是对修改和删除文件有效，新文件还是要 git add，不然就是 untracked 状态。 综上所述：git add 和 git rm 都是等价的操作，前者添加修改和新增文件信息，后者添加删除文件信息，它们都是将文件信息提交到暂存区。之后使用 git commit -m \"xxx\" 来将暂存区的文件信息提交到本地仓库，最后使用 git push -u origin main 来提交本地仓库代码到远程仓库的 main 分支。 ","date":"2024.11.1","objectID":"/blog/posts/config/git/:2:0","tags":["config"],"title":"Some about Git","uri":"/blog/posts/config/git/"},{"categories":["config"],"content":"远程仓库到自己仓库\r拉取别人的仓库到自己仓库，主要应对github中没有对应仓库的情况繁琐指南，存在对应仓库，直接进行 fork，然后在本地添加自己的远程。简单的操作如下： git branch -r | grep -v '\\-\u003e' | while read remote; do git branch --track \"${remote#origin/}\" \"$remote\";done # 获取所有远程分支到本地 git fetch --all # 获取该项目远程库的所有分支及其内容 git fetch --tags # 获取该项目远程库的标签(没标签就不必了) git remote rename origin old-origin # 将原来的origin重命名一下 git remote add origin git@172.28.3.77:xs-soc/test-code.git # 指定需要迁移到新的目标地址(自己的仓库) git push origin --all # 推送所有分支及其内容 git push --tags # 推送所有标签及其内容 git remote rm origin # 删除当前远程库 git branch -M main # 重命名主要分支仓库 git push -u origin main # 推送到指定分支 ","date":"2024.11.1","objectID":"/blog/posts/config/git/:3:0","tags":["config"],"title":"Some about Git","uri":"/blog/posts/config/git/"},{"categories":["config"],"content":"Git 加速\rgit失败的原因绝大多数都是网络问题，所以挂代理是最为推荐的选择。以下是起作用的一些方法 通用方法，更换 git 的代理为 443 SSH：连接到主机github.com端口22：连接时间超时 但是对于 wsl，直接使用最新 wsl2 共用主机的代理即可（最为推荐），不嫌麻烦可以给配置个代理 配置wsl镜像 Windows10系统下配置WSL2自动走Clash代理，之后clash打开allow lan模式即可 WSL2内使用Windows的v2ray代理 | Nafx’s Blog，这是v2的模式，首先最后面设置，然后前面配置bashrc 有时候最后的方法会起点作用 git clone失败解决方案 ","date":"2024.11.1","objectID":"/blog/posts/config/git/:4:0","tags":["config"],"title":"Some about Git","uri":"/blog/posts/config/git/"},{"categories":["compile"],"content":"这里记录词法分析，正则表达式，自动机的相关知识。 ","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:0:0","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["compile"],"content":"正则表达式与自动机理论\r","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:1:0","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["compile"],"content":"课程目标\r这里讲解怎么写一个自动化词法分析器生成器。根据前面的理论，我们使用 ANTLR4 来生成词法分析器，其实质上是我们使用 ANTLR4 利用正则表达式（regular expression -\u003e RE）的规则来进行生成词法分析器。同时我们还学习了利用 java 来手写词法分析器，实质就是在使用 java 代码模拟状态转移图，它也就是自动机。那么我们来看 ANTLR4 原理，他就是把 .g4 文件转化为 .java 文件，也就是把正则表达式转化为了自动机，然后通过模拟自动机就可以得到词法分析器了。 因此我们的目标就是通过正则表达式来直接得到得到一个词法分析器。 conversion\r由上图，我们构建词法分析器就是把 RE 转化为 DFA（有穷状态自动机 Deterministic FInite Automata），然后再转化为词法分析器，但是这个过程往往是困难的，所以我们采用简略的方法，通过先转化为 NFA（不确定的又穷状态自动机Nondeteeministic Finite Automata），再转化为 DFA，再进行后续的操作。 ","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:1:1","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["compile"],"content":"编程语言介绍\r语言是字符串构成的集合。 根据这句高度抽象的结论，我们会一层层进行解剖。 字符 字母表 $\\Sigma$ 是一个有限的符号集合，符号没有意义，它的语义是后来自己赋予的。 字符串 字符表 $\\Sigma$ 上的串(s) 是由 $\\Sigma$ 中符号构成的一个有穷序列。 其中 $\\epsilon$ 是空串，我们定义它为零，即 $|\\epsilon| = 0$ 字符串之间存在运算 连接运算， $x = day, y = houce, xy = dayhouce, \\epsilon s = s \\epsilon = s$ 指数运算，$s^{0} \\triangleq \\epsilon$，$s^{i} \\triangleq ss^{i-1}, i\u003e0$，这里存在上标就是连接的意思 语言 语言是给定字母表 $\\Sigma$ 上一个任意的可数的串集合。 $\\empty$，这一个是空集，什么语言都没有；${ \\epsilon }$，这个里面有一个语言，不过是个空串 举例：id：${a,b,c,d,a1}$；ws：${blank, tab, newline }$，if：${ if }$ 我们知道语言是串的集合，正因为是集合，所以我们可以通过集合操作构造新的语言 rules\r","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:1:2","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["compile"],"content":"RE\r每个正则表达式 r 对应一个正则语言 L(r)。正则表达式是语法（ID：[a-zA-Z][a-zA-Z0-9]*），正则语言是语义（{a1,a2,ab,……}） ","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:2:0","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["compile"],"content":"语法\r给定字母表，$\\Sigma$ 上的正则表达式由且仅由以下规则定义： $\\epsilon$ 是正则表达式 $\\forall a \\in \\Sigma$，a 是正则表达式 如果 r 是正则表达式，则 (r) 是正则表达式 如果 r 与 s 是正则表达式，则 r | s，rs，r* 也是正则表达式 运算优先级：$() \\succ * \\succ 连接 \\succ |$ ，例子：$(a) \\mid ((b)^{*} (c)) \\equiv a | b^{*} c$ ","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:2:1","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["compile"],"content":"语义\r正则表达式对应的正则语言 $L(r)$ $L(\\epsilon) = { \\epsilon}$ $L(a) = a, \\forall a \\in \\Sigma$ $L((r)) = L(r)$ $L(r|s)=L(r) \\cup L(s)\\quad L(rs)=L(r)L(s)\\quad L(r^{*})=(L(r))^{*}$ ","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:2:2","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["compile"],"content":"符号\rsymbol\rsymbol\r","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:2:3","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["compile"],"content":"自动机\r","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:3:0","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["compile"],"content":"NFA\r语法\r非确定性有穷自动机 $\\mathcal{A}$ 是一个五元组 $\\mathcal{A} = (\\Sigma, S, s_0, \\delta, F)$ 字母表 $\\Sigma (\\epsilon \\notin \\Sigma)$ 有穷的状态集合 $S$ 唯一的初始状态 $s_0 \\in S$，这里的唯一不是强求，因为可以转化为唯一形态，转化方法就是前面再添加ige初始状态，然后通过 ${\\epsilon }$ 边转移到原始初始状态即可。 状态转移函数 $\\delta$，$\\delta: S \\times (\\Sigma \\cup {\\epsilon}) \\rightarrow 2^S$ 接受状态集合 $F \\subseteq S$，下图的 3 就是接受状态 这里非确定一个就是指接受统一字符的状态转移不唯一，如下图的 0 号节点，它接受字符 a 可以跑到两个状态上去；另一个就是可能存在 ${ \\epsilon }$ 边，在没有字符驱动的情况下自发的跑到另外一个状态。 state transfer\r上面的状态转移图没有规定如果碰到其他的字符该怎么处理，所以下图就约定所有没有对应出边的字符默认指向 空状态 $\\empty$，也就是 $(\\Sigma \\cup {\\epsilon})$，它表示达到自身，也意味着一个死状态。 state transfer\r语义\r有穷自动机是一类及其简单的计算装置，它可以识别（接收/拒绝）$\\Sigma$ 上的字符串 接收 （非确定性）有穷自动机 $\\mathcal{A}$ 接受字符串 x，当且仅当存在一条从开始状态 $s_0$ 到某个接受状态 $f \\in F$ 、标号为 x 的路径。 对于上面的状态转移图，只有 3 是接受状态，因此 $aabb \\in L(\\mathcal{a}), ababab \\notin L(\\mathcal{A})$ 因此，$\\mathcal{A}$ 定义了一种语言 $L(\\mathcal{A})$：它能接受的所有字符串构成的集合。所以根据上方状态转移图，可以得到自动机语言：$L(\\mathcal{A}) = L((a|b)^*abb)$ 由上面的语义，我们可以得到自动机的两个基本问题 Membership 问题：给定字符串 $x$，$x \\in L(\\mathcal{A})?$ $L(\\mathcal{A})$ 究竟是什么？ ","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:3:1","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["compile"],"content":"DFA\r语法\r确定性有穷自动机 $\\mathcal{A}$ 是一个五元组 $\\mathcal{A} = (\\Sigma, S, s_0, \\delta, F)$ 字母表 $\\Sigma (\\epsilon \\notin \\Sigma)$ 有穷的状态集合 $S$ 唯一的初始状态 $s_0 \\in S$，这个唯一是一定需要的 状态转移函数 $\\delta$，$\\delta: S \\times \\Sigma \\rightarrow S$ 接受状态集合 $F \\subseteq S$，下图的 3 就是接受状态 state transfer\r这里的约定就是：所有没有对应出边的字符串默认指向一个“死状态” 语义\r上图的自动机语言还是 $L(\\mathcal{A}) = L((a|b)^*abb)$，也就是上面的 NFA 和下面的 DFA 等价的。因此可以看出 NFA 适合去表达一个语言，容易得出语言是什么；而 DFA 则是因为状态的转移确定，适合写词法分析器。即 NFA 简介易于理解，便于描述语言 $L(\\mathcal{A})$；DFA易于判断$x \\in L(\\mathcal{A})$，适合产生词法分析器。那么转换就是 $RE \\Rightarrow NFA \\Rightarrow DFA \\Rightarrow$ 词法分析器。 ","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:3:2","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["compile"],"content":"相互转换\r这里就是根据下面这张图，使得正则表达式和自动机之间相互转换。 conversion\r","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:4:0","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["compile"],"content":"RE -\u003e NFA\r采用 Thompson 构造法，使得 $r \\Rightarrow NFA$，要求 $L(N(r)) = L(r)$，即两个语言等价。这里就是对于正则表达式语法的每个规则来定义自动机，然后最后将这些自动机按规则进行组合就得到了 NFA。 $N(r)$ 的性质以及 Thompson 构造法复杂度分析 $N(r)$ 的开始状态和接受状态均唯一 开始状态没有入边，接受状态没有出边 $N(r)$ 的状态数 $|S| \u003c 2 \\times |r|$（$|r|: r$ 中运算符和运算分量的总和） 每个状态最多有两个 $\\epsilon \\text{-}$ 入边与两个 $\\epsilon \\text{-}$ 出边 $\\forall a \\in \\Sigma$，每个状态最多有一个 $a \\text{-}$ 入边与一个 $a\\text{-}$ 出边 自动机构造如下： Thompson\r","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:4:1","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["compile"],"content":"NFA -\u003e DFA\r原理\r采用子集构造法，也就是用 DFA 模拟 NFA。 子集构造法\r下面就是从 NFA 到 DFA 的构造对应表，有了这张表就有了自动机。之所以是子集构造法，是因为构造出来的 DFA 对应于 NFA 的一个状态子集。同时这里因为在 NFA 中 10 是接受状态，所以在 DFA 中，对应的 E 也是接收状态。 构造对应表\r形式化描述子集构造法\r这里根据上图的转化，会得到两个重要的公式： $\\epsilon$ 闭包：从状态 s 开始，只通过 $\\epsilon \\text{-}$ 转移可达的状态集合 $\\epsilon\\text{-closure}(s)={t\\in S_N|s\\xrightarrow{\\epsilon^*}t}$，这个公式的含义就是把 NFA 中的初始状态归结于 DFA 中的初始状态。上图中 NFA 的 ${0,1,2,4,7}$，它是初始状态，在 NFA 中，从 0 开始，通过 $\\epsilon$ 边进行连接的状态在 DFA 中都是初始状态。之后进行扩展操作 $\\epsilon \\text{-closure(T)} = \\bigcup_{s \\in T}\\epsilon \\text{-closure(s)}$，这个就是把上面的初始状态都添加在一起，转化为了集合形式，即状态集合，它为下面的 move 公式提供操作变量。 $\\text{move(T,a)} = \\bigcup_{s\\in T} \\delta(s,a)$，这个公式就是根据集合的当前状态，然后根据转移函数 $\\delta$，逐个查看集合中每个元素在同一个字符作用的目标元素是什么，最后将目标元素添加到新集合中，这个集合就是 DFA 中的下一个状态。 之后就可以形式化描述子集构造法：子集构造法($N \\rightarrow D$) 的原理： $$ \\begin{array}{l} N: (\\Sigma_N, S_N, n_0, \\delta_N, F_N) \\\\ D: (\\Sigma_D, S_D, d_0, \\delta_D, F_D) \\\\ \\Sigma_{D} = \\Sigma_{N} \\\\ S_{D} \\subseteq 2^{S_{N}} \\quad (\\forall s_{D} \\in S_{D} : s_{D} \\subseteq S_{N}) \\end{array} $$ 初始状态：$d_{0} = \\epsilon \\text{-closure}(n_{0})$ 状态转移：$\\forall a \\in \\Sigma_{D} : \\delta_{D}(s_{D}, a) = \\epsilon\\text{-closure}(\\operatorname{move}(s_{D}, a))$ 接受状态集：$F_{\\mathcal{D}} = { s_{D} \\in S_{\\mathcal{D}} \\mid \\exists f \\in F_N \\colon f \\in s_{D} }$ 子集构造法的复杂度分析：（$|S_N=n|$，下面的符号就是算法分析中的分析符号） $$\\left|S_{D}\\right| = \\Theta\\left(2^{n}\\right) = O\\left(2^{n}\\right) \\cap \\Omega\\left(2^{n}\\right)$$ 对于任何算法，最坏情况下，$|S_{D}| = \\Omega\\left(2^{n}\\right)$。 ","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:4:2","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["compile"],"content":"DFA最小化\r方法\r我们还是查看之前使用 NFA -\u003e DFA 的转换图来看，下面的 DFA 就是使用子集构造法将 NFA 转化而来的，毫无疑问，与上面的图相比，它不是最小的，所以这里需要探究的就是如何将 DFA 转化为最小化的形式。 conversion\r这里DFA最小化算法基本思想：等价的状态可以合并。对于等价而言，如果存在某个能区分状态 s 与 t 的字符串，则称 s 与 t 是可区分的；否则，称 s 与 t 是等价的。这里的字符串 x 区分状态 s 与 t，就是指如果分别从 s 与 t 出发，沿着标号为 x 的路径到达的两个状态中只有一个是接受状态，则称区分了状态 s 与 t，也就是s 与 t 不等价。 所以状态等价就是说，对于两个状态而言，在任意同一个字符的驱动下从当前状态进行转换，转换后的状态也是等价的。它可以用下面的公式进行表示： $$ \\begin{array}{l} s \\sim t \\iff \\forall a \\in \\Sigma. \\left( (s \\xrightarrow{a} s’) \\land (t \\xrightarrow{a} t’) \\implies (s’ \\sim t’) \\right)\\\\ s \\nsim t \\iff \\exists a \\in \\Sigma. \\left( (s \\xrightarrow{a} s’) \\land (t \\xrightarrow{a} t’) \\land (s’ \\nsim t’) \\right) \\end{array} $$ 基于该定义，不断合并等价的状态，直到无法合并为止。但是我们的定义是一个递归的，不知道一开始要从什么地方入手，同时我们又得到所有的接受状态并不是等价的。所以这里采取的办法就是划分，利用反例公式 $s \\nsim t \\Longleftrightarrow \\exists a \\in \\Sigma. ( s \\xrightarrow{a} s’ ) \\land ( t \\xrightarrow{a} t’ ) \\land ( s’ \\nsim t’ )$ 进行划分，而非合并。也就是首先根据接受状态与非接受状态必定不等价先划分为两类 $\\Pi = {F, S \\setminus F}$，然后在这个基础上根据上面的反例公式进行分裂，直至再也无法划分为止，这里就到达了不动点，之后就是将同一等价类里的状态合并。 划分步骤\r上面就是分裂的过程，在 $\\Pi_0$ 到 $\\Pi_1$ 的过程中，${A,B,C}$ 和 ${D}$ 在经过 b 进行传递的状态是不等价的，此时 D 转移到 E 上了，E 输出 $S \\setminus F$，所以不等价。之后的操作也是这样挑选字符看转移后的状态处于哪一个集合中，如果不在本身的集合，那么就是不等价，需要进行分裂。 合并\r上图就是最后的分裂之后再合并得到最小化 DFA 的转换。 注意\r需要注意处理\"死状态\"，也就是指向 ${ \\empty}$ 的一些没有画出来的边，在进行分裂时需要添加上，即${F, S \\setminus F, { \\empty }}$ 刚刚的算法不适用于 NFA 最小化，NFA最小化问题是 PSPACE-complete 的，复杂度很高。 ","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:4:3","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["compile"],"content":"DFA -\u003e 词法分析器\r这里对于词法分析器的构造，需要注意一下几个要求，然后按照之前使用 java 模拟的方法进行构造即可： 需要满足最前优先匹配和最长优先匹配，与此同时，因为需要生成词法分析器的特定目的，所以要保留各个 NFA 的接受状态的信息，表明匹配的是什么正则表达式 需要消除 “死状态”，避免词法分析器徒劳消耗输入流。如果加上死状态，那么词法分析器就有可能走这条路径，然后会进行一直匹配，最后匹配出的也是死状态，妨碍正确匹配。 进行模拟的过程如下图所示，和之前 java 模拟的过程一样。 模拟过程\r最后需要注意初始划分需要考虑不同的词法单元。之前的划分按照接受状态和非接受状态进行划分，但是这里需要写词法分析器，所以最后的接收状态对应了不同的词法单元，所以也需要进一步划分为不同的集合。 特定词法单元\r","date":"2024.10.31","objectID":"/blog/posts/compile/lexer-re-automata/:4:4","tags":["compile"],"title":"01 Lexer Re Automata","uri":"/blog/posts/compile/lexer-re-automata/"},{"categories":["programming"],"content":"这里是 python 使用技巧的记录，包括日常使用和数据之间的转换。 ","date":"2024.10.31","objectID":"/blog/posts/programming/python/:0:0","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"基础知识\r","date":"2024.10.31","objectID":"/blog/posts/programming/python/:1:0","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"字符串\r\"\"\" \"\"\" 可以存储数行字符串 str = \"\"\"learn python the smart way 2nd edition hello word\"\"\" 使用 enumerate() 可以获得元素的序号 for idx, c in enumerate(str): print(idx, c) str.split 会把字符串划分为一个列表，依照空格进行划分 for word in str.split(): print(word) str.splitlines 会把字符串划分为一个列表，依照\"\\n\"进行划分 for line in str.splitlines(): if(line.startswith(\"hello\")): # startswith print(line) ","date":"2024.10.31","objectID":"/blog/posts/programming/python/:1:1","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"函数\r接收不定长参数，*args 表示参数数目不定，可以看成一个元组，把第一个参数后面的参数当作元组中的元素 def add(x, *args): total = x for arg in args: total += arg return total 上面的函数不能使用关键词传入参数，要使用关键词 **kwargs，它表示参数数目不定，相当于一个字典，键和值对应于键值对 def add(x, **kwargs): total = x for arg, value in kwargs.items(): print(\"adding %s=%s\" % (arg,value)) total += value return total # 使用方法如下： def foo(*args, **kwargs): print(args, kwargs) add(1, 2, 3, 4) foo(2, 3, x='bar', z=10) map 方法生成序列，map(aFun, aSeq)，函数 aFun 应用到序列 aSeq 上的每一个元素上，返回一个列表，不管这个序列原来是什么类型。事实上，根据函数参数的多少，map 可以接受多组序列，将其对应的元素作为参数传入函数。 def square(a, b, c): return a**2 + b + c a = [1,2,3] b = (4, 5, 6) print(list(map(square, a, b, b))) ","date":"2024.10.31","objectID":"/blog/posts/programming/python/:1:2","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"文件读写\rpython 提供安全的 with 来进行文件读写，当 with 块的内容结束后，Python 会自动调用它的 close 方法，确保读写的安全。 模式 描述 r 只读。该文件必须已存在。 r+ 可读可写。该文件必须已存在，写为追加在文件内容末尾。 rb 表示以二进制方式读取文件。该文件必须已存在。 w 只写。打开即默认创建一个新文件，如果文件已存在，则覆盖写（即文件内原始数据会被新写入的数据清空覆盖）。 w+ 写读。打开创建新文件并写入数据，如果文件已存在，则覆盖写。 wb 表示以二进制写方式打开，只能写文件， 如果文件不存在，创建该文件；如果文件已存在，则覆盖写。 a 追加写。若打开的是已有文件则直接对已有文件操作，若打开文件不存在则创建新文件，只能执行写（追加在后面），不能读。 a+ 追加读写。打开文件方式与写入方式和a一样，但是可以读。需注意的是你若刚用a+打开一个文件，一般不能直接读取，因为此时光标已经是文件末尾，除非你把光标移动到初始位置或任意非末尾的位置。 import os os.remove('newfile.txt') with open('newfile.txt','w+') as f: for i in range(30): x = 1.0 / (i - 10) f.write('hello world: ' + str(i) + '\\n') ","date":"2024.10.31","objectID":"/blog/posts/programming/python/:1:3","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"其他\r通过 split 对 “,” 进行分割，使得一行可以输入多个值。 a, b = input().split(\",\") print(f\"a = {a}, b = {b}\") print 操作，默认每次输入后会换行，控制结尾的参数是 end，设置 end 把 “\\n” 替换成了 “//\"。同时它一次也可以输出多个内容，默认以空格分隔，这里控制分割的参数就是 sep，修改之后空格变成 “//\"。 print(\"data\", end=\"//\") print(\"Data\", \"whale\", sep=\"//\") ","date":"2024.10.31","objectID":"/blog/posts/programming/python/:1:4","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"数据转换\r这里强制自己使用byte类型，这样可以统一python的不同数据类型 ","date":"2024.10.31","objectID":"/blog/posts/programming/python/:2:0","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"AllToBytes\r字符串转化为 bytes，也可以直接在前面加 'b' 来转换 string = \"Hello World\" str_byte = bytes(string, 'utf-8') # -\u003e b'Hello World' 二进制字符串转化为 bytes hex_string = \"68 656c6c6f20776f726c64\" # 这里空格不会影响结果，但是需要是两个字符(68中间不能加空格)一组 hex_byte = bytes.fromhex(hex_string) # -\u003e b'hello world' # list(hex_byte) -\u003e [104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100] 长整型转换为 bytes，小端序方式 long_i = 6788912312 # 下面就是计算转化为16进制的字节数 int.to_bytes(long_i, (long_i.bit_length() + 7) // 8, byteorder=\"little\") # -\u003e b'\\xb8\\x94\\xa6\\x94\\x01' 十六进制数转化为 bytes，小端序方式 hex_int = 0x12345678 int_byte = int.to_bytes(hex_int, 4, byteorder='little') # -\u003e b'xV4\\x12' 整型列表转化为 bytes list_num = [0x12, 0x34, 0x56, 0x78] list_byte = bytes(list_num) # -\u003e b'\\x124Vx' 字节列表转化为 bytes，先转化为字符串，再转化 str_list = ['1', 'C', 'E', 'B', 'E', '0', '8', '9', '7', '4', 'A', '9', '6', '1', 'C', '5'] # 先转str再转byte bytes(\"\".join(str_list), encoding=\"utf-8\") # -\u003e b'1CEBE08974A961C5' ","date":"2024.10.31","objectID":"/blog/posts/programming/python/:2:1","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"AllToBytes\rbytes 转化为字符串 byte = b'Hello World' byte_str = str(byte, 'utf-8') # -\u003e 'Hello World' bytes 转化为十六进制字符串 byte = b'hello world' byte_hex = byte.hex() # -\u003e '68656c6c6f20776f726c64' bytes 转化为长整型，小端序 byte = b'\\xb8\\x94\\xa6\\x94\\x01' i = int.from_bytes(byte, byteorder='little') # -\u003e 6788912312 bytes 转化为十六进制整型 byte = b'xV4\\x12' int_num = int.from_bytes(byte, byteorder='little') # -\u003e 305419896 0x12345678 bytes 转化为整型列表 byte = b'\\x124Vx' list_num = list(byte) # -\u003e [18, 52, 86, 120] bytes 转化为字符串列表 byte = b'1CEBE08974A961C5' str_list = [str(byte[i:i + 2], 'utf-8') for i in range(0, len(byte), 2)] # -\u003e ['1C', 'EB', 'E0', '89', '74', 'A9', '61', 'C5'] ","date":"2024.10.31","objectID":"/blog/posts/programming/python/:2:2","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"其余转换处理\r转化为字符串 chr(a) # 将 int 类型的 a 根据其 ascii 码转化为 str 字符 hex(a) # 将 int 类型的 a 转化为其十六进制 str 类型 str(a) # 将所有类型的 a 按照其本身转化为 str 类型 str = a.decode() # 将 bytes 类型的 a 转化为 str 类型 转化为整型 # a 为 k 进制数，使用 int 将 k 进制数的 a 转化为十进制数 # # int(a) 直接将字符 a 转化为 int 类型，此时 a 必须为数字字符，注意：不是转化 为ascii 码，而是转化为数字类型，即值不变，类型改变 int(a,k) # 将 str 类型的十六进制数 a 转化为 int 类型(这里十六进制需要加上0x) eval(a) # 将字符类型的 a 按其 ascii 码转化为 int 类型 ord(a) ","date":"2024.10.31","objectID":"/blog/posts/programming/python/:2:3","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"数据处理\rstruct 模块可以解决 bytes 和其他二进制数据类型的转换。pack 函数把任意数据类型变成 bytes，unpack 把 bytes 变成相应的数据类型。这里的格式就是(format:str, v1, v2, …)，其中format对于后面的数据进行匹配，然后输出。 ","date":"2024.10.31","objectID":"/blog/posts/programming/python/:3:0","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"pack\rpack 函数把任意数据类型变成 bytes struct.pack('\u003cII', 10240099, 1767863401) # -\u003e b'\\x00\\x9c@ci_ti' 如果符合ascii的标准，就直接转化为字符 ","date":"2024.10.31","objectID":"/blog/posts/programming/python/:3:1","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"unpack\runpack 把 bytes 变成相应的数据类型 struct.unpack('\u003cI', b'it_i') # -\u003e (1767863401,) 这里只有一个符合 I 的规则，所以只有一个数据 ","date":"2024.10.31","objectID":"/blog/posts/programming/python/:3:2","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"数据格式\rformat 参数就是上面使用的描述符，struct 利用它可以指定使用大端序还是小端序来解析或者生成数据 Character Byte order Size Alignment @ native native native，凑足4个字节 = native standard none \u003c little-endian standard none \u003e big-endian standard none ! network(=big-endian) standard none 数据格式，用于匹配当前字符的数据 Format C Type Python type Standard size x pad byte no value c char string of length 1 1 b signed char integer 1 B unsigned char integer 1 ? _Bool bool 1 h short integer 2 H unsigned short integer 2 i int integer 4 I unsigned int integer 4 l long integer 4 L unsigned long integer 4 q long long integer 8 Q unsigned long long integer 8 f float float 4 d double float 8 s char[] string 1 p char[] string 1 P void * integer 0 ","date":"2024.10.31","objectID":"/blog/posts/programming/python/:3:3","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"ipython\r","date":"2024.10.31","objectID":"/blog/posts/programming/python/:4:0","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"特点\ripython 比原生的 python 解释器好用很多，它拥有高亮、补全、魔法函数等功能 Tab，可以自动补全，例如输入 imp 然后按 Tab 键，它会自动补齐 import 这个单词；如果再按 tab，提示所有可导入的模块，按方向键可以进行导航。 ?，在变量和函数后面添加 ? 会输出相关文档，?? 会打印源码（前提库是 python 写的） %hist 会显示用户输入命令的历史记录 %edit 会打开系统文本编辑器 %edit x-y 打开 Notepad 并写入指定范围内的命令 %history -n 会显示历史记录及对应的序号，然后通过 %edit x-y 命令对某一范围输入为脚本供后续使用 ","date":"2024.10.31","objectID":"/blog/posts/programming/python/:4:1","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"使用相关\r对于 for 和 def 这些需要换行缩进的地方而言，换行会自动进行缩进处理。如果想删除这个缩进，直接使用 back 就可以了。如果需要保存或者运行，enter 之后不再输入内容，直接再按一次即可。 换行缩进使用\r","date":"2024.10.31","objectID":"/blog/posts/programming/python/:4:2","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":["programming"],"content":"Conda\r提一下 Wsl 中的 miniconda，这里通过它来配置 python 虚拟环境。但是当使用 conda activate py 开启虚拟环境时，使用 pip list 展示的往往就是原本 wsl python 的安装包，同时在 vscode 的命令行中使用 pip install 也有可能直接下载到原本的环境中。因此最好在 PowerShell 中使用 pip install 来将库安装到相应的虚拟环境中，然后可以通过 conda list 查看当前虚拟环境下的 python 库，其中 Channel 是 defaults 表示本来存在的，是 pypi 表示通过 pip 新安装的。 ","date":"2024.10.31","objectID":"/blog/posts/programming/python/:5:0","tags":["programming"],"title":"Some about Python","uri":"/blog/posts/programming/python/"},{"categories":null,"content":"关于我\r在系统/软件安全领域学习的菜狗🐶 有趣的事情总能吸引我，然后忘记正事 ","date":"2024.10.29","objectID":"/blog/about/:1:0","tags":null,"title":"About","uri":"/blog/about/"},{"categories":null,"content":"网址\rhttps://cztangt.github.io/blog/ https://github.com/czTangt ","date":"2024.10.29","objectID":"/blog/about/:2:0","tags":null,"title":"About","uri":"/blog/about/"},{"categories":null,"content":"联系方式\r渠道 信息 QQ Mjk3MzE3NDU5Mg== 邮箱 Y3ouVGFuZ3RAZ21haWwuY29t ","date":"2024.10.29","objectID":"/blog/about/:3:0","tags":null,"title":"About","uri":"/blog/about/"}]
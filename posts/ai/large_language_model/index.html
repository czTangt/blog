<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>Large Language Model - czTang</title><meta name=author content="czTang">
<meta name=description content="这里主要记录在大模型原理和技术课程中的学习。
"><meta name=keywords content='AI'><meta itemprop=name content="Large Language Model"><meta itemprop=description content="这里主要记录在大模型原理和技术课程中的学习。"><meta itemprop=datePublished content="2025-01-17T10:17:51+08:00"><meta itemprop=dateModified content="2025-01-17T10:17:51+08:00"><meta itemprop=wordCount content="1365"><meta itemprop=keywords content="AI"><meta property="og:url" content="https://czTangt.github.io/blog/posts/ai/large_language_model/"><meta property="og:site_name" content="czTang"><meta property="og:title" content="Large Language Model"><meta property="og:description" content="这里主要记录在大模型原理和技术课程中的学习。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-17T10:17:51+08:00"><meta property="article:modified_time" content="2025-01-17T10:17:51+08:00"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="Large Language Model"><meta name=twitter:description content="这里主要记录在大模型原理和技术课程中的学习。"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=https://nuthecz.oss-cn-hangzhou.aliyuncs.com/file/202410312135963.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=https://czTangt.github.io/blog/posts/ai/large_language_model/ title="Large Language Model - czTang"><link rel=prev type=text/html href=https://czTangt.github.io/blog/posts/staticanalysis/data-flow-analysis/ title="03 Data Flow Analysis"><link rel=stylesheet href=/blog/css/style.min.css><link rel=preload href=/blog/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/blog/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/blog/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/blog/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Large Language Model","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/czTangt.github.io\/blog\/posts\/ai\/large_language_model\/"},"genre":"posts","keywords":"AI","wordcount":1365,"url":"https:\/\/czTangt.github.io\/blog\/posts\/ai\/large_language_model\/","datePublished":"2025-01-17T10:17:51+08:00","dateModified":"2025-01-17T10:17:51+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"czTang"},"description":""}</script><script src=/blog/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=left><div class=header-title><a href=/blog/ title=czTang><span class=header-title-pre><i class='fa fa-coffee'>&nbsp</i></span><span class=header-title-text>czTang</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/blog/archives/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> Archives</a></li><li class=menu-item><a class=menu-link href=/blog/categories/><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden=true></i> Categories</a></li><li class=menu-item><a class=menu-link href=/blog/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> Tags</a></li><li class=menu-item><a class=menu-link href=/blog/about/><i class="fa-solid fa-user fa-fw fa-sm" aria-hidden=true></i> About</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/blog/ title=czTang><span class=header-title-pre><i class='fa fa-coffee'>&nbsp</i></span><span class=header-title-text>czTang</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/blog/archives/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> Archives</a></li><li class=menu-item><a class=menu-link href=/blog/categories/><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden=true></i> Categories</a></li><li class=menu-item><a class=menu-link href=/blog/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> Tags</a></li><li class=menu-item><a class=menu-link href=/blog/about/><i class="fa-solid fa-user fa-fw fa-sm" aria-hidden=true></i> About</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集><div class="details collection-details open"><div class="details-summary collection-summary"><i class="fa-solid fa-layer-group fa-fw" aria-hidden=true></i>
<span class=collection-name title=合集>AI</span>
<span class=collection-count>1</span><i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class="details-content collection-content"><nav><ul class=collection-list><li class=collection-item><span class=active title="Large Language Model">Large Language Model</span></li></ul><div class=collection-nav-simple><i class="fa-solid fa-angle-left fa-fw collection-nav-item text-secondary" aria-hidden=true></i><span class=text-secondary>1/1</span><i class="fa-solid fa-angle-right fa-fw collection-nav-item text-secondary" aria-hidden=true></i></div></nav></div></div></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>Large Language Model</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/czTangt title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src=/blog/images/avatar.jpg alt=czTang data-title=czTang width=20 height=20 class=avatar style="background:url(/blog/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'>&nbsp;czTang</a></span><span class=post-included-in>&nbsp;收录于 <a href=/blog/categories/ai/ class=post-category title="分类 - AI"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> AI</a> 和 <a href=/blog/collections/ai/ class=post-collection title="合集 - AI"><i class="fa-solid fa-layer-group fa-fw" aria-hidden=true></i> AI</a></span></div><div class=post-meta-line><span title="发布于 2025-01-17 10:17:51"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2025.1.17>2025.1.17</time></span>&nbsp;<span title="1365 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 1400 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 3 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#语言模型基础>语言模型基础</a><ul><li><a href=#基于统计方法的语言模型>基于统计方法的语言模型</a><ul><li><a href=#n-grams-语言模型>n-grams 语言模型</a></li><li><a href=#n-grams-的统计学原理>n-grams 的统计学原理</a></li></ul></li><li><a href=#基于-rnn-的语言模型>基于 RNN 的语言模型</a><ul><li><a href=#rnn-的基本原理>RNN 的基本原理</a></li><li><a href=#基于-rnn-的语言模型-1>基于 RNN 的语言模型</a></li></ul></li><li><a href=#基于-transformer-的语言模型>基于 Transformer 的语言模型</a><ul><li><a href=#transformer-的基本原理>Transformer 的基本原理</a></li><li><a href=#基于-transformer-的语言模型-1>基于 Transformer 的语言模型</a></li></ul></li><li><a href=#语言模型的采样方法>语言模型的采样方法</a></li><li><a href=#语言模型的评测>语言模型的评测</a></li></ul></li></ul></nav></div></div><div class=content id=content><p>这里主要记录在<a href="https://www.bilibili.com/video/BV1PB6XYFET2/?spm_id_from=333.1387.collection.video_card.click" target=_blank rel="external nofollow noopener noreferrer">大模型原理和技术</a>课程中的学习。</p><h2 id=语言模型基础 class=heading-element><span>语言模型基础</span>
<a href=#%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%9f%ba%e7%a1%80 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>语言是概率的。并且，语言的概率性与认知的概率性存在着密不可分的关系。语言模型（Language Models, LMs）旨在准确预测语言符号的概率。，语言模型经历了从规则模型到统计模型，再到神经网络模型的发展历程，逐步从呆板的机械式问答程序成长为具有强大泛化能力的多任务智能模型。下面按照语言模型发展的顺序依次进行讲解。</p><h3 id=基于统计方法的语言模型 class=heading-element><span>基于统计方法的语言模型</span>
<a href=#%e5%9f%ba%e4%ba%8e%e7%bb%9f%e8%ae%a1%e6%96%b9%e6%b3%95%e7%9a%84%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h4 id=n-grams-语言模型 class=heading-element><span>n-grams 语言模型</span>
<a href=#n-grams-%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>定义</strong>：n-grams 是一种基于统计的语言模型，通过统计语料库中词序列的频率来预测语言符号的概率。</li><li><strong>计算方法</strong>：n-grams 模型通过计算词序列的相对频率来预测文本的概率。公式为：
$$
P_{n-grams}(w_{1:N}) = \prod_{i=n}^{N} \frac{C(w_{i-n+1}:i)}{C(w_{i-n+1}:i-1)}
$$
其中，$C(w_{i-n+1}:i)$ 是词序列在语料库中出现的次数。</li><li><strong>马尔可夫假设</strong>：n-grams 模型基于 n 阶马尔可夫假设，即当前词的概率只与前 n 个词有关。</li><li><strong>零概率问题</strong>：当 n 较大时，语料库中可能找不到与 n-gram 完全匹配的词序列，导致零概率问题。可以通过平滑技术来缓解。</li></ul><h4 id=n-grams-的统计学原理 class=heading-element><span>n-grams 的统计学原理</span>
<a href=#n-grams-%e7%9a%84%e7%bb%9f%e8%ae%a1%e5%ad%a6%e5%8e%9f%e7%90%86 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>极大似然估计</strong>：n-grams 模型通过极大似然估计来近似词序列的概率。具体来说，n-grams 模型通过统计词序列的频率来估计条件概率 $P(w_i|w_{i-n+1}:i-1)$。</li></ul><h3 id=基于-rnn-的语言模型 class=heading-element><span>基于 RNN 的语言模型</span>
<a href=#%e5%9f%ba%e4%ba%8e-rnn-%e7%9a%84%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h4 id=rnn-的基本原理 class=heading-element><span>RNN 的基本原理</span>
<a href=#rnn-%e7%9a%84%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>循环传播范式</strong>：RNN 通过环路将历史状态叠加到当前状态上，从而能够基于历史信息进行预测。</li><li><strong>隐状态</strong>：RNN 的隐状态 $h_t$ 通过以下公式计算：
$$
h_t = g(W_H h_{t-1} + W_I x_t)
$$
其中，$g(\cdot)$ 是激活函数，$W_H$ 和 $W_I$ 是权重矩阵。</li><li><strong>梯度消失与爆炸</strong>：RNN 在训练过程中容易遇到梯度消失或梯度爆炸问题，可以通过 GRU 或 LSTM 等门控结构来缓解。</li></ul><h4 id=基于-rnn-的语言模型-1 class=heading-element><span>基于 RNN 的语言模型</span>
<a href=#%e5%9f%ba%e4%ba%8e-rnn-%e7%9a%84%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>预测下一个词</strong>：基于 RNN 的语言模型通过当前词和隐状态来预测下一个词的概率：
$$
P(w_{i+1}|w_{1:i}) = P(w_{i+1}|w_i, h_{i-1})
$$</li><li><strong>损失函数</strong>：通常使用交叉熵损失函数来训练 RNN 语言模型：
$$
l_{CE}(o_i) = -\log o_i [w_{i+1}]
$$</li><li><strong>Teacher Forcing</strong>：在训练过程中，使用标准答案作为下一轮的输入，以减少错误级联放大问题。</li></ul><h3 id=基于-transformer-的语言模型 class=heading-element><span>基于 Transformer 的语言模型</span>
<a href=#%e5%9f%ba%e4%ba%8e-transformer-%e7%9a%84%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h4 id=transformer-的基本原理 class=heading-element><span>Transformer 的基本原理</span>
<a href=#transformer-%e7%9a%84%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>注意力机制</strong>：Transformer 通过自注意力机制将前文信息叠加到当前状态上。自注意力层的输出为：
$$
Attention(x_t) = \sum_{i=1}^{t} \alpha_{t,i} v_i
$$
其中，$\alpha_{t,i}$ 是注意力权重。</li><li><strong>全连接前馈层</strong>：全连接前馈层负责记忆存储，公式为：
$$
FFN(v) = \max(0, W_1 v + b_1) W_2 + b_2
$$</li><li><strong>层正则化与残差连接</strong>：层正则化用于加速训练，残差连接用于缓解梯度消失问题。</li></ul><h4 id=基于-transformer-的语言模型-1 class=heading-element><span>基于 Transformer 的语言模型</span>
<a href=#%e5%9f%ba%e4%ba%8e-transformer-%e7%9a%84%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>下一词预测</strong>：Transformer 语言模型通过上文预测下一个词的概率：
$$
P(w_{i+1}|w_{1:i}) = o_i [w_{i+1}]
$$</li><li><strong>损失函数</strong>：同样使用交叉熵损失函数进行训练。</li></ul><h3 id=语言模型的采样方法 class=heading-element><span>语言模型的采样方法</span>
<a href=#%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e9%87%87%e6%a0%b7%e6%96%b9%e6%b3%95 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><ol><li><p><strong>概率最大化方法</strong>：</p><ul><li><strong>贪心搜索</strong>：每轮选择概率最大的词，容易陷入局部最优。</li><li><strong>波束搜索</strong>：每轮保留多个候选词，最终选择联合概率最大的词序列。</li></ul></li><li><p><strong>随机采样方法</strong>：</p><ul><li><strong>Top-K 采样</strong>：每轮选择概率最高的 K 个词，然后根据概率分布进行随机采样。</li><li><strong>Top-P 采样</strong>：设定概率阈值 p，选择累积概率超过 p 的词进行采样。</li><li><strong>Temperature 机制</strong>：通过调整 Temperature 参数来控制采样的随机性。</li></ul></li></ol><h3 id=语言模型的评测 class=heading-element><span>语言模型的评测</span>
<a href=#%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%af%84%e6%b5%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><ol><li><p><strong>内在评测</strong>：</p><ul><li><strong>困惑度（Perplexity）</strong>：衡量语言模型对测试文本的“困惑”程度，困惑度越低，模型性能越好。
$$
PPL(s_{test}) = P(w_{1:N})^{-\frac{1}{N}}
$$</li></ul></li><li><p><strong>外在评测</strong>：</p><ul><li><strong>基于统计指标的评测</strong>：如 BLEU 和 ROUGE，通过计算生成文本与标准答案的重合程度来评估模型性能。</li><li><strong>基于语言模型的评测</strong>：如 BERTScore 和 G-EVAL，利用语言模型的上下文词嵌入或生成能力来评估生成文本的质量。</li></ul></li></ol></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2025-01-17 10:17:51">更新于 2025.1.17&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href="https://github.com/czTangt/blog.git/blob/main/content/posts%5cAI%5cllm.md?plain=1" title=查看源码 target=_blank rel="external nofollow noopener noreferrer" class=link-to-source>查看源码</a></span><span><a href=https://github.com/czTangt/blog.git/edit/main/content/posts%5cAI%5cllm.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span><span><a href="https://github.com/czTangt/blog.git/issues/new?title=[BUG]%20Large+Language+Model&amp;body=%7cField%7cValue%7c%0A%7c-%7c-%7c%0A%7cTitle%7cLarge+Language+Model%7c%0A%7cURL%7chttps://czTangt.github.io/blog/posts/ai/large_language_model/%7c%0A%7cFilename%7chttps://github.com/czTangt/blog.git/blob/main/content/posts%5cAI%5cllm.md?plain=1%7c" title=报告问题 target=_blank rel="external nofollow noopener noreferrer" class=link-to-report>报告问题</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=https://czTangt.github.io/blog/posts/ai/large_language_model/ data-title="Large Language Model" data-hashtags=AI><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://czTangt.github.io/blog/posts/ai/large_language_model/ data-hashtag=AI><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://czTangt.github.io/blog/posts/ai/large_language_model/ data-title="Large Language Model"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/blog/tags/ai/ class=post-tag title="标签 - AI">AI</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/blog/>主页</a></span></section></div><div class=post-nav><a href=/blog/posts/staticanalysis/data-flow-analysis/ class=post-nav-item rel=prev title="03 Data Flow Analysis"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>03 Data Flow Analysis</a></div></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered order-1">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.136.5"><img class=hugo-icon src=/blog/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.17-30a67c4b"><img class=fixit-icon src=/blog/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright order-2" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2024 - 2025</span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><a href=https://github.com/czTangt/blog title="View source on GitHub" target=_blank rel="external nofollow" class="github-corner left d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true" width="56" height="56"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/blog/lib/lightgallery/css/lightgallery-bundle.min.css><link rel=preload href=/blog/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/blog/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/blog/lib/cookieconsent/cookieconsent.min.css><script src=/blog/lib/autocomplete/autocomplete.min.js defer></script><script src=/blog/lib/fuse/fuse.min.js defer></script><script src=/blog/lib/lightgallery/lightgallery.min.js defer></script><script src=/blog/lib/lightgallery/plugins/thumbnail/lg-thumbnail.min.js defer></script><script src=/blog/lib/lightgallery/plugins/zoom/lg-zoom.min.js defer></script><script src=/blog/lib/sharer/sharer.min.js async defer></script><script src=/blog/lib/katex/katex.min.js defer></script><script src=/blog/lib/katex/auto-render.min.js defer></script><script src=/blog/lib/katex/mhchem.min.js defer></script><script src=/blog/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/blog/js/codeblock.js defer></script><script src=https://libs.baidu.com/jquery/2.1.4/jquery.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:10},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},lightgallery:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{distance:100,findAllMatches:!1,fuseIndexURL:"/blog/search.json",highlightTag:"em",ignoreFieldNorm:!1,ignoreLocation:!1,isCaseSensitive:!1,location:0,maxResultLength:10,minMatchCharLength:2,noResultsFound:"没有找到结果",snippetLength:30,threshold:.3,type:"fuse",useExtendedSearch:!1},version:"v0.3.17-30a67c4b"}</script><script src=/blog/js/theme.min.js defer></script></body></html>
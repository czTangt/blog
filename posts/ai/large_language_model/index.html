<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>Large Language Model - czTang</title><meta name=author content="czTang">
<meta name=description content="这里主要记录在大模型原理和技术课程中的学习。
"><meta name=keywords content='AI'><meta itemprop=name content="Large Language Model"><meta itemprop=description content="这里主要记录在大模型原理和技术课程中的学习。"><meta itemprop=datePublished content="2025-01-17T10:17:51+08:00"><meta itemprop=dateModified content="2025-01-17T10:46:04+08:00"><meta itemprop=wordCount content="2619"><meta itemprop=keywords content="AI"><meta property="og:url" content="https://czTangt.github.io/blog/posts/ai/large_language_model/"><meta property="og:site_name" content="czTang"><meta property="og:title" content="Large Language Model"><meta property="og:description" content="这里主要记录在大模型原理和技术课程中的学习。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-17T10:17:51+08:00"><meta property="article:modified_time" content="2025-01-17T10:46:04+08:00"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="Large Language Model"><meta name=twitter:description content="这里主要记录在大模型原理和技术课程中的学习。"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=https://nuthecz.oss-cn-hangzhou.aliyuncs.com/file/202410312135963.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=https://czTangt.github.io/blog/posts/ai/large_language_model/ title="Large Language Model - czTang"><link rel=prev type=text/html href=https://czTangt.github.io/blog/posts/staticanalysis/data-flow-analysis/ title="03 Data Flow Analysis"><link rel=stylesheet href=/blog/css/style.min.css><link rel=preload href=/blog/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/blog/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/blog/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/blog/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Large Language Model","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/czTangt.github.io\/blog\/posts\/ai\/large_language_model\/"},"genre":"posts","keywords":"AI","wordcount":2619,"url":"https:\/\/czTangt.github.io\/blog\/posts\/ai\/large_language_model\/","datePublished":"2025-01-17T10:17:51+08:00","dateModified":"2025-01-17T10:46:04+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"czTang"},"description":""}</script><script src=/blog/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=left><div class=header-title><a href=/blog/ title=czTang><span class=header-title-pre><i class='fa fa-coffee'>&nbsp</i></span><span class=header-title-text>czTang</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/blog/archives/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> Archives</a></li><li class=menu-item><a class=menu-link href=/blog/categories/><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden=true></i> Categories</a></li><li class=menu-item><a class=menu-link href=/blog/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> Tags</a></li><li class=menu-item><a class=menu-link href=/blog/about/><i class="fa-solid fa-user fa-fw fa-sm" aria-hidden=true></i> About</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/blog/ title=czTang><span class=header-title-pre><i class='fa fa-coffee'>&nbsp</i></span><span class=header-title-text>czTang</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/blog/archives/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> Archives</a></li><li class=menu-item><a class=menu-link href=/blog/categories/><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden=true></i> Categories</a></li><li class=menu-item><a class=menu-link href=/blog/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> Tags</a></li><li class=menu-item><a class=menu-link href=/blog/about/><i class="fa-solid fa-user fa-fw fa-sm" aria-hidden=true></i> About</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集><div class="details collection-details open"><div class="details-summary collection-summary"><i class="fa-solid fa-layer-group fa-fw" aria-hidden=true></i>
<span class=collection-name title=合集>AI</span>
<span class=collection-count>1</span><i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class="details-content collection-content"><nav><ul class=collection-list><li class=collection-item><span class=active title="Large Language Model">Large Language Model</span></li></ul><div class=collection-nav-simple><i class="fa-solid fa-angle-left fa-fw collection-nav-item text-secondary" aria-hidden=true></i><span class=text-secondary>1/1</span><i class="fa-solid fa-angle-right fa-fw collection-nav-item text-secondary" aria-hidden=true></i></div></nav></div></div></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>Large Language Model</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/czTangt title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src=/blog/images/avatar.jpg alt=czTang data-title=czTang width=20 height=20 class=avatar style="background:url(/blog/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'>&nbsp;czTang</a></span><span class=post-included-in>&nbsp;收录于 <a href=/blog/categories/ai/ class=post-category title="分类 - AI"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> AI</a> 和 <a href=/blog/collections/ai/ class=post-collection title="合集 - AI"><i class="fa-solid fa-layer-group fa-fw" aria-hidden=true></i> AI</a></span></div><div class=post-meta-line><span title="发布于 2025-01-17 10:17:51"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2025.1.17>2025.1.17</time></span>&nbsp;<span title="更新于 2025-01-17 10:46:04"><i class="fa-regular fa-calendar-check fa-fw me-1" aria-hidden=true></i><time datetime=2025.1.17>2025.1.17</time></span>&nbsp;<span title="2619 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 2700 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 6 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#语言模型基础>语言模型基础</a><ul><li><a href=#基于统计方法的语言模型>基于统计方法的语言模型</a><ul><li><a href=#n-grams-语言模型>n-grams 语言模型</a></li><li><a href=#n-grams-的统计学原理>n-grams 的统计学原理</a></li></ul></li><li><a href=#基于-rnn-的语言模型>基于 RNN 的语言模型</a><ul><li><a href=#rnn-的基本原理>RNN 的基本原理</a></li><li><a href=#基于-rnn-的语言模型-1>基于 RNN 的语言模型</a></li></ul></li><li><a href=#基于-transformer-的语言模型>基于 Transformer 的语言模型</a><ul><li><a href=#transformer-的基本原理>Transformer 的基本原理</a></li><li><a href=#基于-transformer-的语言模型-1>基于 Transformer 的语言模型</a></li></ul></li><li><a href=#语言模型的采样方法>语言模型的采样方法</a></li><li><a href=#语言模型的评测>语言模型的评测</a></li></ul></li><li><a href=#大语言模型架构>大语言模型架构</a><ul><li><a href=#大数据与大模型>大数据与大模型</a></li><li><a href=#大语言模型架构概览>大语言模型架构概览</a><ul><li><a href=#主流模型架构>主流模型架构</a></li><li><a href=#注意力矩阵>注意力矩阵</a></li></ul></li><li><a href=#基于-encoder-only-架构>基于 Encoder-only 架构</a><ul><li><a href=#bert-模型>BERT 模型</a></li><li><a href=#bert-衍生模型>BERT 衍生模型</a></li></ul></li><li><a href=#基于-encoder-decoder-架构>基于 Encoder-Decoder 架构</a><ul><li><a href=#t5-模型>T5 模型</a></li><li><a href=#bart-模型>BART 模型</a></li></ul></li><li><a href=#基于-decoder-only-架构>基于 Decoder-only 架构</a><ul><li><a href=#gpt-系列>GPT 系列</a></li><li><a href=#llama-系列>LLaMA 系列</a></li></ul></li><li><a href=#非-transformer-架构>非 Transformer 架构</a></li></ul></li></ul></nav></div></div><div class=content id=content><p>这里主要记录在<a href="https://www.bilibili.com/video/BV1PB6XYFET2/?spm_id_from=333.1387.collection.video_card.click" target=_blank rel="external nofollow noopener noreferrer">大模型原理和技术</a>课程中的学习。</p><h2 id=语言模型基础 class=heading-element><span>语言模型基础</span>
<a href=#%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%9f%ba%e7%a1%80 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>语言是概率的。并且，语言的概率性与认知的概率性存在着密不可分的关系。语言模型（Language Models, LMs）旨在准确预测语言符号的概率。，语言模型经历了从规则模型到统计模型，再到神经网络模型的发展历程，逐步从呆板的机械式问答程序成长为具有强大泛化能力的多任务智能模型。下面按照语言模型发展的顺序依次进行讲解。</p><h3 id=基于统计方法的语言模型 class=heading-element><span>基于统计方法的语言模型</span>
<a href=#%e5%9f%ba%e4%ba%8e%e7%bb%9f%e8%ae%a1%e6%96%b9%e6%b3%95%e7%9a%84%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h4 id=n-grams-语言模型 class=heading-element><span>n-grams 语言模型</span>
<a href=#n-grams-%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>定义</strong>：n-grams 是一种基于统计的语言模型，通过统计语料库中词序列的频率来预测语言符号的概率。</li><li><strong>计算方法</strong>：n-grams 模型通过计算词序列的相对频率来预测文本的概率。公式为：
$$
P_{n-grams}(w_{1:N}) = \prod_{i=n}^{N} \frac{C(w_{i-n+1}:i)}{C(w_{i-n+1}:i-1)}
$$
其中，$C(w_{i-n+1}:i)$ 是词序列在语料库中出现的次数。</li><li><strong>马尔可夫假设</strong>：n-grams 模型基于 n 阶马尔可夫假设，即当前词的概率只与前 n 个词有关。</li><li><strong>零概率问题</strong>：当 n 较大时，语料库中可能找不到与 n-gram 完全匹配的词序列，导致零概率问题。可以通过平滑技术来缓解。</li></ul><h4 id=n-grams-的统计学原理 class=heading-element><span>n-grams 的统计学原理</span>
<a href=#n-grams-%e7%9a%84%e7%bb%9f%e8%ae%a1%e5%ad%a6%e5%8e%9f%e7%90%86 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>极大似然估计</strong>：n-grams 模型通过极大似然估计来近似词序列的概率。具体来说，n-grams 模型通过统计词序列的频率来估计条件概率 $P(w_i|w_{i-n+1}:i-1)$。</li></ul><h3 id=基于-rnn-的语言模型 class=heading-element><span>基于 RNN 的语言模型</span>
<a href=#%e5%9f%ba%e4%ba%8e-rnn-%e7%9a%84%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h4 id=rnn-的基本原理 class=heading-element><span>RNN 的基本原理</span>
<a href=#rnn-%e7%9a%84%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>循环传播范式</strong>：RNN 通过环路将历史状态叠加到当前状态上，从而能够基于历史信息进行预测。</li><li><strong>隐状态</strong>：RNN 的隐状态 $h_t$ 通过以下公式计算：
$$
h_t = g(W_H h_{t-1} + W_I x_t)
$$
其中，$g(\cdot)$ 是激活函数，$W_H$ 和 $W_I$ 是权重矩阵。</li><li><strong>梯度消失与爆炸</strong>：RNN 在训练过程中容易遇到梯度消失或梯度爆炸问题，可以通过 GRU 或 LSTM 等门控结构来缓解。</li></ul><h4 id=基于-rnn-的语言模型-1 class=heading-element><span>基于 RNN 的语言模型</span>
<a href=#%e5%9f%ba%e4%ba%8e-rnn-%e7%9a%84%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>预测下一个词</strong>：基于 RNN 的语言模型通过当前词和隐状态来预测下一个词的概率：
$$
P(w_{i+1}|w_{1:i}) = P(w_{i+1}|w_i, h_{i-1})
$$</li><li><strong>损失函数</strong>：通常使用交叉熵损失函数来训练 RNN 语言模型：
$$
l_{CE}(o_i) = -\log o_i [w_{i+1}]
$$</li><li><strong>Teacher Forcing</strong>：在训练过程中，使用标准答案作为下一轮的输入，以减少错误级联放大问题。</li></ul><h3 id=基于-transformer-的语言模型 class=heading-element><span>基于 Transformer 的语言模型</span>
<a href=#%e5%9f%ba%e4%ba%8e-transformer-%e7%9a%84%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h4 id=transformer-的基本原理 class=heading-element><span>Transformer 的基本原理</span>
<a href=#transformer-%e7%9a%84%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>注意力机制</strong>：Transformer 通过自注意力机制将前文信息叠加到当前状态上。自注意力层的输出为：
$$
Attention(x_t) = \sum_{i=1}^{t} \alpha_{t,i} v_i
$$
其中，$\alpha_{t,i}$ 是注意力权重。</li><li><strong>全连接前馈层</strong>：全连接前馈层负责记忆存储，公式为：
$$
FFN(v) = \max(0, W_1 v + b_1) W_2 + b_2
$$</li><li><strong>层正则化与残差连接</strong>：层正则化用于加速训练，残差连接用于缓解梯度消失问题。</li></ul><h4 id=基于-transformer-的语言模型-1 class=heading-element><span>基于 Transformer 的语言模型</span>
<a href=#%e5%9f%ba%e4%ba%8e-transformer-%e7%9a%84%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>下一词预测</strong>：Transformer 语言模型通过上文预测下一个词的概率：
$$
P(w_{i+1}|w_{1:i}) = o_i [w_{i+1}]
$$</li><li><strong>损失函数</strong>：同样使用交叉熵损失函数进行训练。</li></ul><h3 id=语言模型的采样方法 class=heading-element><span>语言模型的采样方法</span>
<a href=#%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e9%87%87%e6%a0%b7%e6%96%b9%e6%b3%95 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><ol><li><p><strong>概率最大化方法</strong>：</p><ul><li><strong>贪心搜索</strong>：每轮选择概率最大的词，容易陷入局部最优。</li><li><strong>波束搜索</strong>：每轮保留多个候选词，最终选择联合概率最大的词序列。</li></ul></li><li><p><strong>随机采样方法</strong>：</p><ul><li><strong>Top-K 采样</strong>：每轮选择概率最高的 K 个词，然后根据概率分布进行随机采样。</li><li><strong>Top-P 采样</strong>：设定概率阈值 p，选择累积概率超过 p 的词进行采样。</li><li><strong>Temperature 机制</strong>：通过调整 Temperature 参数来控制采样的随机性。</li></ul></li></ol><h3 id=语言模型的评测 class=heading-element><span>语言模型的评测</span>
<a href=#%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%af%84%e6%b5%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><ol><li><p><strong>内在评测</strong>：</p><ul><li><strong>困惑度（Perplexity）</strong>：衡量语言模型对测试文本的“困惑”程度，困惑度越低，模型性能越好。
$$
PPL(s_{test}) = P(w_{1:N})^{-\frac{1}{N}}
$$</li></ul></li><li><p><strong>外在评测</strong>：</p><ul><li><strong>基于统计指标的评测</strong>：如 BLEU 和 ROUGE，通过计算生成文本与标准答案的重合程度来评估模型性能。</li><li><strong>基于语言模型的评测</strong>：如 BERTScore 和 G-EVAL，利用语言模型的上下文词嵌入或生成能力来评估生成文本的质量。</li></ul></li></ol><h2 id=大语言模型架构 class=heading-element><span>大语言模型架构</span>
<a href=#%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><h3 id=大数据与大模型 class=heading-element><span>大数据与大模型</span>
<a href=#%e5%a4%a7%e6%95%b0%e6%8d%ae%e4%b8%8e%e5%a4%a7%e6%a8%a1%e5%9e%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><ol><li><p><strong>大数据与大模型的关系</strong>：</p><ul><li>数据规模的增长为模型提供了更丰富的信息源，模型规模的扩大增加了模型的表达能力。</li><li>模型规模和数据规模的增长共同作用，提升了模型的性能和新功能的涌现。</li><li><strong>扩展法则</strong>：<ul><li><strong>Kaplan-McCandlish 扩展法则</strong>：模型性能与模型规模和数据规模高度正相关，模型规模的增长速度应略快于数据规模。</li><li><strong>Chinchilla 扩展法则</strong>：数据集规模与模型规模同等重要，理想的数据集大小应为模型规模的20倍。</li></ul></li></ul></li><li><p><strong>涌现能力</strong>：</p><ul><li>随着模型规模和数据规模的提升，大语言模型涌现出新的能力，如上下文学习、常识推理、代码生成、逻辑推理等。</li><li>这些能力并非通过特定任务训练获得，而是随着模型复杂度的提升自然涌现。</li></ul></li></ol><h3 id=大语言模型架构概览 class=heading-element><span>大语言模型架构概览</span>
<a href=#%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84%e6%a6%82%e8%a7%88 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h4 id=主流模型架构 class=heading-element><span>主流模型架构</span>
<a href=#%e4%b8%bb%e6%b5%81%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>Encoder-only 架构</strong>：仅使用编码器，适合自然语言理解任务，如文本分类、情感分析等。</li><li><strong>Encoder-Decoder 架构</strong>：包含编码器和解码器，适合序列到序列任务，如机器翻译、文本摘要等。</li><li><strong>Decoder-only 架构</strong>：仅使用解码器，适合开放式生成任务，如文本生成、故事生成等。</li></ul><h4 id=注意力矩阵 class=heading-element><span>注意力矩阵</span>
<a href=#%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%9f%a9%e9%98%b5 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>Encoder-only</strong>：双向注意力，捕捉输入序列中所有Token的关系。</li><li><strong>Encoder-Decoder</strong>：结合编码器的双向注意力和解码器的单向注意力，适合复杂生成任务。</li><li><strong>Decoder-only</strong>：单向注意力，适合自回归生成任务。</li></ul><h3 id=基于-encoder-only-架构 class=heading-element><span>基于 Encoder-only 架构</span>
<a href=#%e5%9f%ba%e4%ba%8e-encoder-only-%e6%9e%b6%e6%9e%84 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h4 id=bert-模型 class=heading-element><span>BERT 模型</span>
<a href=#bert-%e6%a8%a1%e5%9e%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>结构</strong>：基于 Transformer 编码器，包含多层自注意力和前馈网络。</li><li><strong>预训练任务</strong>：掩码语言建模（MLM）和下文预测（NSP）。</li><li><strong>下游任务</strong>：文本分类、问答系统、语义相似度计算等。</li></ul><h4 id=bert-衍生模型 class=heading-element><span>BERT 衍生模型</span>
<a href=#bert-%e8%a1%8d%e7%94%9f%e6%a8%a1%e5%9e%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>RoBERTa</strong>：优化了 BERT 的预训练过程，使用更大的数据集和动态掩码。</li><li><strong>ALBERT</strong>：通过参数共享和嵌入分解减少参数量，提升训练效率。</li><li><strong>ELECTRA</strong>：引入生成器-判别器架构，提升训练效率和下游任务表现。</li></ul><h3 id=基于-encoder-decoder-架构 class=heading-element><span>基于 Encoder-Decoder 架构</span>
<a href=#%e5%9f%ba%e4%ba%8e-encoder-decoder-%e6%9e%b6%e6%9e%84 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h4 id=t5-模型 class=heading-element><span>T5 模型</span>
<a href=#t5-%e6%a8%a1%e5%9e%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>结构</strong>：统一的文本到文本生成框架，适合多种 NLP 任务。</li><li><strong>预训练任务</strong>：Span Corruption，预测被掩码的连续文本片段。</li><li><strong>下游任务</strong>：通过 Prompt 工程和微调适配多种任务。</li></ul><h4 id=bart-模型 class=heading-element><span>BART 模型</span>
<a href=#bart-%e6%a8%a1%e5%9e%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>结构</strong>：结合编码器和解码器，适合文本生成和理解任务。</li><li><strong>预训练任务</strong>：多种文本破坏任务，如 Token 遮挡、句子打乱等。</li><li><strong>下游任务</strong>：文本生成、文本摘要、问答系统等。</li></ul><h3 id=基于-decoder-only-架构 class=heading-element><span>基于 Decoder-only 架构</span>
<a href=#%e5%9f%ba%e4%ba%8e-decoder-only-%e6%9e%b6%e6%9e%84 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h4 id=gpt-系列 class=heading-element><span>GPT 系列</span>
<a href=#gpt-%e7%b3%bb%e5%88%97 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>GPT-1</strong>：初代 Decoder-only 模型，使用下一词预测任务进行预训练。</li><li><strong>GPT-2</strong>：增加模型规模和预训练数据，提升任务泛化能力。</li><li><strong>GPT-3</strong>：大幅增加模型规模和数据规模，涌现出上下文学习能力。</li><li><strong>ChatGPT 和 GPT-4</strong>：引入人类反馈强化学习（RLHF），提升指令跟随能力。</li></ul><h4 id=llama-系列 class=heading-element><span>LLaMA 系列</span>
<a href=#llama-%e7%b3%bb%e5%88%97 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><strong>LLaMA1</strong>：小模型+大数据理念，优化了 Transformer 的嵌入和注意力机制。</li><li><strong>LLaMA2</strong>：增加预训练数据规模，引入人类反馈强化学习。</li><li><strong>LLaMA3</strong>：大幅增加预训练数据规模，提升跨语言处理能力。</li></ul><h3 id=非-transformer-架构 class=heading-element><span>非 Transformer 架构</span>
<a href=#%e9%9d%9e-transformer-%e6%9e%b6%e6%9e%84 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><ol><li><p><strong>状态空间模型（SSM）</strong>：</p><ul><li><strong>SSM</strong>：通过状态变量捕捉系统状态的变化，适合处理长序列数据。</li><li><strong>RWKV</strong>：结合RNN和Transformer的优点，实现高效的长序列处理。</li><li><strong>Mamba</strong>：引入选择机制和硬件感知算法，提升长序列处理效率。</li></ul></li><li><p><strong>测试时训练（TTT）</strong>：</p><ul><li><strong>TTT</strong>：在推理阶段动态更新模型参数，提升长上下文建模能力。</li><li><strong>优势</strong>：线性时间复杂度，适合处理超长序列任务。</li></ul></li></ol></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2025-01-17 10:46:04">更新于 2025.1.17&nbsp;
<a class=git-hash href=https://github.com/czTangt/blog.git/commit/96f7edd1aa725b1a66f7b14de2c12957f3554c9e rel="external nofollow noopener noreferrer" target=_blank title="commit by czTangt(cz.tangt@gmail.com) 96f7edd1aa725b1a66f7b14de2c12957f3554c9e: add llm foundation"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>96f7edd</a></span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href="https://github.com/czTangt/blog.git/blob/main/content/posts%5cAI%5cllm.md?plain=1" title=查看源码 target=_blank rel="external nofollow noopener noreferrer" class=link-to-source>查看源码</a></span><span><a href=https://github.com/czTangt/blog.git/edit/main/content/posts%5cAI%5cllm.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span><span><a href="https://github.com/czTangt/blog.git/issues/new?title=[BUG]%20Large+Language+Model&amp;body=%7cField%7cValue%7c%0A%7c-%7c-%7c%0A%7cTitle%7cLarge+Language+Model%7c%0A%7cURL%7chttps://czTangt.github.io/blog/posts/ai/large_language_model/%7c%0A%7cFilename%7chttps://github.com/czTangt/blog.git/blob/main/content/posts%5cAI%5cllm.md?plain=1%7c" title=报告问题 target=_blank rel="external nofollow noopener noreferrer" class=link-to-report>报告问题</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=https://czTangt.github.io/blog/posts/ai/large_language_model/ data-title="Large Language Model" data-hashtags=AI><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://czTangt.github.io/blog/posts/ai/large_language_model/ data-hashtag=AI><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://czTangt.github.io/blog/posts/ai/large_language_model/ data-title="Large Language Model"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/blog/tags/ai/ class=post-tag title="标签 - AI">AI</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/blog/>主页</a></span></section></div><div class=post-nav><a href=/blog/posts/staticanalysis/data-flow-analysis/ class=post-nav-item rel=prev title="03 Data Flow Analysis"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>03 Data Flow Analysis</a></div></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered order-1">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.136.5"><img class=hugo-icon src=/blog/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.17-30a67c4b"><img class=fixit-icon src=/blog/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright order-2" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2024 - 2025</span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><a href=https://github.com/czTangt/blog title="View source on GitHub" target=_blank rel="external nofollow" class="github-corner left d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true" width="56" height="56"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/blog/lib/lightgallery/css/lightgallery-bundle.min.css><link rel=preload href=/blog/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/blog/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/blog/lib/cookieconsent/cookieconsent.min.css><script src=/blog/lib/autocomplete/autocomplete.min.js defer></script><script src=/blog/lib/fuse/fuse.min.js defer></script><script src=/blog/lib/lightgallery/lightgallery.min.js defer></script><script src=/blog/lib/lightgallery/plugins/thumbnail/lg-thumbnail.min.js defer></script><script src=/blog/lib/lightgallery/plugins/zoom/lg-zoom.min.js defer></script><script src=/blog/lib/sharer/sharer.min.js async defer></script><script src=/blog/lib/katex/katex.min.js defer></script><script src=/blog/lib/katex/auto-render.min.js defer></script><script src=/blog/lib/katex/mhchem.min.js defer></script><script src=/blog/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/blog/js/codeblock.js defer></script><script src=https://libs.baidu.com/jquery/2.1.4/jquery.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:10},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},lightgallery:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{distance:100,findAllMatches:!1,fuseIndexURL:"/blog/search.json",highlightTag:"em",ignoreFieldNorm:!1,ignoreLocation:!1,isCaseSensitive:!1,location:0,maxResultLength:10,minMatchCharLength:2,noResultsFound:"没有找到结果",snippetLength:30,threshold:.3,type:"fuse",useExtendedSearch:!1},version:"v0.3.17-30a67c4b"}</script><script src=/blog/js/theme.min.js defer></script></body></html>